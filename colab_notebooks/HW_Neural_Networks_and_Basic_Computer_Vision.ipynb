{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "fy9GDvxTONlw",
        "Hjt3fqb6VXPG",
        "kC1LBEI5VjBr",
        "-zXwk08XV2tL",
        "LD91NdrIWF9N",
        "fTRKd4UtU2N4",
        "JbbkbHuHUtT-",
        "z-YXofzkVFAi"
      ],
      "authorship_tag": "ABX9TyMSxYyB02N9loCrqCKAFpXb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/HW_Neural_Networks_and_Basic_Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW: https://docs.google.com/document/d/1ouD1Z1SVBqcBmdcy28_SgVPnx-k5GSnfZv7hAFIdmn0/edit"
      ],
      "metadata": {
        "id": "fy9GDvxTONlw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_yEYyl8ODjM"
      },
      "outputs": [],
      "source": [
        "# https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
        "import pandas as pd\n",
        "# example of tf.keras python idiom\n",
        "import tensorflow as tf\n",
        "# use keras API\n",
        "model = tf.keras.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "HXWNgCzM9PMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check version\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "id": "pp1G5mMY92hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the SGD optimizer class\n",
        "from keras.optimizers import SGD\n",
        "# compile the model\n",
        "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy') #‘sparse_categorical_crossentropy‘ for multi-class classification, ‘mse‘ (mean squared error) for regression\n",
        "# compile the model\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "eR9vWZ3E_Inc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #X, y = data - ?\n",
        "# # fit the model\n",
        "# model.fit(X, y, epochs=100, batch_size=32, verbose=0)\n",
        "# # evaluate the model\n",
        "# loss = model.evaluate(X, y, verbose=0)\n",
        "# # make a prediction\n",
        "# yhat = model.predict(X)"
      ],
      "metadata": {
        "id": "1Gs5-xFUBHb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a model defined with the sequential api\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_shape=(8,)))\n",
        "model.add(Dense(80))\n",
        "model.add(Dense(30))\n",
        "model.add(Dense(10))\n",
        "model.add(Dense(5))\n",
        "model.add(Dense(1))"
      ],
      "metadata": {
        "id": "KIMjbAplCXh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a model defined with the functional api\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "# define the layers\n",
        "x_in = Input(shape=(8,))\n",
        "x = Dense(10)(x_in)\n",
        "x_out = Dense(1)(x)\n",
        "# define the model\n",
        "model = Model(inputs=x_in, outputs=x_out)"
      ],
      "metadata": {
        "id": "63hCc2CLDGtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for binary classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "df = read_csv(path, header=None)\n",
        "# split into input and output columns\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# ensure all data are floating point values\n",
        "X = X.astype('float32')\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
        "# evaluate the model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Accuracy: %.3f' % acc)\n",
        "# make a prediction\n",
        "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
        "yhat = model.predict([row])\n",
        "print('Predicted: %.3f' % yhat)"
      ],
      "metadata": {
        "id": "IZOYVabmEzcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for multiclass classification\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
        "df = read_csv(path, header=None)\n",
        "# split into input and output columns\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# ensure all data are floating point values\n",
        "X = X.astype('float32')\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
        "# evaluate the model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Accuracy: %.3f' % acc)\n",
        "# make a prediction\n",
        "row = [5.1,3.5,1.4,0.2]\n",
        "yhat = model.predict([row])\n",
        "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
      ],
      "metadata": {
        "id": "4csOaVLuPW6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for regression\n",
        "from numpy import sqrt\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "df = read_csv(path, header=None)\n",
        "# split into input and output columns\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
        "# evaluate the model\n",
        "error = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))\n",
        "# make a prediction\n",
        "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
        "yhat = model.predict([row])\n",
        "print('Predicted: %.3f' % yhat)"
      ],
      "metadata": {
        "id": "Iw_GStm5Pqwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading and plotting the mnist dataset\n",
        "from tensorflow.keras.datasets.mnist import load_data\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
        "# plot first few images\n",
        "for i in range(25):\n",
        "\t# define subplot\n",
        "\tpyplot.subplot(5, 5, i+1)\n",
        "\t# plot raw pixel data\n",
        "\tpyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "rMHb0ETEQfh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a cnn for image classification\n",
        "from numpy import asarray\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from tensorflow.keras.datasets.mnist import load_data\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "# reshape data to have a single channel\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
        "# determine the shape of the input images\n",
        "in_shape = x_train.shape[1:]\n",
        "# determine the number of classes\n",
        "n_classes = len(unique(y_train))\n",
        "print(in_shape, n_classes)\n",
        "# normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=in_shape))\n",
        "model.add(MaxPool2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "# define loss and optimizer\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# fit the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
        "# evaluate the model\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Accuracy: %.3f' % acc)\n",
        "# make a prediction\n",
        "image = x_train[0]\n",
        "yhat = model.predict(asarray([image]))\n",
        "print('Predicted: class=%d' % argmax(yhat))"
      ],
      "metadata": {
        "id": "A34TNyI5RS7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm for time series forecasting\n",
        "from numpy import sqrt\n",
        "from numpy import asarray\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn asarray(X), asarray(y)\n",
        "\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
        "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
        "# retrieve the values\n",
        "values = df.values.astype('float32')\n",
        "# specify the window size\n",
        "n_steps = 5\n",
        "# split into samples\n",
        "X, y = split_sequence(values, n_steps)\n",
        "# reshape into [samples, timesteps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "# split into train/test\n",
        "n_test = 12\n",
        "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
        "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))\n",
        "# evaluate the model\n",
        "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, sqrt(mse), mae))\n",
        "# make a prediction\n",
        "row = asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
        "yhat = model.predict(row)\n",
        "print('Predicted: %.3f' % (yhat))"
      ],
      "metadata": {
        "id": "ZEr_kKLHSeHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of summarizing a model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(8,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "tIX4-ymDTJcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of plotting a model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(8,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# summarize the model\n",
        "plot_model(model, 'model.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "S93w4aG1TXzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of plotting learning curves\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from matplotlib import pyplot\n",
        "# create the dataset\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
        "# determine the number of input features\n",
        "n_features = X.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "sgd = SGD(learning_rate=0.001, momentum=0.8)\n",
        "model.compile(optimizer=sgd, loss='binary_crossentropy')\n",
        "# fit the model\n",
        "history = model.fit(X, y, epochs=100, batch_size=32, verbose=0, validation_split=0.3)\n",
        "# plot learning curves\n",
        "pyplot.title('Learning Curves')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.ylabel('Cross Entropy')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='val')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "qVbHhUloUCro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install h5py"
      ],
      "metadata": {
        "id": "xT_1nQmjUnzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of saving a fit model\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "# create the dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=1)\n",
        "# determine the number of input features\n",
        "n_features = X.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "sgd = SGD(learning_rate=0.001, momentum=0.8)\n",
        "model.compile(optimizer=sgd, loss='binary_crossentropy')\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=32, verbose=0, validation_split=0.3)\n",
        "# save model to file\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "LI57XOLrU3mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading a saved model\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow.keras.models import load_model\n",
        "# create the dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=1)\n",
        "# load the model from file\n",
        "model = load_model('model.h5')\n",
        "# make a prediction\n",
        "row = [1.91518414, 1.14995454, -1.52847073, 0.79430654]\n",
        "yhat = model.predict([row])\n",
        "print('Predicted: %.3f' % yhat[0])"
      ],
      "metadata": {
        "id": "o4IQkS0yVErp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of using dropout\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from matplotlib import pyplot\n",
        "# create the dataset\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
        "# determine the number of input features\n",
        "n_features = X.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "id": "1ffMs38hVxa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of using batch normalization\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from matplotlib import pyplot\n",
        "# create the dataset\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
        "# determine the number of input features\n",
        "n_features = X.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "id": "gUanjfk3WItX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of using early stopping\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# create the dataset\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
        "# determine the number of input features\n",
        "n_features = X.shape[1]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "# configure early stopping\n",
        "es = EarlyStopping(monitor='val_loss', patience=5)\n",
        "# fit the model\n",
        "history = model.fit(X, y, epochs=200, batch_size=32, verbose=0, validation_split=0.3, callbacks=[es])"
      ],
      "metadata": {
        "id": "ZEoKKwP4XEzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic part:\n",
        "1. Regressssion\n",
        "2. Classification"
      ],
      "metadata": {
        "id": "Hjt3fqb6VXPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression task - [Data](https://drive.google.com/drive/folders/13zKKNCa5srQSC8pqYWCIn5oRPXMlOGid?usp=sharing) (same dataset with regression 1 module)\n",
        "\n",
        "Analysis:\n",
        "1. the following features were dropped because of the 97% on nan values: Unnamed: 16,Unnamed: 15, NMHC(GT)\n",
        "2. the nan values in the following features are inputed (mean and mode) as it less that. 1%: 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH'\n",
        "3. Convert \"T\" into \"T_Farengetus\"\n",
        "3. Data preparation: normalization of data / scaling\n",
        "4. Non-linear feature engineering\n",
        "5. Model Creation:the network is going to be with one input layer, one hidden layer, and one output layer. We will use the ReLU activation function in the hidden layer and linear activation function in the output layer. We will also use the mean squared error loss function and the Adam optimizer.\n",
        "6. Model Evaluation: the model's performance is measured by plotting the training and validation loss over the training epochs and by calculating the correlation coefficient between the predicted and actual values on the testing set.\n",
        "\n",
        "Conclusions:\n",
        "1. A test loss of 0.0063 indicates that the regression neural network was able to accurately predict the target variable (C6H6) for the test dataset. A lower test loss indicates better performance, so this value is a good sign.\n",
        "The correlation coefficient of 0.9985 indicates a strong positive linear relationship between the predicted values and the actual values of C6H6. This suggests that the model is highly accurate in predicting the target variable.\n",
        "Overall, these results suggest that the regression neural network is a highly effective model for predicting the value of C6H6 based on the given dataset.\n",
        "2. Considering [the previous HW](https://colab.research.google.com/drive/1mtacDLrSW2RICYNAM0qlRjtV46bU-ER0#scrollTo=W-WKfoU1pOOp&uniqifier=2), the Linear Legression results are:\n",
        "    - Mean squared error: 1.939847787611042e-05\n",
        "    - MSE: 1.939847787611042e-05\n",
        "    - RMSE: 0.004404370315506\n",
        "    - MAE: 0.0036665067855805476\n",
        "    - R2: 0.99998178102174\n",
        "\n",
        "  When comparing these results with the output from the regression neural network, it appears that the linear regression model may perform slightly better in terms of mean squared error and root mean squared error. However, the correlation coefficient for the neural network was very high, indicating a strong linear relationship between the predicted values and actual values of the target variable."
      ],
      "metadata": {
        "id": "kC1LBEI5VjBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "url = \"/content/drive/MyDrive/Colab Notebooks/AirQualityUCI.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=';')\n",
        "data = data.drop_duplicates() #remove duplicates\n",
        "data.head(25)"
      ],
      "metadata": {
        "id": "W1EWeGbZXjMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for each dataset column print unique values\n",
        "for col in data.columns:\n",
        "    n_unique_values = data[col].nunique()\n",
        "    unique_values = data[col].unique()\n",
        "    print(f\"{col}: {n_unique_values}: {unique_values}\")"
      ],
      "metadata": {
        "id": "jiDOjE4ooQDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable and way to handle it. missing data can imply a reduction of the sample size\n",
        "\n",
        "total = data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (data.isna().sum()/data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "b-NBTU-KoWQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unnamed: 16,Unnamed: 15 delete\n",
        "data=data.drop(['Unnamed: 16','Unnamed: 15'],axis=1)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "DpFGBRRroZ3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace ',' with '.'\n",
        "data = data.replace(',', '.', regex=True)\n",
        "\n",
        "# convert date and time columns to datetime format\n",
        "data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')\n",
        "data['Time'] = pd.to_datetime(data['Time'], format='%H.%M.%S').dt.time\n",
        "\n",
        "# merge date and time columns into a new column and set it as the index\n",
        "data['DateTime'] = data['Date'].dt.strftime('%Y-%m-%d ') + data['Time'].astype(str)\n",
        "data = data.set_index(pd.to_datetime(data['DateTime'], format='%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# extract desired features from date column\n",
        "data['Day of week'] = data.index.dayofweek\n",
        "data['Day number'] = data.index.day\n",
        "data['Month number'] = data.index.month\n",
        "data['Year'] = data.index.year\n",
        "data['Season'] = (data.index.month % 12 + 3) // 3\n",
        "\n",
        "# extract desired features from time column\n",
        "data['Hour'] = data.index.hour\n",
        "data['Part of day'] = np.where(data['Hour'].isin(range(6, 12)), 1,\n",
        "                               np.where(data['Hour'].isin(range(12, 18)), 2,\n",
        "                                        np.where(data['Hour'].isin(range(18, 24)) | data['Hour'].isin(range(0, 6)), 3, np.nan)))\n",
        "\n",
        "# drop rows with missing values in Part of day column\n",
        "data = data.dropna(subset=['Part of day'])\n",
        "\n",
        "# drop unnecessary columns\n",
        "data = data.drop(['Date', 'Time', 'DateTime'], axis=1)\n",
        "\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "Aqv8iI0ioiKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert columns to integer data type\n",
        "data['Part of day'] = data['Part of day'].astype(int)\n",
        "data['Hour'] = data['Hour'].astype(int)\n",
        "data['Season'] = data['Season'].astype(int)\n",
        "data['Year'] = data['Year'].astype(int)\n",
        "data['Month number'] = data['Month number'].astype(int)\n",
        "data['Day number'] = data['Day number'].astype(int)\n",
        "data['Day of week'] = data['Day of week'].astype(int)\n",
        "\n",
        "#convert O(GT), C6H6(GT) and RH, AH, T into float data type\n",
        "data['CO(GT)'] = data['CO(GT)'].astype(float)\n",
        "data['C6H6(GT)'] = data['C6H6(GT)'].astype(float)\n",
        "data['RH'] = data['RH'].astype(float)\n",
        "data['AH'] = data['AH'].astype(float)\n",
        "data['T'] = data['T'].astype(float)\n",
        "\n",
        "# replace '-200' values with NaN so that we may inpute it\n",
        "data = data.replace(['-200', -200, -200.000000], np.nan)\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "YvbtG5V_rGFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature conversion function\n",
        "def celsius_to_fahrenheit(celsius):\n",
        "    return (celsius * 9/5) + 32\n",
        "# Convert temperature from Celsius to Fahrenheit\n",
        "data['T_Fahrenheit'] = data['T'].apply(celsius_to_fahrenheit)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data.head(5))"
      ],
      "metadata": {
        "id": "BldP-79LbDiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handle missing values - NMHC(GT) - column drop, others with median and mode\n",
        "# NMHC(GT) column drop\n",
        "data = data.drop(['NMHC(GT)', 'T'], axis=1)\n",
        "\n",
        "#devide features into num and categirical\n",
        "#empty lists for numerical and categorical features\n",
        "categorical_cols = ['Day of week', 'Day number', 'Month number', 'Year', 'Season', 'Hour', 'Part of day']\n",
        "\n",
        "# Get numerical columns by subtracting categorical columns from all columns\n",
        "numerical_cols = list(set(data.columns) - set(categorical_cols))\n",
        "\n",
        "print(\"All columns:\", data.columns)\n",
        "print(\"Numerical features:\", numerical_cols)\n",
        "print(\"Categorical features:\", categorical_cols)\n",
        "\n",
        "# fill missing values with median and mode\n",
        "median_values = data[numerical_cols].median()\n",
        "mode_values = data[categorical_cols].mode().iloc[0]\n",
        "data[numerical_cols] = data[numerical_cols].fillna(median_values)\n",
        "data[categorical_cols] = data[categorical_cols].fillna(mode_values)\n",
        "\n",
        "print(data.isna().sum())\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "5LxEMw1NrbJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering - basic transformations (nonlinear)\n",
        "# Apply square transformation to the data\n",
        "num_data_squared = np.square(data[numerical_cols])\n",
        "num_data_squared.columns = [col + '_sq' for col in num_data_squared.columns]\n",
        "\n",
        "# Apply cube transformation to the data\n",
        "num_data_cubed = np.power(data[numerical_cols], 3)\n",
        "num_data_cubed.columns = [col + '_cube' for col in num_data_cubed.columns]\n",
        "\n",
        "# Apply square root transformation to the data\n",
        "num_data_sqrt = np.sqrt(data[numerical_cols])\n",
        "num_data_sqrt.columns = [col + '_sqrt' for col in num_data_sqrt.columns]\n",
        "\n",
        "# Apply log transformation to the data\n",
        "num_data_log = np.log(data[numerical_cols])\n",
        "num_data_log.columns = [col + '_log' for col in num_data_log.columns]\n",
        "\n",
        "# Concatenate the original data with the transformed data\n",
        "num_transformed_data = pd.concat([data[numerical_cols], num_data_squared, num_data_cubed, num_data_sqrt, num_data_log], axis=1)\n",
        "num_transformed_data.index = data[numerical_cols].index\n",
        "\n",
        "print(num_transformed_data.columns)\n",
        "print(num_transformed_data.isna().sum())\n",
        "print(num_transformed_data.head())"
      ],
      "metadata": {
        "id": "1zLCrzYksboz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable\n",
        "total = num_transformed_data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (num_transformed_data.isnull().sum()/num_transformed_data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = num_transformed_data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (num_transformed_data.isna().sum()/num_transformed_data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "dVsHIve_4OrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standatization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_s = StandardScaler().fit(num_transformed_data)\n",
        "rescaled_d = scaler_s.transform(num_transformed_data)\n",
        "np.set_printoptions(precision=3)\n",
        "num_normalized_data_df = pd.DataFrame(rescaled_d, columns=num_transformed_data.columns)\n",
        "num_normalized_data_df.index = num_transformed_data.index\n",
        "num_normalized_data_df.head(15)"
      ],
      "metadata": {
        "id": "WG9RMO95sHe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge\n",
        "categorical_data_df = pd.DataFrame(data[categorical_cols])\n",
        "categorical_data_df.index = data[categorical_cols].index\n",
        "\n",
        "concatenated_data_1= categorical_data_df.merge(num_normalized_data_df, how='inner', on='DateTime')\n",
        "concatenated_data_2= pd.concat([categorical_data_df, num_normalized_data_df], axis=1)\n",
        "print('concatenated_data_1')\n",
        "print(concatenated_data_1.head())\n",
        "print(concatenated_data_1.isna().sum())\n",
        "print('concatenated_data_2')\n",
        "print(concatenated_data_2.head())\n",
        "print(concatenated_data_2.isna().sum())"
      ],
      "metadata": {
        "id": "V2r82EWuLt1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(concatenated_data_1, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=[len(train_data.keys())]),\n",
        "    keras.layers.Dropout(0.2), #during training, 20% of the inputs to the Dense layer will be randomly set to zero at each update, helping to prevent overfitting\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data, train_data['C6H6(GT)'], validation_split=0.2, epochs=100, verbose=0)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "test_loss = model.evaluate(test_data, test_data['C6H6(GT)'], verbose=0)\n",
        "print(f'Test loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "id": "UfpDdNJmxXnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training and validation loss over the epochs\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict the C6H6 values on the testing set\n",
        "test_predictions = model.predict(test_data).flatten()\n",
        "\n",
        "# Calculate the correlation coefficient between the predicted and actual values\n",
        "corr_coef = np.corrcoef(test_predictions, test_data['C6H6(GT)'])[0][1]\n",
        "print(f'Correlation coefficient: {corr_coef:.4f}')"
      ],
      "metadata": {
        "id": "s_sTLhM5y3fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification task - [Data](https://drive.google.com/drive/folders/1UF9FPkA06W7Z2rK3U2oDnkW3phq0VSYg?usp=sharing) (same dataset with classification 1 module)\n",
        "\n",
        "EDA:\n",
        "1. if TBG_measured, TSH_measured, T3_measured, TT4_measured, T4U_measured, FTI_measured = false, then TBG,FTI,T4U,TSH,T3,TT4 = 0\n",
        "2. Missing values are replaced in sex column with mode and missing values in age column with median\n",
        "3. Inpute outliers, except 0 values\n",
        "4. Categorical encoding for y and filter out entries with y = 3\n",
        "5. Data normalization - Standardization for numerical data\n",
        "6. Dummy data for categorical data + binary data lable encoding\n",
        "7. resampling using SMOTE\n",
        "8. In this code, we first load the data and split it into training and testing sets. Then, we define the neural network architecture using the Sequential class from Keras. Since I have a multi-class classification problem with three classes, the output layer should have three neurons with the activation function softmax.\n",
        "\n",
        "  We then compile the model by specifying the loss function (multi-class cross-entropy), optimizer (Adam), and evaluation metric (accuracy). We train the model for 50 epochs with a batch size of 32 and evaluate it on the test set.\n",
        "\n",
        "Conclusions:\n",
        "1. As for the number of hidden layers, there is no one-size-fits-all answer. The number of hidden layers and the number of neurons in each layer depend on the complexity of the problem and the size of the dataset. In general, deeper networks with more hidden layers can learn more complex representations of the data, but they are also more prone to overfitting. It's always a good idea to start with a simpler model and gradually increase the complexity until you start seeing diminishing returns in performance.\n",
        "2. The model with more layers outperformed the original model with two layers. The former had a test loss of 0.3042 and a test accuracy of 0.8803, while the latter had a test loss of 0.6862 and a test accuracy of 0.7165. Therefore, the model with more layers has learned more complex representations of the input data and is better at generalizing to new data.\n",
        "3. Considering [the previous work](https://colab.research.google.com/drive/1kWB-bOxdWRiONKbFIY8C5mBDe_eLPeUu#scrollTo=R3BX8dts-u0M) metrics for the machine learning models the deep learning classiifier with more layers is the best-performing model among all models in terms of accuracy, precision, recall, and F1 scores"
      ],
      "metadata": {
        "id": "-zXwk08XV2tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"/content/drive/MyDrive/Colab Notebooks/dataset_57_hypothyroid.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=',')\n",
        "data = data.drop_duplicates() #remove duplicates\n",
        "data.head(25)"
      ],
      "metadata": {
        "id": "3jcMgqHIXj7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the \"?\" value count\n",
        "data.replace('?', np.nan, inplace=True)\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "LYP_V9uQ8hSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if TBG_measured, TSH_measured, T3_measured, TT4_measured, T4U_measured, FTI_measured = false, then TBG,FTI,T4U,TSH,T3,TT4 = 0\n",
        "# replace NaN values with 0 for columns based on their respective measured columns\n",
        "cols_to_replace = ['TBG', 'FTI', 'T4U', 'TSH', 'T3', 'TT4']\n",
        "measured_cols = ['TBG_measured', 'FTI_measured', 'T4U_measured', 'TSH_measured', 'T3_measured', 'TT4_measured']\n",
        "for col, measured_col in zip(cols_to_replace, measured_cols):\n",
        "    data[col] = data[col].where(data[measured_col] == 't', 0)\n",
        "\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "BjMi96YO894s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert age, TSH, T3, TT4, T4U, FTI float data type\n",
        "data[['age', 'TSH', 'T3', 'TT4', 'T4U', 'FTI']] = data[['age', 'TSH', 'T3', 'TT4', 'T4U', 'FTI']].astype(float)\n",
        "\n",
        "#devide features into num and categirical\n",
        "#empty lists for numerical and categorical features\n",
        "numerical_cols = []\n",
        "categorical_cols = []\n",
        "\n",
        "#loop over each column and determine its data type\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    else:\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "print(\"All columns:\", data.columns)\n",
        "print(\"Numerical features:\", numerical_cols)\n",
        "print(\"Categorical features:\", categorical_cols)"
      ],
      "metadata": {
        "id": "bMarSj029JK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace missing values in sex column with mode\n",
        "data['sex'].fillna(data['sex'].mode()[0], inplace=True)\n",
        "\n",
        "# replace missing values in age column with median\n",
        "data['age'].fillna(data['age'].median(), inplace=True)\n",
        "\n",
        "# print the first 10 rows of the updated dataset\n",
        "print(data)"
      ],
      "metadata": {
        "id": "lQyPOIw89RUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outliers detection - Interquartile Range (IQR) method (values outside the normal range)\n",
        "for col in numerical_cols:\n",
        "    vals = data[col].values\n",
        "    q1, q3 = np.percentile(vals, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    outliers = vals[(vals < lower) | (vals > upper)]\n",
        "    unique_vals = data[col][data[col].isin(outliers)].value_counts()\n",
        "    print(f\"{col} has {len(outliers)} outliers with the following unique value counts:\\n{unique_vals}\\n\")"
      ],
      "metadata": {
        "id": "DmJHiAgy95PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 values\n",
        "for col in numerical_cols:\n",
        "    zero_vals = data.loc[data[col].isin([0, 0.0, 0.00]), col]\n",
        "    print(f\"{col} has {len(zero_vals)} rows with a value of 0, 0.0, or 0.00:\\n{zero_vals}\\n\")"
      ],
      "metadata": {
        "id": "0XxxN3fs97X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inpute outliers\n",
        "for outlier in outliers:\n",
        "    if outlier not in [0, 0.0, 0.00]:\n",
        "        if outlier < lower:\n",
        "            data[col][data[col] == outlier] = lower\n",
        "        else:\n",
        "            data[col][data[col] == outlier] = upper"
      ],
      "metadata": {
        "id": "rahbd4U_-R3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_cols:\n",
        "    zero_vals = data.loc[data[col].isin([0, 0.0, 0.00]), col]\n",
        "    print(f\"{col} has {len(zero_vals)} rows with a value of 0, 0.0, or 0.00:\\n{zero_vals}\\n\")"
      ],
      "metadata": {
        "id": "PnXAej-y-Zrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical encoding for y\n",
        "labelencoder = LabelEncoder()\n",
        "data['y'] = labelencoder.fit_transform(data['Class'])\n",
        "print(data['y'].value_counts())\n",
        "print(data['y'])"
      ],
      "metadata": {
        "id": "GHKjOqar-rEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out entries where y = 3\n",
        "data = data[data['y'] != 3]\n",
        "y=data['y']\n",
        "print(y.value_counts())\n",
        "print(y)"
      ],
      "metadata": {
        "id": "JY9lXjPb-1-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy variables for the 'sex' column\n",
        "sex_dummy = pd.get_dummies(data['sex'], prefix='sex')\n",
        "sex_dummy"
      ],
      "metadata": {
        "id": "xi4-HIT--992"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode binary columns with 1s and 0s\n",
        "binary_cols = ['on_thyroxine', 'query_on_thyroxine', 'on_antithyroid_medication',\n",
        "               'sick', 'pregnant', 'thyroid_surgery', 'I131_treatment',\n",
        "               'query_hypothyroid', 'query_hyperthyroid', 'lithium', 'goitre',\n",
        "               'tumor', 'hypopituitary', 'TSH_measured', 'T3_measured',\n",
        "               'TT4_measured', 'T4U_measured', 'FTI_measured', 'psych']\n",
        "data[binary_cols] = data[binary_cols].apply(lambda x: x.map({'t': 1, 'f': 0}))\n",
        "data"
      ],
      "metadata": {
        "id": "dIRIVW4u_D_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy variables for the 'referral_source' column\n",
        "referral_dummy = pd.get_dummies(data['referral_source'], prefix='referral')\n",
        "referral_dummy"
      ],
      "metadata": {
        "id": "C_gXM27H_KUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the encoded columns with the original data\n",
        "data_encoded = data.drop(['sex', 'referral_source', 'Class', 'TBG', 'TBG_measured'], axis=1) #'y'\n",
        "data_encoded = pd.merge(data_encoded, sex_dummy, left_index=True, right_index=True)\n",
        "data_encoded = pd.merge(data_encoded, referral_dummy, left_index=True, right_index=True)\n",
        "data_encoded = pd.DataFrame(data=data_encoded, columns=data_encoded.columns)\n",
        "\n",
        "data_encoded"
      ],
      "metadata": {
        "id": "3FoQsjDM_OmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate binary features from continuous numerical features\n",
        "binary_features = ['y'] + [col for col in data_encoded.columns if data_encoded[col].nunique() == 2]\n",
        "numerical_features = [col for col in data_encoded.columns if col not in binary_features]\n",
        "\n",
        "# print the binary and numerical features\n",
        "print(\"Binary Features: \", binary_features)\n",
        "print(\"Numerical Features: \", numerical_features)"
      ],
      "metadata": {
        "id": "slJo5Tb3_ZPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data normalization - Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_s = StandardScaler().fit(data_encoded[numerical_features])\n",
        "rescaled_d = scaler_s.transform(data_encoded[numerical_features])\n",
        "#np.set_printoptions(precision=3)\n",
        "\n",
        "# create a new dataframe with the rescaled numerical features\n",
        "rescaled_df = pd.DataFrame(data=rescaled_d, columns=numerical_features)\n",
        "rescaled_df"
      ],
      "metadata": {
        "id": "kaWEgdF6_fLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the binary and numerical features\n",
        "data_rescaled = data_encoded[binary_features].merge(rescaled_df, how='inner', left_index=True, right_index=True)\n",
        "print(data_rescaled)"
      ],
      "metadata": {
        "id": "KMQGx3A__q9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resampling using SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "print(data_rescaled['y'].value_counts())\n",
        "y = data_rescaled['y'].values\n",
        "print('y:', y.shape)\n",
        "X = data_rescaled.drop('y', axis=1).values\n",
        "print('X:', X.shape)\n",
        "\n",
        "oversample = SMOTE(k_neighbors=10) #https://machinelearningmastery.com/multi-class-imbalanced-classification/\n",
        "X_resampled, y_resampled = oversample.fit_resample(X, y) #ValueError: Expected n_neighbors <= n_samples,  but n_samples = 2, n_neighbors = 3\n",
        "\n",
        "#summarize distribution\n",
        "counter = Counter(y_resampled)\n",
        "for k, v in counter.items():\n",
        "   per = v / len(y_resampled) * 100\n",
        "   print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
        "\n",
        "#plot the distribution\n",
        "pyplot.bar(counter.keys(), counter.values())\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "sp5RcujFAXYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# define the model architecture with dropout layers\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# define the model architecture with more layers and dropout layers\n",
        "model_2 = Sequential()\n",
        "model_2.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(32, activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(16, activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(9, activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "model_2.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "loss_2, accuracy_2 = model_2.evaluate(X_test, y_test)\n",
        "print('Test loss for the model with more layers:', loss_2)\n",
        "print('Test accuracy for the model with more layers:', accuracy_2)"
      ],
      "metadata": {
        "id": "fo7S-y1z0clT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CV part:\n",
        "1. Dataset - https://drive.google.com/file/d/1zew1CtclL_AJSpPTpNTR84mQOOeiHbF3/view?usp=sharing\n",
        "2. Binary classification model for classifying images in two classes, use [this](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) example as a basis.\n"
      ],
      "metadata": {
        "id": "LD91NdrIWF9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline-model"
      ],
      "metadata": {
        "id": "fTRKd4UtU2N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mplcyberpunk"
      ],
      "metadata": {
        "id": "gIJd_HpbU5W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mplcyberpunk #!pip install mplcyberpunk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Activation,\n",
        "    BatchNormalization,\n",
        "    Conv2D,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    MaxPooling2D,\n",
        ")\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Specify the path of the zip archive and the directory where you want to extract the files\n",
        "ZIP_PATH = \"/content/drive/MyDrive/Colab Notebooks/hotdog__not_hotdog.zip\"\n",
        "EXTRACT_PATH = \"/content/drive/MyDrive/Colab Notebooks/data/\"\n",
        "\n",
        "\n",
        "def load_data(img_height: int = 150, img_width: int = 150, batch_size: int = 32) -> tuple:\n",
        "    \"\"\"\n",
        "    Loads the training and testing datasets and applies data augmentation to the training set.\n",
        "\n",
        "    Args:\n",
        "        img_height: Height of the input images.\n",
        "        img_width: Width of the input images.\n",
        "        batch_size: Batch size for training and testing.\n",
        "\n",
        "    Returns:\n",
        "        train_ds: A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images\n",
        "            with shape (batch_size, img_height, img_width, 3) and y is a numpy array of corresponding labels.\n",
        "        test_ds: A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images\n",
        "            with shape (batch_size, img_height, img_width, 3) and y is a numpy array of corresponding labels.\n",
        "    \"\"\"\n",
        "    # Define data augmentation for the training dataset\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1.0 / 255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "    )\n",
        "\n",
        "    # Define rescaling for the testing dataset\n",
        "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "    # Load the training dataset\n",
        "    train_ds = datagen.flow_from_directory(\n",
        "        f\"{EXTRACT_PATH}/hotdog__not_hotdog/train\",\n",
        "        target_size=(img_height, img_width),\n",
        "        class_mode=\"binary\",\n",
        "        batch_size=batch_size,\n",
        "        subset=\"training\",\n",
        "    )\n",
        "\n",
        "    # Load the testing dataset\n",
        "    test_ds = test_datagen.flow_from_directory(\n",
        "        f\"{EXTRACT_PATH}/hotdog__not_hotdog/test\",\n",
        "        target_size=(img_height, img_width),\n",
        "        class_mode=\"binary\",\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_ds, test_ds\n",
        "\n",
        "def create_model(input_shape: tuple = (150, 150, 3), dropout_rate: float = 0.3) -> keras.models.Model:\n",
        "    \"\"\"\n",
        "    Creates a baseline CNN model for binary classification.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of the input images.\n",
        "        dropout_rate: Dropout rate for regularization.\n",
        "\n",
        "    Returns:\n",
        "        model: A keras Model object representing the baseline CNN model.\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential()\n",
        "\n",
        "    # Add convolutional, batch normalization, activation, and pooling layers\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten the output and add dense layers with dropout regularization\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the model using the RMSprop optimizer\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model: keras.models.Model, train_ds: tf.keras.preprocessing.image.DirectoryIterator, test_ds: tf.keras.preprocessing.image.DirectoryIterator, epochs: int = 10) -> keras.callbacks.History:\n",
        "    \"\"\"\n",
        "    Trains the specified model on the specified datasets.\n",
        "    Args:\n",
        "        model: The keras model to train.\n",
        "        train_ds: The training dataset.\n",
        "        test_ds: The testing dataset.\n",
        "        epochs: The number of epochs to train for.\n",
        "\n",
        "    Returns:\n",
        "        history: A keras History object containing information about the training process.\n",
        "    \"\"\"\n",
        "    optimizer = RMSprop(learning_rate=0.001)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(patience=3, monitor=\"val_loss\"),\n",
        "        keras.callbacks.ModelCheckpoint(\"hotdog_classifier.h5\", save_best_only=True),\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=epochs,\n",
        "        validation_data=test_ds,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_history(history: keras.callbacks.History):\n",
        "    \"\"\"\n",
        "    Plots the training and validation accuracy and loss curves.\n",
        "    Args:\n",
        "        history: A keras History object containing information about the training process.\n",
        "    \"\"\"\n",
        "    acc = history.history[\"accuracy\"]\n",
        "    val_acc = history.history[\"val_accuracy\"]\n",
        "    loss = history.history[\"loss\"]\n",
        "    val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.style.use(\"cyberpunk\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, \"b\", label=\"Training accuracy\")\n",
        "    plt.plot(epochs, val_acc, \"r\", label=\"Validation accuracy\")\n",
        "    plt.title(\"Training and validation accuracy\")\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
        "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
        "    plt.title(\"Training and validation loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "    mplcyberpunk.add_glow_effects()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Extract the dataset\n",
        "  with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(EXTRACT_PATH)\n",
        "\n",
        "  # Load the data\n",
        "  train_ds, test_ds = load_data()\n",
        "\n",
        "  # Create the model\n",
        "  model = create_model()\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
        "\n",
        "  # Train the model\n",
        "  history = model.fit(train_ds, epochs=10, validation_data=test_ds)\n",
        "\n",
        "  # Plot the training and validation accuracy and loss\n",
        "  fig, axs = plt.subplots(2, figsize=(8, 8))\n",
        "  fig.suptitle(\"Training Metrics\")\n",
        "\n",
        "  # Plot the accuracy\n",
        "  axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "  axs[0].plot(history.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
        "  axs[0].set_ylabel(\"Accuracy\")\n",
        "  axs[0].legend(loc=\"lower right\")\n",
        "  axs[0].grid(color=\"w\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "  # Plot the loss\n",
        "  axs[1].plot(history.history[\"loss\"], label=\"train loss\")\n",
        "  axs[1].plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
        "  axs[1].set_ylabel(\"Loss\")\n",
        "  axs[1].set_xlabel(\"Epoch\")\n",
        "  axs[1].legend(loc=\"upper right\")\n",
        "  axs[1].grid(color=\"w\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "  # Add the cyberpunk style to each subplot\n",
        "  for ax in axs.ravel():\n",
        "    mplcyberpunk.add_glow_effects(ax)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Save the model\n",
        "  model.save(\"/content/drive/MyDrive/Colab Notebooks/data/models/hotdog_not_hotdog.h5\")"
      ],
      "metadata": {
        "id": "G0bTXH6uke10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib, os, random, mplcyberpunk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Specify the path of the zip archive\n",
        "zip_path = \"/content/drive/MyDrive/Colab Notebooks/hotdog__not_hotdog.zip\"\n",
        "\n",
        "# Specify the directory where you want to extract the files\n",
        "extract_path = \"/content/drive/MyDrive/Colab Notebooks/data/\"\n",
        "\n",
        "# Extract the files from the zip archive\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size = 32\n",
        "img_height, img_width = 150, 150\n",
        "input_shape = (img_height, img_height, 3)\n",
        "\n",
        "train_ds = datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/data/hotdog__not_hotdog/train', target_size=(img_height, img_width), class_mode='binary', batch_size=batch_size, subset='training')\n",
        "test_ds = test_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/data/hotdog__not_hotdog/test', target_size=(img_height, img_width), class_mode='binary', batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class BaseLineModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(BaseLineModel, self).__init__()\n",
        "        self.C1 = Conv2D(32, (3 * 3), padding='same', input_shape = input_shape)\n",
        "        self.B1 = BatchNormalization()\n",
        "        self.A1 = Activation('relu')\n",
        "        self.P1 = MaxPooling2D(2, padding='same')\n",
        "\n",
        "        self.C2 = Conv2D(32, (3 * 3), padding='same')\n",
        "        self.B2 = BatchNormalization()\n",
        "        self.A2 = Activation('relu')\n",
        "        self.P2 = MaxPooling2D(2, padding='same')\n",
        "        self.Dr1 = Dropout(0.3)\n",
        "\n",
        "        self.F1 = Flatten()\n",
        "        self.D1 = Dense(128, activation='relu')\n",
        "        self.B3 = BatchNormalization()\n",
        "        self.D2 = Dense(128, activation='relu')\n",
        "        self.D3 = Dense(64, activation='relu')\n",
        "        self.D4 = Dense(64, activation='relu')\n",
        "        self.D5 = Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.C1(x)\n",
        "        x = self.B1(x)\n",
        "        x = self.A1(x)\n",
        "        x = self.P1(x)\n",
        "\n",
        "        x = self.C2(x)\n",
        "        x = self.B2(x)\n",
        "        x = self.A2(x)\n",
        "        x = self.P2(x)\n",
        "        x = self.Dr1(x)\n",
        "\n",
        "        x = self.F1(x)\n",
        "        x = self.D1(x)\n",
        "        x = self.B3(x)\n",
        "        x = self.D2(x)\n",
        "        x = self.D3(x)\n",
        "        x = self.D4(x)\n",
        "        y = self.D5(x)\n",
        "        return y\n",
        "\n",
        "BaseModel = BaseLineModel()\n",
        "BaseModel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "checkpoint_save_path = '/content/drive/MyDrive/Colab Notebooks/data/BaseLine.ckpt'\n",
        "if os.path.exists(checkpoint_save_path +'.index'):\n",
        "    print('---------------------------------------Loading---------------------------------------')\n",
        "    BaseModel.load_weights(checkpoint_save_path)\n",
        "\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_best_only=True, save_weights_only=True)\n",
        "history = BaseModel.fit(train_ds, steps_per_epoch=10, epochs=10, batch_size=batch_size, callbacks=[cp_callback], validation_data=test_ds, validation_steps=10)\n",
        "\n",
        "BaseModel.save_weights('/content/drive/MyDrive/Colab Notebooks/data/models/first_try.h5')\n",
        "BaseModel.summary()"
      ],
      "metadata": {
        "id": "fGmRS5pl8v6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.style.use('cyberpunk')\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Acc')\n",
        "plt.title('Training Acc')\n",
        "plt.legend()\n",
        "mplcyberpunk.add_glow_effects()\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "mplcyberpunk.add_glow_effects()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xok7zn6UPEnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-trained models"
      ],
      "metadata": {
        "id": "JbbkbHuHUtT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set image dimensions\n",
        "img_height = 75\n",
        "img_width = 75\n",
        "batch_size = 32\n",
        "\n",
        "# create data generators with image augmentations\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# load the train data\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/data/hotdog__not_hotdog/train',\n",
        "                                                    target_size=(img_height, img_width),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='binary')\n",
        "\n",
        "# load the test data\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/data/hotdog__not_hotdog/test',\n",
        "                                                  target_size=(img_height, img_width),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='binary')\n",
        "\n",
        "# load the pre-trained models\n",
        "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
        "vgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
        "resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
        "inception_v3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
        "\n",
        "# freeze the layers of the pre-trained models\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in resnet50.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in inception_v3.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# add new layers for binary classification\n",
        "model_vgg16 = Sequential()\n",
        "model_vgg16.add(vgg16)\n",
        "model_vgg16.add(GlobalAveragePooling2D())\n",
        "model_vgg16.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_vgg19 = Sequential()\n",
        "model_vgg19.add(vgg19)\n",
        "model_vgg19.add(GlobalAveragePooling2D())\n",
        "model_vgg19.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_resnet50 = Sequential()\n",
        "model_resnet50.add(resnet50)\n",
        "model_resnet50.add(GlobalAveragePooling2D())\n",
        "model_resnet50.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_inception_v3 = Sequential()\n",
        "model_inception_v3.add(inception_v3)\n",
        "model_inception_v3.add(GlobalAveragePooling2D())\n",
        "model_inception_v3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the models\n",
        "model_vgg16.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_vgg19.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_resnet50.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_inception_v3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the models\n",
        "history_vgg16 = model_vgg16.fit(train_generator, epochs=10, validation_data=test_generator)\n",
        "history_vgg19 = model_vgg19.fit(train_generator, epochs=10, validation_data=test_generator)\n",
        "history_resnet50 = model_resnet50.fit(train_generator, epochs=10, validation_data=test_generator)\n",
        "history_inception_v3 = model_inception_v3.fit(train_generator, epochs=10, validation_data=test_generator)\n",
        "\n",
        "# plot the learning curves\n",
        "plt.plot(history_vgg16.history['val_accuracy'], label='VGG16')\n",
        "plt.plot(history_vgg19.history['val_accuracy'], label='VGG19')\n",
        "plt.plot(history_resnet50.history['val_accuracy'], label='ResNet50')\n",
        "plt.plot(history_inception_v3.history['val_accuracy'], label='InceptionV3')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LFLtJ5knRBaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def one_cycle(epoch, lr):\n",
        "    max_lr = 0.01\n",
        "    end_percentage = 0.1\n",
        "    steps = int(epoch * len(train_generator) * end_percentage / batch_size) + 1\n",
        "    if epoch < steps:\n",
        "        return max_lr * epoch / steps\n",
        "    else:\n",
        "        return max_lr * (1 - (epoch - steps) / (steps * (1 - end_percentage)))\n",
        "\n",
        "# define the callback\n",
        "lr_callback = LearningRateScheduler(one_cycle)\n",
        "\n",
        "# train the models with the callback\n",
        "history_vgg16_lr = model_vgg16.fit(train_generator, epochs=10, validation_data=test_generator, callbacks=[lr_callback])\n",
        "history_vgg19_lr = model_vgg19.fit(train_generator, epochs=10, validation_data=test_generator, callbacks=[lr_callback])\n",
        "history_resnet50_lr = model_resnet50.fit(train_generator, epochs=10, validation_data=test_generator, callbacks=[lr_callback])\n",
        "history_inception_v3_lr = model_inception_v3.fit(train_generator, epochs=10, validation_data=test_generator, callbacks=[lr_callback])\n",
        "\n",
        "# plot the learning rate and accuracy curves for VGG16\n",
        "plt.plot(history_vgg16_lr.history['lr'], history_vgg16_lr.history['accuracy'])\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('VGG16 Learning Rate vs. Accuracy')\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "# plot the learning rate and accuracy curves for VGG19\n",
        "plt.plot(history_vgg19_lr.history['lr'], history_vgg19_lr.history['accuracy'])\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('VGG19 Learning Rate vs. Accuracy')\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "# plot the learning rate and accuracy curves for ResNet50\n",
        "plt.plot(history_resnet50_lr.history['lr'], history_resnet50_lr.history['accuracy'])\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('ResNet50 Learning Rate vs. Accuracy')\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "# plot the learning rate and accuracy curves for InceptionV3\n",
        "plt.plot(history_inception_v3_lr.history['lr'], history_inception_v3_lr.history['accuracy'])\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('InceptionV3 Learning Rate vs. Accuracy')\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OgD2rGdkRnTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning an image classification model"
      ],
      "metadata": {
        "id": "z-YXofzkVFAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define the data directory\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/data/hotdog__not_hotdog\"\n",
        "\n",
        "# Define the image size, batch size, and number of epochs\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Define data generators for train, validation, and test sets\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_ds = train_datagen.flow_from_directory(\n",
        "    directory=data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"training\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "validation_ds = train_datagen.flow_from_directory(\n",
        "    directory=data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"validation\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_ds = test_datagen.flow_from_directory(\n",
        "    directory=data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    subset=None,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Define the base model\n",
        "base_model = tf.keras.applications.Xception(\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(img_size[0], img_size[1], 3),\n",
        "    include_top=False\n",
        ")\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the model by adding layers on top of the base model\n",
        "inputs = tf.keras.Input(shape=(img_size[0], img_size[1], 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_ds,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_ds.reset()\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
        "print(\"Test accuracy: %.2f%%\" % (test_acc * 100))\n",
        "\n",
        "# Plot the training and validation accuracy and loss curves\n",
        "acc = history.history['binary_accuracy']\n",
        "val_acc = history.history['val_binary_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NaASDMtdVMZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the layers to fine-tune and epochs to fine-tune for\n",
        "fine_tune_at = 100\n",
        "epochs_fine = 10\n",
        "\n",
        "# Unfreeze layers after `fine_tune_at` layer\n",
        "for layer in base_model.layers[fine_tune_at:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model for fine-tuning\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        ")\n",
        "\n",
        "# Train the model for fine-tuning\n",
        "if 'history' in locals():\n",
        "    initial_epoch = history.epoch[-1]\n",
        "else:\n",
        "    initial_epoch = 0\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs_fine + initial_epoch,\n",
        "    initial_epoch=initial_epoch,\n",
        "    validation_data=validation_ds,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "94evSM2gjG0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss_fine, test_acc_fine = model.evaluate(test_ds, verbose=0)\n",
        "print(\"Fine-tuned test accuracy: %.2f%%\" % (test_acc_fine * 100))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['binary_accuracy'], label='Initial Training Accuracy')\n",
        "plt.plot(history.history['val_binary_accuracy'], label='Initial Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy During Initial Training')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_fine.history['binary_accuracy'], label='Fine-tuned Training Accuracy')\n",
        "plt.plot(history_fine.history['val_binary_accuracy'], label='Fine-tuned Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy During Fine-tuning')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sm16AeUcBARz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}