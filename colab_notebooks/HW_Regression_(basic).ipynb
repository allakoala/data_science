{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "q36uvoKX7S0a",
        "ZfZmGWfxMSDD",
        "56cB4uF3IfnN",
        "nQ1OF3qBKois",
        "zMhLHCsMFL9F",
        "mfY44joqMv3G",
        "E_LMQshIhAwT",
        "31ywIBB0hPV4",
        "YRF8BcyVkQj8",
        "4CC2psdVkVWj",
        "W-WKfoU1pOOp",
        "W11MNg_9pTpX"
      ],
      "authorship_tag": "ABX9TyNjF+knQy27AA5vwCalykQx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/HW_Regression_(basic).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HW: https://docs.google.com/document/d/1er_E-XQrx9xsGEd0uNwCVfXqq_kU5YtHZLizHjJq2vA/edit?usp=sharing"
      ],
      "metadata": {
        "id": "M7uHxcStmRxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. EDA: exploration of variables and properties of data"
      ],
      "metadata": {
        "id": "q36uvoKX7S0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions:\n",
        "1. the following features were dropped because of the 97% on nan values: Unnamed: 16,Unnamed: 15, NMHC(GT)\n",
        "2. the nan values in the following features should be inputed (mean and mode) as it less that. 1%: 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH'\n",
        "3. All columns have no outliers.\n"
      ],
      "metadata": {
        "id": "ZfZmGWfxMSDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "plt.style.use('bmh')\n",
        "\n",
        "from pylab import rcParams"
      ],
      "metadata": {
        "id": "bJYvwF09oafm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q3E75kPSnuTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChGQ73NdmLzf"
      },
      "outputs": [],
      "source": [
        "url = \"/content/drive/MyDrive/Data/AirQualityUCI.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=';')\n",
        "data = data.drop_duplicates() #remove duplicates\n",
        "data.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#identify the data type\n",
        "data.info()"
      ],
      "metadata": {
        "id": "8r6lRyxivwNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for each dataset column print unique values\n",
        "for col in data.columns:\n",
        "    n_unique_values = data[col].nunique()\n",
        "    unique_values = data[col].unique()\n",
        "    print(f\"{col}: {n_unique_values}: {unique_values}\")"
      ],
      "metadata": {
        "id": "Fn2CGQk0-Fgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable and way to handle it. missing data can imply a reduction of the sample size\n",
        "\n",
        "total = data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (data.isna().sum()/data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "uZVorSml_YOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unnamed: 16,Unnamed: 15 delete\n",
        "data=data.drop(['Unnamed: 16','Unnamed: 15'],axis=1)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "LvEgt8BJ_yzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace ',' with '.'\n",
        "data = data.replace(',', '.', regex=True)"
      ],
      "metadata": {
        "id": "7xOjernMHgyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert date and time columns to datetime format\n",
        "data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')\n",
        "data['Time'] = pd.to_datetime(data['Time'], format='%H.%M.%S').dt.time\n",
        "\n",
        "# merge date and time columns into a new column and set it as the index\n",
        "data['DateTime'] = data['Date'].dt.strftime('%Y-%m-%d ') + data['Time'].astype(str)\n",
        "data = data.set_index(pd.to_datetime(data['DateTime'], format='%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# extract desired features from date column\n",
        "data['Day of week'] = data.index.dayofweek\n",
        "data['Day number'] = data.index.day\n",
        "data['Month number'] = data.index.month\n",
        "data['Year'] = data.index.year\n",
        "data['Season'] = (data.index.month % 12 + 3) // 3\n",
        "\n",
        "# extract desired features from time column\n",
        "data['Hour'] = data.index.hour\n",
        "data['Part of day'] = np.where(data['Hour'].isin(range(6, 12)), 1,\n",
        "                               np.where(data['Hour'].isin(range(12, 18)), 2,\n",
        "                                        np.where(data['Hour'].isin(range(18, 24)) | data['Hour'].isin(range(0, 6)), 3, np.nan)))\n",
        "\n",
        "# drop rows with missing values in Part of day column\n",
        "data = data.dropna(subset=['Part of day'])\n",
        "\n",
        "# drop unnecessary columns\n",
        "data = data.drop(['Date', 'Time', 'DateTime'], axis=1)\n",
        "\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "TI7aKOAQEDQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Part of day column to integer data type\n",
        "data['Part of day'] = data['Part of day'].astype(int)\n",
        "data['Hour'] = data['Hour'].astype(int)\n",
        "data['Season'] = data['Season'].astype(int)\n",
        "data['Year'] = data['Year'].astype(int)\n",
        "data['Month number'] = data['Month number'].astype(int)\n",
        "data['Day number'] = data['Day number'].astype(int)\n",
        "data['Day of week'] = data['Day of week'].astype(int)\n",
        "\n",
        "#convert O(GT), C6H6(GT) and RH, AH, T into float data type\n",
        "data['CO(GT)'] = data['CO(GT)'].astype(float)\n",
        "data['C6H6(GT)'] = data['C6H6(GT)'].astype(float)\n",
        "data['RH'] = data['RH'].astype(float)\n",
        "data['AH'] = data['AH'].astype(float)\n",
        "data['T'] = data['T'].astype(float)"
      ],
      "metadata": {
        "id": "XhgB86YWClx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#devide features into num and categirical\n",
        "\n",
        "#empty lists for numerical and categorical features\n",
        "numerical_cols = []\n",
        "categorical_cols = []\n",
        "\n",
        "#loop over each column and determine its data type\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    else:\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "print(\"All columns:\", data.columns)\n",
        "print(\"Numerical features:\", numerical_cols)\n",
        "print(\"Categorical features:\", categorical_cols)"
      ],
      "metadata": {
        "id": "p-93CBmHBUWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical into dummy data\n",
        "#cat_dummies_data = pd.get_dummies(data[categorical_cols]) #categorical input data (One-hot encode categorical features)\n",
        "#cat_dummies_data.columns"
      ],
      "metadata": {
        "id": "jO6zU1wOuTYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outliers detection\n",
        "# Interquartile Range (IQR) method (values outside the normal range)\n",
        "for col in numerical_cols:\n",
        "    vals = data[col].values\n",
        "    q1, q3 = np.percentile(vals, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    outliers = vals[(vals < lower) | (vals > upper)]\n",
        "    print(f\"{col} has {len(outliers)} outliers\")"
      ],
      "metadata": {
        "id": "cJ_M4ql5B2Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna()\n",
        "data.info()"
      ],
      "metadata": {
        "id": "2s-M7bf1Iq9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace '-200' values with NaN so that we may inpute it\n",
        "data = data.replace(['-200', -200, -200.000000], np.nan)\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "nGkGey7Yz6k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable and way to handle it. missing data can imply a reduction of the sample size\n",
        "total = data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (data.isna().sum()/data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "jtyc9Lzw1EtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handle missing values - NMHC(GT) - column drop, others with median and mode\n",
        "# NMHC(GT) column drop\n",
        "data = data.drop('NMHC(GT)', axis=1)\n",
        "\n",
        "# median inputation\n",
        "data = data.fillna(data.median())\n",
        "\n",
        "# mode inputation\n",
        "#data = data.apply(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "EOu4D_MW2L20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#devide features into num and categirical\n",
        "\n",
        "#empty lists for numerical and categorical features\n",
        "numerical_cols = []\n",
        "categorical_cols = []\n",
        "\n",
        "#loop over each column and determine its data type\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    else:\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "print(\"All columns:\", data.columns)\n",
        "print(\"Numerical features:\", numerical_cols)\n",
        "print(\"Categorical features:\", categorical_cols)"
      ],
      "metadata": {
        "id": "Ftg6MS8YKWzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numerical features analysis:\n",
        "All list ar—É printed out after the plots in the code section below:\n",
        "1. mean < median (distribution of values in that column is negatively skewed and it has a long tail on the left-hand side of the median and is more spread out towards the lower values. there are more observations with lower values than with higher values)\n",
        "2. mean > median (distribution of values in that column is positevly skewed, presence of abnormal high values)\n",
        "3. big difference between 75th %tile and max  and 25% and nim values (sign of extreme values-Outliers, check out box plots):\n",
        "4. high standard deviation (values are more spread out and have a wider range of values)\n",
        "5. low standard deviation (values are more tightly clustered around the mean and have a narrower range of values)\n",
        "6. Feature pairs with positive slopes\n",
        "7. Feature pairs with negative slopes\n",
        "8. pairs of features which have a high positive correlation coefficient (close to 1), then they are likely measuring similar aspects of the data and may be redundant\n",
        "9. pairs of features which have a high negative correlation coefficient (close to -1), then they may be measuring opposite aspects of the data and may be providing conflicting information"
      ],
      "metadata": {
        "id": "56cB4uF3IfnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#continuous variables distribution visualization using a histogram\n",
        "\n",
        "#summary statistics of the numerical features\n",
        "print(data[numerical_cols].describe())\n",
        "\n",
        "#the histogram is colored by the target variable 'y' (which is binary) to see the differences in the distribution between the two target classes.\n",
        "for col in numerical_cols:\n",
        "    sns.histplot(data=data, x=col, kde=True)\n",
        "    plt.show()\n",
        "\n",
        "#lists of columns with specified characteristics\n",
        "stats = data[numerical_cols].describe()\n",
        "print(f\"\\nColumns with mean < median:\\n{stats.columns[stats.loc['mean'] < stats.loc['50%']]}\")\n",
        "print(f\"\\nColumns with mean > median:\\n{stats.columns[stats.loc['mean'] > stats.loc['50%']]}\")\n",
        "print(f\"\\nColumns with big difference between (75th %tile and max) or/and (25th %tile and min values):\\n{stats.columns[((stats.loc['75%'] - stats.loc['max']).abs() > 100) | ((stats.loc['25%'] - stats.loc['min']).abs() > 100)]}\")\n",
        "print(f\"\\nColumns with high standard deviation:\\n{stats.columns[stats.loc['std'] > 100]}\")\n",
        "print(f\"\\nColumns with low standard deviation:\\n{stats.columns[stats.loc['std'] < 0.1]}\")"
      ],
      "metadata": {
        "id": "c81SPIs8InCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create empty lists for positive and negative slopes\n",
        "positive_slope_plots = []\n",
        "negative_slope_plots = []\n",
        "\n",
        "for i in range(len(numerical_cols)):\n",
        "    for j in range(i+1, len(numerical_cols)):\n",
        "        # calculate the slope of the regression line\n",
        "        slope = data[numerical_cols[j]].corr(data[numerical_cols[i]])\n",
        "\n",
        "        # create scatter plot with color-coded points\n",
        "        sns.scatterplot(x=numerical_cols[i], y=numerical_cols[j], data=data)\n",
        "\n",
        "        # add regression line\n",
        "        sns.regplot(x=numerical_cols[i], y=numerical_cols[j], data=data, scatter=False, color=\"black\")\n",
        "\n",
        "        # set plot title and axis labels\n",
        "        plt.title(f\"{numerical_cols[i]} vs {numerical_cols[j]}\")\n",
        "        plt.xlabel(numerical_cols[i])\n",
        "        plt.ylabel(numerical_cols[j])\n",
        "\n",
        "        # append plot and feature pair to the appropriate list based on the sign of the slope\n",
        "        if slope > 0:\n",
        "            positive_slope_plots.append((f\"{numerical_cols[i]} vs {numerical_cols[j]}\", plt))\n",
        "        elif slope < 0:\n",
        "            negative_slope_plots.append((f\"{numerical_cols[i]} vs {numerical_cols[j]}\", plt))\n",
        "\n",
        "        # display plot\n",
        "        plt.show()\n",
        "\n",
        "        # clear current figure to free up memory\n",
        "        plt.clf()\n",
        "\n",
        "# print out the lists of feature pairs\n",
        "print(\"Feature pairs with positive slopes:\")\n",
        "for feature_pair, plot in positive_slope_plots:\n",
        "    print(f\"{feature_pair}: {plot}\")\n",
        "\n",
        "print(\"\\nFeature pairs with negative slopes:\")\n",
        "for feature_pair, plot in negative_slope_plots:\n",
        "    print(f\"{feature_pair}: {plot}\")"
      ],
      "metadata": {
        "id": "8Ulr6wT3JN0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmap style\n",
        "sns.set(style='darkgrid')\n",
        "corrmat = data.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, annot=True, fmt=\".2f\", cmap='coolwarm', vmax=.8, square=True)\n",
        "plt.show()\n",
        "\n",
        "# print highly positively correlated pairs\n",
        "pos_corr_pairs = []\n",
        "for i in range(len(corrmat.columns)):\n",
        "    for j in range(i+1, len(corrmat.columns)):\n",
        "        if abs(corrmat.iloc[i, j]) >= 0.5:\n",
        "            pos_corr_pairs.append((corrmat.columns[i], corrmat.columns[j]))\n",
        "\n",
        "print(\"Highly positively correlated pairs:\")\n",
        "for pair in pos_corr_pairs:\n",
        "    print(pair)\n",
        "\n",
        "# print highly negatively correlated pairs\n",
        "neg_corr_pairs = []\n",
        "for i in range(len(corrmat.columns)):\n",
        "    for j in range(i+1, len(corrmat.columns)):\n",
        "        if abs(corrmat.iloc[i, j]) <=  -0.5:\n",
        "            neg_corr_pairs.append((corrmat.columns[i], corrmat.columns[j]))\n",
        "\n",
        "print(\"Highly negatively correlated pairs:\")\n",
        "for pair in neg_corr_pairs:\n",
        "    print(pair)"
      ],
      "metadata": {
        "id": "u8fC0Lr-Jrhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Categorical features analusis:\n",
        "1. there are no categorical features"
      ],
      "metadata": {
        "id": "nQ1OF3qBKois"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#frequency table of the categorical features\n",
        "\n",
        "for col in categorical_cols:\n",
        "  print(f\"\\n{col}:\")\n",
        "  print(data[col].value_counts())"
      ],
      "metadata": {
        "id": "DiLEFnLiKr4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Data preparation: normalization of data / scaling"
      ],
      "metadata": {
        "id": "zMhLHCsMFL9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions:\n",
        "1.If the data has a Gaussian distribution and outliers, it might be more appropriate to use a method like the Z-score normalization or standardization, which scales the data to have zero mean and unit variance. This method subtracts the mean and divides by the standard deviation of the column to transform the data."
      ],
      "metadata": {
        "id": "mfY44joqMv3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Standatization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_s = StandardScaler().fit(data[numerical_cols])\n",
        "rescaled_d = scaler_s.transform(data[numerical_cols])\n",
        "np.set_printoptions(precision=3)\n",
        "print(rescaled_d)"
      ],
      "metadata": {
        "id": "JC_zWpGwFTIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_normalized_data_df = pd.DataFrame(rescaled_d, columns=numerical_cols)\n",
        "num_normalized_data_df.head(15)"
      ],
      "metadata": {
        "id": "5MQfukCchf_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Feature engineering - basic transformations (nonlinear)"
      ],
      "metadata": {
        "id": "E_LMQshIhAwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions:\n",
        "1. The following features have been added: 'CO(GT)_sq', 'PT08.S1(CO)_sq', 'C6H6(GT)_sq','PT08.S2(NMHC)_sq', 'NOx(GT)_sq', 'PT08.S3(NOx)_sq', 'NO2(GT)_sq', 'PT08.S4(NO2)_sq', 'PT08.S5(O3)_sq', 'T_sq', 'RH_sq', 'AH_sq','Day of week_sq', 'Day number_sq', 'Month number_sq', 'Year_sq','Season_sq', 'Hour_sq', 'Part of day_sq', 'CO(GT)_cube','PT08.S1(CO)_cube', 'C6H6(GT)_cube', 'PT08.S2(NMHC)_cube','NOx(GT)_cube', 'PT08.S3(NOx)_cube', 'NO2(GT)_cube','PT08.S4(NO2)_cube', 'PT08.S5(O3)_cube', 'T_cube', 'RH_cube', 'AH_cube','Day of week_cube', 'Day number_cube', 'Month number_cube', 'Year_cube','Season_cube', 'Hour_cube', 'Part of day_cube', 'CO(GT)_sqrt','PT08.S1(CO)_sqrt', 'C6H6(GT)_sqrt', 'PT08.S2(NMHC)_sqrt','NOx(GT)_sqrt', 'PT08.S3(NOx)_sqrt', 'NO2(GT)_sqrt','PT08.S4(NO2)_sqrt', 'PT08.S5(O3)_sqrt', 'T_sqrt', 'RH_sqrt', 'AH_sqrt','Day of week_sqrt', 'Day number_sqrt', 'Month number_sqrt', 'Year_sqrt','Season_sqrt', 'Hour_sqrt', 'Part of day_sqrt', 'CO(GT)_log','PT08.S1(CO)_log', 'C6H6(GT)_log', 'PT08.S2(NMHC)_log', 'NOx(GT)_log','PT08.S3(NOx)_log', 'NO2(GT)_log', 'PT08.S4(NO2)_log','PT08.S5(O3)_log', 'T_log', 'RH_log', 'AH_log', 'Day of week_log','Day number_log', 'Month number_log', 'Year_log', 'Season_log','Hour_log', 'Part of day_log'"
      ],
      "metadata": {
        "id": "31ywIBB0hPV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply square transformation to the data\n",
        "num_data_squared = np.square(num_normalized_data_df)\n",
        "num_data_squared.columns = [col + '_sq' for col in num_data_squared.columns]\n",
        "\n",
        "# Apply cube transformation to the data\n",
        "num_data_cubed = np.power(num_normalized_data_df, 3)\n",
        "num_data_cubed.columns = [col + '_cube' for col in num_data_cubed.columns]\n",
        "\n",
        "# Apply square root transformation to the data\n",
        "num_data_sqrt = np.sqrt(num_normalized_data_df)\n",
        "num_data_sqrt.columns = [col + '_sqrt' for col in num_data_sqrt.columns]\n",
        "\n",
        "# Apply log transformation to the data\n",
        "num_data_log = np.log(num_normalized_data_df)\n",
        "num_data_log.columns = [col + '_log' for col in num_data_log.columns]\n",
        "\n",
        "# Concatenate the original data with the transformed data\n",
        "num_transformed_data = pd.concat([num_normalized_data_df, num_data_squared, num_data_cubed, num_data_sqrt, num_data_log], axis=1)\n",
        "#num_transformed_data.head(15)\n",
        "num_transformed_data.columns"
      ],
      "metadata": {
        "id": "lL8V9pqHkPGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Baseline model - linear regression without regularization"
      ],
      "metadata": {
        "id": "YRF8BcyVkQj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions:\n",
        "1. The lower the value of the MSE, the better the performance of the model, and the closer the predicted values are to the actual values. This indicates that the linear regression model has performed well in predicting the target variable on the testing data.\n",
        "2. All features with missing data is droped."
      ],
      "metadata": {
        "id": "4CC2psdVkVWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replacedata with the num_transformed_data data and cat_dummies_data\n",
        "#data = pd.concat([cat_dummies_data, num_transformed_data], axis=1)\n",
        "#data.head(15)"
      ],
      "metadata": {
        "id": "GtxGEWq_iBav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable\n",
        "data = num_transformed_data\n",
        "total = data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (data.isna().sum()/data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "u3s_xGZv8QCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(axis=1)\n",
        "data.info()"
      ],
      "metadata": {
        "id": "hWmuwUul8Wz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = data.filter(regex='^(?!.*C6H6\\(GT\\)).*$')  # Select columns that do not contain \"C6H6(GT)\" in their name\n",
        "y = data[\"C6H6(GT)\"] # y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
        "\n",
        "len(y) #check this"
      ],
      "metadata": {
        "id": "9ua2R3DVkYAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a linear regression object\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)"
      ],
      "metadata": {
        "id": "HSq-iBsE6Sg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a regression problem https://www.datacamp.com/tutorial/tutorial-normal-equation-for-linear-regression\n",
        "#X, y = make_regression(\n",
        "#    n_samples=100,\n",
        "#    n_features=2,\n",
        "#    n_informative=2,\n",
        "#    noise = 10,\n",
        "#    random_state=25\n",
        "#    )\n",
        "\n",
        "# Visualize feature at index 1 vs target\n",
        "#plt.subplots(figsize=(8, 5))\n",
        "#plt.scatter(X[:, 1], y, marker='o')\n",
        "#plt.xlabel(\"Feature at Index 1\")\n",
        "#plt.ylabel(\"Target\")\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "PlQMfCnShKlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Metrics chosen as well as reasoning behind each metric"
      ],
      "metadata": {
        "id": "W-WKfoU1pOOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions:\n",
        "1. The mean squared error (MSE) is a measure of the average squared difference between the predicted and actual values. A lower MSE indicates better performance of the model. In this case, the MSE is 1.939847787611042e-05, which is very low and indicates good performance.\n",
        "2. The root mean squared error (RMSE) is the square root of the MSE and provides an estimate of the average deviation of the predictions from the actual values. The RMSE is 0.004404370315506, which is also very low, indicating that the model is making accurate predictions.\n",
        "3. The mean absolute error (MAE) is another measure of the average difference between the predicted and actual values. Like the MSE and RMSE, a lower MAE indicates better performance. The MAE in this case is 0.0036665067855805476, which is very low and indicates good performance.\n",
        "4. The R-squared (R2) score measures the proportion of variance in the dependent variable that can be explained by the independent variables. The R2 score ranges from 0 to 1, with higher values indicating better performance. In this case, the R2 score is 0.99998178102174, which is very high and indicates that the model is able to explain a large proportion of the variance in the dependent variable."
      ],
      "metadata": {
        "id": "W11MNg_9pTpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the model using MSE, RMSE, MAE, and R2\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "r2 = model.score(X_test, y_test)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R2:\", r2)"
      ],
      "metadata": {
        "id": "fnalE5O2pUpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Feature importance, hyperparameters tuning"
      ],
      "metadata": {
        "id": "P_3Zws36qin0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions:\n",
        "1. The output provides the RMSE value of the best model, which is 0.0044. This value represents the root mean squared error of the model's predictions on the test set. The lower the RMSE, the better the model's performance.\n",
        "2. The feature importance is represented by the coefficients of the linear regression model. The coefficients with the largest absolute values are the most important features, positively or negatively impacting the target variable.\n",
        "Looking at the coefficients, we can see that the \"Year\" feature has the largest negative coefficient, followed by \"Year_cube\" and \"Part of day.\" This indicates that these features have the most significant negative impact on the target variable, while \"PT08.S2(NMHC),\" \"PT08.S2(NMHC)_sq,\" and \"PT08.S2(NMHC)_cube\" have the largest positive coefficients and the most significant positive impact on the target variable.\n",
        "3. Comparing the results of both models, we can see that Ridge regression has a slightly better performance than Lasso regression as it has a lower RMSE value. However, both models have produced highly accurate results, with R-squared scores indicating that the models explain the variability of the data almost perfectly.\n",
        "the RMSE values are relatively small, which means that the model's predictions are very close to the actual values. This is a good indication that the model has learned the underlying patterns in the data and can make accurate predictions on new data.\n",
        "  - The Lasso regression has found the best alpha value of 0.001, which results in an R-squared score of 0.999974 and an RMSE of 0.005.\n",
        "  - The Ridge regression has found the best alpha value of 0.1, which results in an R-squared score of 0.999982 and an RMSE of 0.004."
      ],
      "metadata": {
        "id": "_sNSBm89CIJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import seaborn as sns\n",
        "\n",
        "# Get the feature coefficients\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Plot the feature coefficients\n",
        "plt.figure(figsize=(10,6)) # Set the figure size to 10x6\n",
        "sns.barplot(x=X_train.columns, y=coefficients)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Coefficients')\n",
        "plt.title('Linear Regression Coefficients')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g5EjUIyMYa5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune hyperparameters using grid search\n",
        "param_grid = {\"fit_intercept\": [True, False]}\n",
        "grid_search = GridSearchCV(LinearRegression(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Refit the model using the best hyperparameters\n",
        "best_model = LinearRegression(**grid_search.best_params_)\n",
        "best_model.fit(X_train, y_train)\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
        "print(f\"RMSE (best model): {rmse_best}\")\n",
        "\n",
        "# Analyze residuals to check for model assumptions\n",
        "residuals = y_test - y_pred_best\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Residual Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Sort feature coefficients in descending order of absolute magnitude\n",
        "coefficients = best_model.coef_\n",
        "feature_importance = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values('Absolute Coefficient', ascending=False).reset_index(drop=True)\n",
        "feature_importance.drop('Absolute Coefficient', axis=1, inplace=True)\n",
        "\n",
        "# Display feature importance in descending order\n",
        "print('Feature Importance (Descending Order):')\n",
        "display(feature_importance)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=[f[:20] + '...' for f in feature_importance.Feature], y=feature_importance.Coefficient)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Coefficients')\n",
        "plt.title('Linear Regression Coefficients')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ppXYo_qVZrK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso model with regularization using alpha values from GridSearchCV\n",
        "lasso = Lasso()\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
        "lasso_cv = GridSearchCV(lasso, param_grid, cv=5)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "# Fit Lasso model and make predictions on test set\n",
        "lasso_preds = lasso_cv.predict(X_test)\n",
        "\n",
        "# Evaluate Lasso model\n",
        "print(\"\\nLasso Regression Results:\")\n",
        "print(f\"Best alpha value: {lasso_cv.best_params_['alpha']}\")\n",
        "print(f\"R^2 score: {r2_score(y_test, lasso_preds)}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, lasso_preds))}\")\n",
        "\n",
        "# Ridge model with regularization using alpha values from GridSearchCV\n",
        "ridge = Ridge()\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
        "ridge_cv = GridSearchCV(ridge, param_grid, cv=5)\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "\n",
        "# Fit Ridge model and make predictions on test set\n",
        "ridge_preds = ridge_cv.predict(X_test)\n",
        "\n",
        "# Evaluate Ridge model\n",
        "print(\"\\nRidge Regression Results:\")\n",
        "print(f\"Best alpha value: {ridge_cv.best_params_['alpha']}\")\n",
        "print(f\"R^2 score: {r2_score(y_test, ridge_preds)}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, ridge_preds))}\")"
      ],
      "metadata": {
        "id": "kxdt3PLVZ3WS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}