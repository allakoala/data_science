{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "OS77V0vA0eXu",
        "nmYgaXVd0mK4",
        "QR4JeC-a0umG",
        "s_3wm9C60u0F",
        "rYqv4pk50u_A",
        "4y0xfgqR0vHI",
        "O80Bcmxa0vQP",
        "q0wPB-fV0vbh"
      ],
      "authorship_tag": "ABX9TyMulc5KnS9jIY7DnfZi19Hr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/HW_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW: https://docs.google.com/document/d/1pMJgIkYXNGxT6ZMpvQuyz-qY6o9QByAf/edit\n",
        "\n",
        "Previous hw: https://colab.research.google.com/drive/1r-J7BcwY1E-deeWoe1CaDoGvK4s8LCif#scrollTo=qcAWHBpVZ_LH\n",
        "\n",
        "\n",
        "\n",
        "1.   Goal: Predict the bike renting studied in previous homework using Time Series techniques.\n",
        "2.   Data: The same data as for homework on Advanced regression\n",
        "3.   Target: `cnt` - Number of bikes rented per hour.\n",
        "4.   Endogenous variables: All variables engineered from target and time\n",
        "5.   Exogenous variables: Other than endogenous (e.g., something about weather)\n",
        "6.   Test sample: last month of data\n",
        "7.   Metrics: MAE\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bF3CNQajhiro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EDA using Time Series Analysis techniques:\n",
        "1. According to the TS count plot, the count of total rental bikes plotted through time makes a double bell-like graph, showing the same overall shape, and an increase in bike rental in 2012. So the time series shows a general tendency of rental increasing in the first two seasons then decreases in the last two seasons. The time series also shows a repeating short-term cycle through the two years.\n",
        "\n",
        "2. According to the correlation matrix:\n",
        "  * We observe a highly positive correlation between 'temp' and 'atemp' and between 'casual' and 'registered'.\n",
        "  * 'Windspeed' displays an insignificant contribution to the count.\n",
        "  * Hence, we will drop a few unnecessary columns later.\n",
        "\n",
        "3. The seasonal component confirms our hypothesis, and shows up even a more interesting seasonal monthly cycle - which could be removed\n",
        "\n",
        "4. Based on these results, we can conclude that the \"cnt\" time series appears to be stationary. Stationary time series exhibit consistent statistical properties over time, which is desirable for many time series analysis techniques:\n",
        "\n",
        "    *   ADF Statistic: -6.822918711895098 - The ADF statistic is significantly below the critical values, indicating strong evidence against the null hypothesis of non-stationarity. This suggests that the \"cnt\" series is likely stationary.\n",
        "    *   p-value: 1.9808626277977946e-09 - The extremely low p-value (close to zero) further supports the rejection of the null hypothesis. It suggests that the \"cnt\" series is unlikely to be non-stationary.\n",
        "    Critical Values:\n",
        "    *   The ADF statistic is lower than all critical values at the 1%, 5%, and 10% significance levels. This provides additional evidence that the \"cnt\" series is stationary.\n",
        "\n",
        "5. The seasonal sub series plot can be more informative when redrawn with seasonal box plots. The box plot displays both central tendency and dispersion\n",
        "within the seasonal data over a batch of time units.\n",
        "\n",
        "6. To determine if the deseasonal_cnt series is stationary, should be performed a stationarity test. One commonly used test is the Augmented Dickey-Fuller (ADF) test. Overall, based on the ADF test results, we can conclude that the \"deseasonal_cnt\" series is stationary. This indicates that the seasonal component has been effectively removed, and the series exhibits no significant trends or patterns over time:\n",
        "\n",
        "    *   To make the time series stationary, weâ€™ll be using Differencing. Differencing is a process of subtracting each data point in the series from its successor.\n",
        "    *   ADF Statistic: The ADF statistic value of -7.106123757516585 indicates a significant level of stationarity in the \"deseasonal_cnt\" series. A more negative ADF statistic suggests a stronger rejection of the null hypothesis of non-stationarity.\n",
        "    *   p-value: The p-value of 4.0494746104502425e-10 is extremely small, indicating strong evidence against the null hypothesis of non-stationarity. The small p-value suggests that the \"deseasonal_cnt\" series is likely stationary.\n",
        "    *   Critical Values: The ADF test also provides critical values at different significance levels (1%, 5%, and 10%) to compare with the ADF statistic. In this case, all critical values are more negative than the ADF statistic, further supporting the rejection of the null hypothesis. These critical values serve as thresholds for determining the stationarity of the series.\n"
      ],
      "metadata": {
        "id": "OS77V0vA0eXu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoM4YEqohd61"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#path of the file to read\n",
        "url = \"/content/drive/MyDrive/Colab Notebooks/data/regression advanced/hour.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=',')\n",
        "\n",
        "#examine the data\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Create the df\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert 'dteday' column to datetime type\n",
        "df['dteday'] = pd.to_datetime(df['dteday'])\n",
        "\n",
        "# Set 'dteday' as the DataFrame index\n",
        "df.set_index('dteday', inplace=True)\n",
        "\n",
        "# Sort the data by 'dteday' column\n",
        "df = df.sort_values('dteday')\n",
        "\n",
        "# Smooth the 'cnt' column using a 10-hour moving average\n",
        "df['cnt_smoothed'] = df['cnt'].rolling(window=10, center=True, min_periods=1).mean()\n",
        "\n",
        "# Print the dataframe with desired columns\n",
        "print(df[['cnt', 'cnt_smoothed']])\n",
        "\n",
        "# Perform time series decomposition\n",
        "result = seasonal_decompose(df['cnt'], model='additive', period=1) # 365*24\n",
        "\n",
        "# Plot the time series data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index, df['cnt'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Time Series of Count')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index, df['cnt_smoothed'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Smoothed Time Series of Count')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df.index, df['cnt'], label='Original')\n",
        "plt.plot(df.index, df['cnt_smoothed'], label='Smoothed')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Smoothed Time Series')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nqg6B7V7mHCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decomposition components\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.plot(result.trend)\n",
        "plt.ylabel('Trend')\n",
        "plt.title('Decomposition: Trend')\n",
        "\n",
        "plt.subplot(4, 1, 2)\n",
        "plt.plot(result.seasonal)\n",
        "plt.ylabel('Seasonal')\n",
        "plt.title('Decomposition: Seasonal')\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "plt.plot(result.resid)\n",
        "plt.ylabel('Residual')\n",
        "plt.title('Decomposition: Residual')\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "plt.plot(result.observed)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Observed')\n",
        "plt.title('Decomposition: Observed')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Perform correlation analysis\n",
        "correlation_matrix = df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
        "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Perform stationarity test\n",
        "def adf_test(series):\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    print('Critical Values:')\n",
        "    for key, value in result[4].items():\n",
        "        print(f'   {key}: {value}')\n",
        "\n",
        "print('ADF Test for \"cnt\":')\n",
        "adf_test(df['cnt'])"
      ],
      "metadata": {
        "id": "JStzHNh3b5NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Generate seasonal sub-series plots\n",
        "seasons = df.groupby(df['season'])['cnt'].apply(list)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for season, values in seasons.items():\n",
        "    plt.plot(values, label=f'Season {season}')\n",
        "plt.xlabel('Time Units')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Seasonal Sub-Series Plot')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Generate seasonal box plots using seaborn\n",
        "plt.figure(figsize=(12, 8))\n",
        "df['season_name'] = df['season'].map({1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'})\n",
        "sns.boxplot(x=df['season_name'], y=df['cnt'])\n",
        "plt.xlabel('Season')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Seasonal Box Plots')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3QUjmgYoG18K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the deseasonalized component\n",
        "deseasonal_cnt = df['cnt'] - result.seasonal\n",
        "\n",
        "# Plot the deseasonalized time series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index, deseasonal_cnt)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Deseasonalized Count')\n",
        "plt.title('Deseasonalized Time Series of Count')\n",
        "plt.show()\n",
        "\n",
        "# Print the first few rows of the deseasonalized time series\n",
        "print(deseasonal_cnt.head())"
      ],
      "metadata": {
        "id": "7zpAr0ZBd7K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Perform stationarity test\n",
        "def adf_test(series):\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    print('Critical Values:')\n",
        "    for key, value in result[4].items():\n",
        "        print(f'   {key}: {value}')\n",
        "    return result\n",
        "\n",
        "# Perform ADF test on the deseasonal_cnt series\n",
        "print('ADF Test for \"deseasonal_cnt\":')\n",
        "adf_result = adf_test(deseasonal_cnt)\n",
        "\n",
        "# Make the series stationary\n",
        "if adf_result[1] > 0.05:\n",
        "    stationary_cnt = deseasonal_cnt.diff().dropna()  # Take the difference to make it stationary\n",
        "else:\n",
        "    stationary_cnt = deseasonal_cnt  # The series is already stationary\n",
        "\n",
        "# Add stationary_cnt and deseasonal_cnt to df dataset\n",
        "df['stationary_cnt'] = stationary_cnt\n",
        "df['deseasonal_cnt'] = deseasonal_cnt\n",
        "\n",
        "# Plot the stationary time series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index, stationary_cnt)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stationary Count')\n",
        "plt.title('Stationary Time Series of Count')\n",
        "plt.show()\n",
        "\n",
        "# Print the first few rows of the stationary time series\n",
        "print(stationary_cnt.head())"
      ],
      "metadata": {
        "id": "UORqYMhcfJ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline model using Linear Regression keeping in data only endogenous feature:\n",
        "\n",
        "**Baseline Model using Linear Regression (keeping only endogenous feature):**\n",
        "\n",
        "- Mean Squared Error (MSE): 1.55e-26\n",
        "- R-squared: 1.0\n",
        "- Mean Absolute Error (MAE): 9.16e-13\n",
        "\n",
        "The baseline model demonstrates outstanding performance. The extremely low MSE indicates that the model's predictions closely align with the actual values, exhibiting minimal error. The perfect R-squared score suggests that the model explains all the variance in the target variable using the selected endogenous features. Additionally, the very low MAE confirms that, on average, the model's predictions are remarkably close to the true values, with minimal absolute deviation.\n",
        "\n",
        "**Baseline Model Improvement using Results of Target Decomposition and Exogenous Features:**\n",
        "\n",
        "- Mean Squared Error (MSE): 5.48e-27\n",
        "- R-squared: 1.0\n",
        "- Mean Absolute Error (MAE): 5.51e-14\n",
        "\n",
        "The improved model showcases a significant enhancement over the baseline model. The even lower MSE indicates a further reduction in prediction errors, making the model even more accurate. The perfect R-squared score demonstrates that the model captures all the variance in the target variable by utilizing both the endogenous and exogenous features. Moreover, the extremely low MAE affirms that the model's predictions are exceptionally close to the true values, exhibiting outstanding precision.\n",
        "\n",
        "In conclusion, both the baseline model and the improved model exhibit exceptional performance. They provide highly accurate predictions with minimal errors. The inclusion of exogenous features and leveraging the results of target decomposition have significantly enhanced the models' predictive capabilities. These findings establish the reliability and precision of the models for making accurate predictions on the given dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nmYgaXVd0mK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Select endogenous features\n",
        "endogenous_features = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'casual', 'registered'] # delete target features\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[endogenous_features], df['stationary_cnt'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Baseline model using Linear Regression keeping only endogenous feature:\")\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "id": "smo9tRQW0r0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Baseline model improvement using results of target decomposition and exogenous features\n"
      ],
      "metadata": {
        "id": "QR4JeC-a0umG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Extract the trend component from decomposition\n",
        "df['trend'] = result.trend\n",
        "\n",
        "# Add exogenous features\n",
        "exogenous_features = ['weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
        "\n",
        "# Combine endogenous and exogenous features\n",
        "features = exogenous_features + ['trend']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df['stationary_cnt'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Baseline model improvement using results of target decomposition and exogenous features:\")\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "id": "TlcxjoLy0umG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train ARIMA model:\n",
        "1. Mean Absolute Error (MAE): The MAE value of 56.88 indicates that, on average, the ARIMA model's predictions deviate from the actual values by approximately 56.88 units. Lower MAE values suggest better predictive accuracy, so this value could be considered moderately high.\n",
        "\n",
        "2. AR Coefficients: The AR coefficients [-0.041, -0.152, 0.586, -0.103, -0.181, 0.042, -0.181, 0.099, -0.051, -0.051, -0.090] represent the autoregressive components of the ARIMA model. These coefficients determine the influence of previous observations on the current value. The coefficients can be interpreted as follows: for each unit increase in the corresponding lagged observation, the current value is expected to change by the coefficient value.\n",
        "\n",
        "3. MA Coefficients: The MA coefficients [-0.531, -0.017, -0.839, 0.504, -0.079] represent the moving average components of the ARIMA model. These coefficients capture the dependency between the residual errors and the lagged forecast errors. Similar to the AR coefficients, these values indicate the impact of past errors on the current prediction.\n",
        "\n",
        "4. Residual Analysis: The summary statistics of the residuals reveal important characteristics of the model's performance. The mean residual value of 0.082 indicates a slight positive bias, suggesting that, on average, the model tends to underestimate the true values. The standard deviation of 138.50 reflects the dispersion of the residuals around the mean, indicating a significant amount of variability. The minimum and maximum residuals are -516.22 and 707.06, respectively, suggesting occasional large prediction errors. The quartiles provide insights into the distribution of residuals, with the median (50th percentile) at -21.55 and the interquartile range (25th to 75th percentile) ranging from -81.44 to 65.89.\n",
        "\n",
        "5. In conclusion, the refitted ARIMA model shows a moderate Mean Absolute Error, suggesting that it captures a significant portion of the underlying patterns in the data :\n",
        "\n",
        "    * Mean Absolute Error (MAE): The MAE of 55.372 suggests that, on average, the refitted ARIMA model's predictions deviate by approximately 55.37 units from the actual values. Lower MAE values indicate better model accuracy.\n",
        "    * AR Coefficients: The AR coefficients represent the weights assigned to the past observations in the autoregressive component of the model. The values [-0.017, -0.182, 0.577, -0.078, -0.204, 0.056, -0.175, 0.098, -0.031, -0.068, -0.056] indicate the magnitude and direction of the influence of each lagged observation on the current value. Positive coefficients suggest a positive relationship, while negative coefficients imply an inverse relationship.\n",
        "    * MA Coefficients: The MA coefficients represent the weights assigned to the past forecast errors in the moving average component of the model. The values [-0.556, 0.058, -0.877, 0.482, -0.081] indicate the impact of the forecast errors at different lags on the current value. These coefficients capture any remaining dependency in the errors.\n",
        "    * Residual Analysis: The summary statistics of the residuals indicate that they have a mean close to zero (0.101) with a standard deviation of 138.52. The minimum and maximum residuals are -519.32 and 683.52, respectively. The interquartile range (IQR) spans from -82.01 to 66.39, suggesting that most of the residuals lie within this range.\n",
        "    * AIC and BIC: The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are used to compare models based on their goodness of fit and complexity. The initial model has an AIC of 220,747.06 and a BIC of 220,879.03, while the refitted model has a slightly higher AIC of 220,764.62 and BIC of 220,896.59. Lower AIC and BIC values indicate a better trade-off between model fit and complexity."
      ],
      "metadata": {
        "id": "s_3wm9C60u0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Select the relevant columns for the time series analysis\n",
        "ts_data = df[['stationary_cnt']]\n",
        "ts_data.index = pd.to_datetime(df.index)\n",
        "\n",
        "# Perform autocorrelation and partial autocorrelation analysis\n",
        "plot_acf(ts_data['stationary_cnt'], lags=50)\n",
        "plt.show()\n",
        "\n",
        "plot_pacf(ts_data['stationary_cnt'], lags=50)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GbFDYPOdmNwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the ARIMA coefficients based on the analysis\n",
        "p = 11  # Number of AR terms determined from the PACF plot\n",
        "d = 1  # Number of differences (0 if the data is already stationary)\n",
        "q = 5  # Number of MA terms determined from the ACF plot\n",
        "\n",
        "# Set n as the length of the last month of data\n",
        "last_month = ts_data.index[-1].to_period('M').month\n",
        "second_last_month = ts_data.index[-2].to_period('M').month\n",
        "n = last_month - second_last_month + 1\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data = ts_data.iloc[:-n]\n",
        "test_data = ts_data.iloc[-n:]\n",
        "\n",
        "# Fit ARIMA model\n",
        "model = ARIMA(train_data, order=(p, d, q))\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.predict(start=test_data.index[0], end=test_data.index[-1])\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(test_data['stationary_cnt'], predictions)\n",
        "\n",
        "# Analyze residuals\n",
        "residuals = model_fit.resid\n",
        "\n",
        "# Explanation of ARIMA coefficients\n",
        "ar_coef = model_fit.arparams\n",
        "ma_coef = model_fit.maparams\n",
        "\n",
        "# Print the results\n",
        "print(\"ARIMA model:\")\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"AR Coefficients: {ar_coef}\")\n",
        "print(f\"MA Coefficients: {ma_coef}\")\n",
        "print(\"Residual Analysis:\")\n",
        "print(residuals.describe())"
      ],
      "metadata": {
        "id": "JBBV0MF5n3Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.graphics.tsaplots as tsaplots\n",
        "\n",
        "# Residual analysis\n",
        "print(\"Residual Analysis:\")\n",
        "print(residuals.describe())\n",
        "\n",
        "# Plot ACF and PACF of the residuals\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
        "tsaplots.plot_acf(residuals, lags=20, ax=axes[0])\n",
        "tsaplots.plot_pacf(residuals, lags=20, ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Refit the model using the entire dataset\n",
        "model_refit = ARIMA(ts_data, order=(p, d, q))\n",
        "model_fit_refit = model_refit.fit()\n",
        "\n",
        "# Make predictions on the entire dataset\n",
        "predictions_refit = model_fit_refit.predict(start=test_data.index[0], end=test_data.index[-1])\n",
        "\n",
        "# Calculate MAE for the refit model\n",
        "mae_refit = mean_absolute_error(test_data['stationary_cnt'], predictions_refit)\n",
        "\n",
        "# Print the results\n",
        "print(\"Refitted ARIMA model:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_refit}\")\n",
        "print(f\"AR Coefficients: {model_fit_refit.arparams}\")\n",
        "print(f\"MA Coefficients: {model_fit_refit.maparams}\")\n",
        "print(\"Residual Analysis:\")\n",
        "print(model_fit_refit.resid.describe())\n",
        "\n",
        "# Compare fit criteria such as AIC or BIC\n",
        "print(f\"AIC: {model_fit.aic}\")\n",
        "print(f\"BIC: {model_fit.bic}\")\n",
        "print(f\"Refitted AIC: {model_fit_refit.aic}\")\n",
        "print(f\"Refitted BIC: {model_fit_refit.bic}\")"
      ],
      "metadata": {
        "id": "j6YXnRkhUAZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prophet and SARIMAX."
      ],
      "metadata": {
        "id": "O80Bcmxa0vQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Preprocess the data for Prophet\n",
        "prophet_df = df.copy()\n",
        "prophet_df.reset_index(inplace=True)\n",
        "prophet_df = prophet_df[['dteday', 'cnt']].copy()\n",
        "prophet_df.columns = ['ds', 'y']\n",
        "prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])\n",
        "\n",
        "# Fit Prophet model\n",
        "prophet_model = Prophet(daily_seasonality=False)  # Disabling daily seasonality # add own seasonality\n",
        "prophet_model.fit(prophet_df)\n",
        "\n",
        "# Make future predictions with Prophet\n",
        "future = prophet_model.make_future_dataframe(periods=365)  # Extend the dataframe by 365 days\n",
        "prophet_forecast = prophet_model.predict(future)\n",
        "\n",
        "# Preprocess the data for SARIMAX\n",
        "sarimax_df = prophet_df.copy()\n",
        "sarimax_df.rename(columns={'dteday': 'ds', 'cnt': 'y'}, inplace=True)\n",
        "sarimax_df.set_index('ds', inplace=True)\n",
        "\n",
        "# Fit SARIMAX model\n",
        "sarimax_model = SARIMAX(sarimax_df, order=(1, 0, 1), seasonal_order=(1, 1, 1, 24)) # seasonality - weekly, 24*365\n",
        "sarimax_model_fit = sarimax_model.fit()\n",
        "\n",
        "# Make predictions with SARIMAX\n",
        "sarimax_forecast = sarimax_model_fit.forecast(steps=365)\n",
        "\n",
        "# Print the forecasted values\n",
        "print(\"Prophet Forecast:\")\n",
        "print(prophet_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
        "\n",
        "print(\"\\nSARIMAX Forecast:\")\n",
        "print(sarimax_forecast.tail())\n"
      ],
      "metadata": {
        "id": "Zcj8EDfC0vQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Prophet Forecast\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.plot(prophet_forecast['ds'], prophet_forecast['yhat'], label='Forecast')\n",
        "ax.fill_between(prophet_forecast['ds'], prophet_forecast['yhat_lower'], prophet_forecast['yhat_upper'], alpha=0.3, color='gray', label='Confidence Interval')\n",
        "ax.scatter(prophet_df['ds'], prophet_df['y'], color='red', label='Actual')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Prophet Forecast')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plotting SARIMAX Forecast\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.plot(sarimax_forecast.index, sarimax_forecast.values, label='Forecast')\n",
        "ax.scatter(sarimax_df.index, sarimax_df['y'].values, color='red', label='Actual')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('SARIMAX Forecast')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N5Dmd_1m5k9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced regression model with the time-based features engineered for linear model\n",
        "1. Mean Absolute Error (MAE): The MAE value of 104.92 indicates the average absolute difference between the predicted values and the actual values. A lower MAE indicates better model performance, so the model's predictions are relatively close to the actual values on average.\n",
        "\n",
        "2. Coefficients: The coefficients represent the estimated impact of each feature on the target variable (cnt). Some key observations include:\n",
        "\n",
        "3. The features with the highest positive coefficients are 'atemp' (200.13), 'temp' (103.26), and 'yr' (82.80). These variables have a positive relationship with the target variable, meaning an increase in these features tends to correspond to an increase in the bike count.\n",
        "The features with the highest negative coefficients are 'hum' (-200.40) and 'weathersit' (-3.29). These variables have a negative relationship with the target variable, indicating that an increase in these features is associated with a decrease in the bike count.\n",
        "\n",
        "4. Feature Importance: The feature importance indicates the relative importance of each feature in the model. The importance values can help prioritize feature selection or further analysis. Key observations include: The features 'mnth' and 'month' have the highest importance values of 2.73e+13. This suggests that the month of the year is highly influential in predicting the bike count.\n",
        "Other important features include 'hum' (200.40), 'atemp' (200.13), and 'temp' (103.26), indicating the significance of weather-related variables.\n",
        "Prediction for New Data: The predicted bike count for new data is 177.73. This prediction can be used as an estimate for the bike count given the input variables provided.\n",
        "\n",
        "5. Overall, the model provides insights into the relationship between the features and the bike count. It suggests that factors such as temperature, humidity, month, and weather conditions play a significant role in predicting the bike count.\n",
        "\n"
      ],
      "metadata": {
        "id": "q0wPB-fV0vbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Engineer time-based features\n",
        "data['dteday'] = pd.to_datetime(data['dteday'])  # Convert the 'dteday' column to datetime\n",
        "data['hour'] = data['dteday'].dt.hour  # Extract the hour from the datetime\n",
        "data['month'] = data['dteday'].dt.month  # Extract the month from the datetime\n",
        "data['weekday'] = data['dteday'].dt.weekday  # Extract the weekday from the datetime\n",
        "\n",
        "# Define the features and target variable\n",
        "features = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp',\n",
        "            'atemp', 'hum', 'windspeed', 'hour', 'month']  # Add or modify features as needed\n",
        "target = 'cnt'  # Replace 'cnt' with the actual target variable name\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "# Additional analysis or model refinement can be performed here\n",
        "\n",
        "# Example: Coefficient interpretation\n",
        "coefficients = pd.DataFrame({'Feature': features, 'Coefficient': model.coef_})\n",
        "print(coefficients)\n",
        "\n",
        "# Example: Feature importance analysis\n",
        "importance = pd.DataFrame({'Feature': features, 'Importance': np.abs(model.coef_)})\n",
        "importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
        "print(importance)\n",
        "\n",
        "# Example: Model prediction on new data\n",
        "new_data = pd.DataFrame({'season': [2], 'yr': [0], 'mnth': [6], 'hr': [8], 'holiday': [0], 'weekday': [2],\n",
        "                         'workingday': [1], 'weathersit': [1], 'temp': [0.7], 'atemp': [0.65], 'hum': [0.6],\n",
        "                         'windspeed': [0.25], 'hour': [8], 'month': [6]})\n",
        "new_prediction = model.predict(new_data)\n",
        "print(\"Prediction for new data:\", new_prediction)\n"
      ],
      "metadata": {
        "id": "TeYqm9aLv_ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare obtained results with ones obtained using Advanced Regression techniques in the previous homework where it is possible."
      ],
      "metadata": {
        "id": "4y0xfgqR0vHI"
      }
    }
  ]
}