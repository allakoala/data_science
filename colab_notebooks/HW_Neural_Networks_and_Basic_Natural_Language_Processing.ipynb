{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "j_p9hexEOyRP",
        "E4dNiFgyWr-L",
        "4rUOOXgqWzHa",
        "WFsSsZ1D4QIX",
        "abWbQpyBLsvs",
        "UlneMGZvLxy0",
        "5xuqTG7JlAbC"
      ],
      "authorship_tag": "ABX9TyNodsABFPRzVGcrPkuLBC8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/HW_Neural_Networks_and_Basic_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW - https://docs.google.com/document/d/1RXVGCi56qaWzC2SuNGk8gZXicSnl1eZb8_MqWvW9Igg/edit"
      ],
      "metadata": {
        "id": "jgxZ-weifW_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EDA + Preprocessing"
      ],
      "metadata": {
        "id": "j_p9hexEOyRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. [Previous HW on Clustering](https://colab.research.google.com/drive/1udWpcOae_qcEB-Crj2KaXAafaDt27iZ3#scrollTo=StbTKheJkX2S&uniqifier=5)"
      ],
      "metadata": {
        "id": "E4dNiFgyWr-L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFPkr0utfMrU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#path of the file to read\n",
        "url = \"/content/drive/MyDrive/Colab Notebooks/LargeMovieReviewDataset.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=',')\n",
        "\n",
        "#examine the data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number\n",
        "!pip install contractions\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "UpCOKbi4zuPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from word2number import w2n #!pip install word2number\n",
        "import contractions #!pip install contractions\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import unidecode #!pip install unidecode\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = {word for word in stopwords.words('english') if word not in {'no', 'not'}}\n",
        "\n",
        "def clean_text(documents):\n",
        "    # Remove HTML tags\n",
        "    documents = [BeautifulSoup(doc, \"html.parser\").get_text(separator=\" \") for doc in documents]\n",
        "\n",
        "    # Remove accented characters from text, e.g. caf√©\n",
        "    documents = [unidecode.unidecode(doc) for doc in documents]\n",
        "\n",
        "    # Expand contractions\n",
        "    documents = [contractions.fix(doc) for doc in documents]\n",
        "\n",
        "    # Convert number words to numeric form\n",
        "    new_documents = []\n",
        "    for doc in documents:\n",
        "        words = []\n",
        "        for word in doc.split():\n",
        "            if word.isalpha():\n",
        "                try:\n",
        "                    num = w2n.word_to_num(word)\n",
        "                    words.append(str(num))\n",
        "                except ValueError:\n",
        "                    words.append(word)\n",
        "            else:\n",
        "                words.append(word)\n",
        "        new_documents.append(' '.join(words))\n",
        "    documents = new_documents\n",
        "\n",
        "    # Remove numbers\n",
        "    documents = [re.sub(r'\\b\\d+\\b', '', doc) for doc in documents]\n",
        "\n",
        "    # Remove leading and ending spaces\n",
        "    documents = [\" \".join(doc.strip().split()) for doc in documents]\n",
        "\n",
        "    # Remove punctuation\n",
        "    documents = [doc.translate(str.maketrans('', '', string.punctuation)) for doc in documents]\n",
        "\n",
        "    # Convert to lowercase\n",
        "    documents = [doc.lower() for doc in documents]\n",
        "\n",
        "    # Tokenize text and count the number of words in the corpus\n",
        "    tokens = [nltk.word_tokenize(doc) for doc in documents]\n",
        "    corpus_size_words = np.sum([len(d) for d in tokens])\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [[token for token in doc_tokens if token not in stop_words] for doc_tokens in tokens]\n",
        "\n",
        "    # Print descriptive statistics\n",
        "    corpus_size_docs = len(documents)\n",
        "    sentiment_distr = Counter(data['sentiment'])\n",
        "    print('Corpus Size (Number of Documents): {}'.format(corpus_size_docs))\n",
        "    print('Corpus Size (Number of Words): {}'.format(corpus_size_words))\n",
        "    print('Sentiment Distribution: {}'.format(sentiment_distr))\n",
        "\n",
        "    return [' '.join(filtered_doc_tokens) for filtered_doc_tokens in filtered_tokens]\n",
        "\n",
        "data['review_cleaned'] = clean_text(data['review'])\n",
        "data['review_cleaned']"
      ],
      "metadata": {
        "id": "pVClZVeVz5Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Normalization 1 - lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text1(text):\n",
        "    # Tokenize words\n",
        "    text = nltk.word_tokenize(text)\n",
        "    # Lemmatize words\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    return text\n",
        "\n",
        "data['review_normalized1'] = data['review_cleaned'].apply(normalize_text1)\n",
        "\n",
        "# concatenate the lists of normalized words for all reviews\n",
        "normalized1 = []\n",
        "for review in data['review_normalized1']:\n",
        "    normalized1.extend(review)\n",
        "\n",
        "print(normalized1[:7]) # print first 7 words"
      ],
      "metadata": {
        "id": "sWa08zRB0Mcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert sentiment to binary (0 or 1)\n",
        "data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
        "#data[['review_normalized1','review_normalized2','sentiment']].head()\n",
        "data[['review_normalized1','sentiment']]"
      ],
      "metadata": {
        "id": "PkBddyyy0QEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. According to [the article](https://towardsdatascience.com/deep-learning-for-natural-language-processing-using-word2vec-keras-d9a240c7bb9d)"
      ],
      "metadata": {
        "id": "4rUOOXgqWzHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words"
      ],
      "metadata": {
        "id": "ji8sO-D-xsaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "# import os\n",
        "# os._exit(00)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from num2words import num2words #pip install num2words\n",
        "\n",
        "url = \"/content/drive/MyDrive/Colab Notebooks/LargeMovieReviewDataset.csv\"\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.data = None\n",
        "        self.feature_name = 'review'\n",
        "        self.target_name = 'sentiment'\n",
        "        self.target_encoding = None\n",
        "\n",
        "    def read_input_file(self):\n",
        "        \"\"\"\n",
        "        Reads input file from given filepath\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(self.filepath)\n",
        "        return self.data\n",
        "\n",
        "    def encode_target(self):\n",
        "        \"\"\"\n",
        "        Encodes the target column using factorize method\n",
        "        \"\"\"\n",
        "        self.data[self.target_name], self.target_encoding = pd.factorize(self.data[self.target_name])\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_html_tags(text):\n",
        "        \"\"\"\n",
        "        Removes HTML tags from the text\n",
        "        \"\"\"\n",
        "        clean = re.compile('<.*?>')\n",
        "        return re.sub(clean, '', text)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_accented_characters(text):\n",
        "        \"\"\"\n",
        "        Removes accented characters from the text\n",
        "        \"\"\"\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def expand_contractions(text):\n",
        "        \"\"\"\n",
        "        Expands contractions in the text\n",
        "        \"\"\"\n",
        "        return contractions.fix(text)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_number_words_to_numeric(text):\n",
        "        \"\"\"\n",
        "        Converts number words to numeric in the text\n",
        "        \"\"\"\n",
        "        try:\n",
        "            number = float(text)\n",
        "            return num2words(number, lang='en')\n",
        "        except ValueError:\n",
        "            return text\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_numbers(text):\n",
        "        \"\"\"\n",
        "        Removes numbers from the text\n",
        "        \"\"\"\n",
        "        return re.sub(r'\\d+', '', text)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_punctuation(text):\n",
        "        \"\"\"\n",
        "        Removes punctuation from the text\n",
        "        \"\"\"\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    @staticmethod\n",
        "    def lowercase(text):\n",
        "        \"\"\"\n",
        "        Converts text to lowercase\n",
        "        \"\"\"\n",
        "        return text.lower()\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        \"\"\"\n",
        "        Tokenizes the text\n",
        "        \"\"\"\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_stopwords(tokens):\n",
        "        \"\"\"\n",
        "        Removes stopwords from the tokens\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english')) - {'no', 'not'}\n",
        "        return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    @staticmethod\n",
        "    def lemmatize(word):\n",
        "        \"\"\"\n",
        "        Lemmatizes the word\n",
        "        \"\"\"\n",
        "        wordnet_pos = Preprocessor.get_wordnet_pos(word)\n",
        "        return WordNetLemmatizer().lemmatize(word, pos=wordnet_pos)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_wordnet_pos(word):\n",
        "        \"\"\"\n",
        "        Maps POS tag to first character used by WordNetLemmatizer\n",
        "        \"\"\"\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    def apply_preprocessing(self):\n",
        "        \"\"\"\n",
        "        Applies all preprocessing steps to the data\n",
        "        \"\"\"\n",
        "        self.encode_target()\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.remove_html_tags)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.remove_accented_characters)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.expand_contractions)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.convert_number_words_to_numeric)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.remove_numbers)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.remove_punctuation)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.lowercase)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.tokenize)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(self.remove_stopwords)\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(lambda x: [self.lemmatize(word) for word in x])\n",
        "        self.data[self.feature_name] = self.data[self.feature_name].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    def save_cleaned_data(self, filepath=\"/content/drive/MyDrive/Colab Notebooks/Cleaned_MovieReviewDataset.csv\"):\n",
        "        \"\"\"\n",
        "        Saves the cleaned data to a CSV file\n",
        "        \"\"\"\n",
        "        self.data.to_csv(filepath, index=False)\n",
        "\n",
        "    def load(self, filepath=\"/content/drive/MyDrive/Colab Notebooks/Cleaned_MovieReviewDataset.csv\"):\n",
        "        \"\"\"\n",
        "        Loads the cleaned data from a CSV file\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(filepath)\n",
        "        return self.data\n",
        "\n",
        "# Create a Preprocessing object\n",
        "preprocessor = Preprocessor(url)\n",
        "\n",
        "# Read the input file\n",
        "preprocessor.read_input_file()\n",
        "\n",
        "# Apply preprocessing steps\n",
        "preprocessor.apply_preprocessing()\n",
        "\n",
        "# Save the preprocessed file to a new location\n",
        "preprocessor.save_cleaned_data()\n",
        "\n",
        "# Load the cleaned data\n",
        "preprocessor.load()"
      ],
      "metadata": {
        "id": "3fedi_7p0V6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train linear model based on TF-IDF as a baseline\n",
        "https://realpython.com/python-keras-text-classification/"
      ],
      "metadata": {
        "id": "WFsSsZ1D4QIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Cleaned_MovieReviewDataset.csv')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Train a logistic regression model on the TF-IDF features\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Transform the test data using the same vectorizer\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Predict the test data using the trained model\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model performance on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"The baseline linear model based on TF-IDF Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "K5RdkgE44PJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import learning_curve\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Plot the confusion matrix & report\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Print the classification report\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot the learning curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(model, X_train_tfidf, y_train, cv=5)\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "train_std = train_scores.std(axis=1)\n",
        "test_mean = test_scores.mean(axis=1)\n",
        "test_std = test_scores.std(axis=1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_sizes, train_mean, label='Training Accuracy')\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
        "plt.plot(train_sizes, test_mean, label='Validation Accuracy')\n",
        "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Training Examples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SQNgtaNgXU6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', SGDClassifier(alpha=0.001))\n",
        "])\n",
        "\n",
        "# Fit pipeline on training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Extract the coefficients of the model from the pipeline\n",
        "importances = pipeline.named_steps['clf'].coef_.flatten()\n",
        "\n",
        "# Get featnames from tfidfvectorizer\n",
        "feature_names = np.array(pipeline.named_steps['tfidf'].get_feature_names_out())\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'FEATURE': feature_names,\n",
        "    'IMPORTANCE': importances,\n",
        "    'SENTIMENT': ['pos' if importance >= 0 else 'neg' for importance in importances]\n",
        "})\n",
        "\n",
        "# Select top 10 positive/negative features\n",
        "top_pos_features = feature_importance_df[feature_importance_df['SENTIMENT'] == 'pos'].nlargest(10, 'IMPORTANCE')\n",
        "top_neg_features = feature_importance_df[feature_importance_df['SENTIMENT'] == 'neg'].nsmallest(10, 'IMPORTANCE')\n",
        "\n",
        "# Generate WordCloud for positive features\n",
        "positive_text = ' '.join(top_pos_features['FEATURE'])\n",
        "positive_wordcloud = WordCloud(width=800, height=400).generate(positive_text)\n",
        "\n",
        "# Generate WordCloud for negative features\n",
        "negative_text = ' '.join(top_neg_features['FEATURE'])\n",
        "negative_wordcloud = WordCloud(width=800, height=400).generate(negative_text)\n",
        "\n",
        "# Plot the WordClouds\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].imshow(positive_wordcloud, interpolation='bilinear')\n",
        "axes[0].set_title('Negative Features')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(negative_wordcloud, interpolation='bilinear')\n",
        "axes[1].set_title('Positive Features')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WFD0AgRa7Wji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build and train RNN models: LSTM, Bidirectional LSTM\n",
        "https://www.analyticsvidhya.com/blog/2022/01/the-complete-lstm-tutorial-with-implementation/\n"
      ],
      "metadata": {
        "id": "abWbQpyBLsvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Tokenize the text data https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert text to sequence\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad the sequences\n",
        "max_len = 100\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, truncating='post', padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=X_train_padded.shape[1], truncating='post', padding='post')\n",
        "\n",
        "size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "print('The number of unique words in the training data:', size_of_vocabulary)"
      ],
      "metadata": {
        "id": "VSPKKSnvLtm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deep learning library\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "\n",
        "# Define the LSTM model architecture\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(size_of_vocabulary,300, input_length=max_len, trainable=True))\n",
        "\n",
        "#lstm layer\n",
        "model_lstm.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model_lstm.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model_lstm.add(Dense(64,activation='relu'))\n",
        "model_lstm.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Compile the model, add loss function, metrics, optimizer\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "# Train the LSTM model\n",
        "history_lstm = model_lstm.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the LSTM model on the test data\n",
        "loss, accuracy = model_lstm.evaluate(X_test_padded, y_test)\n",
        "print(model_lstm.summary())\n",
        "print(\"LSTM Model Accuracy:\", accuracy)\n",
        "\n",
        "# Define the Bidirectional LSTM model architecture\n",
        "model_bilstm = Sequential()\n",
        "model_bilstm.add(Embedding(size_of_vocabulary,300, input_length=max_len, trainable=True))\n",
        "model_bilstm.add(Bidirectional(LSTM(128,return_sequences=True,dropout=0.2)))\n",
        "\n",
        "#Global Maxpooling\n",
        "model_bilstm.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model_bilstm.add(Dense(64,activation='relu'))\n",
        "model_bilstm.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_bilstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "# Train the Bidirectional LSTM model\n",
        "history_bilstm = model_bilstm.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the Bidirectional LSTM model on the test data\n",
        "loss, accuracy = model_bilstm.evaluate(X_test_padded, y_test)\n",
        "print(model_bilstm.summary())\n",
        "print(\"Bidirectional LSTM Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "gx73v9gbpErc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import learning_curve\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Plot the confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    labels = ['Negative', 'Positive']\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "# Print the classification report\n",
        "def print_classification_report(y_true, y_pred):\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    print(report)\n",
        "\n",
        "# Plot the learning curve\n",
        "def plot_learning_curve(history):\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    train_acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.plot(train_acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss/Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Generate WordCloud for positive reviews\n",
        "def generate_wordcloud_positive(text, y_train):\n",
        "    positive_text = ' '.join([review for review, sentiment in zip(text, y_train) if sentiment == 1])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('WordCloud - Positive Reviews')\n",
        "    plt.show()\n",
        "\n",
        "# Generate WordCloud for negative reviews\n",
        "def generate_wordcloud_negative(text, y_train):\n",
        "    negative_text = ' '.join([review for review, sentiment in zip(text, y_train) if sentiment == 0])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('WordCloud - Negative Reviews')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix for LSTM model\n",
        "y_pred_lstm = model_lstm.predict(X_test_padded)\n",
        "y_pred_lstm_classes = (y_pred_lstm > 0.5).astype(\"int32\")\n",
        "plot_confusion_matrix(y_test, y_pred_lstm_classes)\n",
        "\n",
        "# Print classification report for LSTM model\n",
        "print_classification_report(y_test, y_pred_lstm_classes)\n",
        "\n",
        "# Plot learning curve for LSTM model\n",
        "plot_learning_curve(history_lstm)\n",
        "\n",
        "# Generate WordCloud for positive reviews in LSTM model\n",
        "generate_wordcloud_positive(X_train, y_train)\n",
        "\n",
        "# Generate WordCloud for negative reviews in LSTM model\n",
        "generate_wordcloud_negative(X_train, y_train)\n",
        "\n",
        "# Plot confusion matrix for Bidirectional LSTM model\n",
        "y_pred_bilstm = model_bilstm.predict(X_test_padded)\n",
        "y_pred_bilstm_classes = (y_pred_bilstm > 0.5).astype(\"int32\")\n",
        "plot_confusion_matrix(y_test, y_pred_bilstm_classes)\n",
        "\n",
        "# Print classification report for Bidirectional LSTM model\n",
        "print_classification_report(y_test, y_pred_bilstm_classes)\n",
        "\n",
        "# Plot learning curve for Bidirectional LSTM model\n",
        "plot_learning_curve(history_bilstm)\n",
        "\n",
        "# Generate WordCloud for positive reviews in Bidirectional LSTM model\n",
        "generate_wordcloud_positive(X_train, y_train)\n",
        "\n",
        "# Generate WordCloud for negative reviews in Bidirectional LSTM model\n",
        "generate_wordcloud_negative(X_train, y_train)"
      ],
      "metadata": {
        "id": "yLobvQZEbA2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN model with: failed to identify any instances of negative sentiment (class 0) - ?\n",
        "\n",
        "1. Pretrained embedding (word2vec, fastText, GloVe)\n",
        "2. Early Stopping using val_loss https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/\n",
        "3. Model Checkpoints (save model)\n",
        "4. Hyperparameter tuning - https://realpython.com/python-keras-text-classification/\n"
      ],
      "metadata": {
        "id": "UlneMGZvLxy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Cleaned_MovieReviewDataset.csv')\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_texts, val_test_texts, train_labels, val_test_labels = train_test_split(df['review'], df['sentiment'], test_size=0.4, random_state=42)\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(val_test_texts, val_test_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Tokenize the texts and pad the sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "max_sequence_length = max(len(sequence) for sequence in train_sequences)\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "val_sequences = pad_sequences(val_sequences, maxlen=max_sequence_length)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Pretrained Embeddings\n",
        "embedding_type = \"word2vec\"  # Change this to \"fasttext\" or \"glove\"\n",
        "\n",
        "if embedding_type == \"word2vec\":\n",
        "    w2v_model = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "    embedding_matrix = np.zeros((size_of_vocabulary, w2v_model.vector_size))\n",
        "\n",
        "elif embedding_type == \"fasttext\":\n",
        "    ft_model = FastText.load(\"/content/drive/MyDrive/Colab Notebooks/wiki-news-300d-1M.vec\")\n",
        "    embedding_matrix = np.zeros((size_of_vocabulary, ft_model.vector_size))\n",
        "\n",
        "elif embedding_type == \"glove\":\n",
        "    glove2word2vec(\"/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.txt\", \"/content/drive/MyDrive/Colab Notebooks/to/glove.6B.100d.txt.word2vec\")\n",
        "    glove_model = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/to/glove.6B.100d.txt.word2vec\", binary=False)\n",
        "    embedding_matrix = np.zeros((size_of_vocabulary, glove_model.vector_size))\n",
        "\n",
        "\n",
        "# Build the RNN model\n",
        "def build_model(lstm_units=128):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(LSTM(lstm_units))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create a wrapper function for Keras model\n",
        "def create_model(lstm_units=128):\n",
        "    model = build_model(lstm_units=lstm_units)\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'batch_size': [32],\n",
        "    'epochs': [5],\n",
        "    'lstm_units': [64]\n",
        "}\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/to/best_model.h5'\n",
        "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Wrap the Keras model with the scikit-learn wrapper\n",
        "keras_model = KerasClassifier(build_fn=create_model)\n",
        "\n",
        "# Perform grid search for hyperparameter tuning\n",
        "model = GridSearchCV(keras_model, param_grid, cv=3, scoring='accuracy')\n",
        "model.fit(train_sequences, train_labels, validation_data=(val_sequences, val_labels), callbacks=[early_stopping, model_checkpoint]) #https://towardsdatascience.com/random-forest-regression-5f605132d19d\n",
        "\n",
        "# Load the best model\n",
        "best_model = build_model(lstm_units=model.best_params_['lstm_units'])\n",
        "best_model.load_weights(checkpoint_path)\n",
        "\n",
        "# Evaluate on the test set\n",
        "predictions = best_model.predict(test_sequences)\n",
        "predictions = np.round(predictions).flatten()\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "LVNoDGbLH5aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the predicted labels\n",
        "predicted_labels = np.round(predictions).flatten()\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-rgXRLr4f9-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "E0X_rF2bf_Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(history):\n",
        "    # Plot training and validation accuracy values\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training and validation loss values\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Train the best model and get the training history\n",
        "history = best_model.fit(train_sequences, train_labels, validation_data=(val_sequences, val_labels), epochs=model.best_params_['epochs'], batch_size=model.best_params_['batch_size'], callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Plot the learning curve\n",
        "plot_learning_curve(history)\n"
      ],
      "metadata": {
        "id": "x9vqN3nmgCGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Get positive reviews\n",
        "positive_reviews = df[df['sentiment'] == 1]['review'].values\n",
        "\n",
        "# Concatenate all positive reviews into a single string\n",
        "positive_text = ' '.join(positive_reviews)\n",
        "\n",
        "# Generate WordCloud\n",
        "wordcloud = WordCloud(width=800, height=400).generate(positive_text)\n",
        "\n",
        "# Plot the WordCloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('WordCloud - Positive Reviews')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uqwKcm2ygFXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get negative reviews\n",
        "negative_reviews = df[df['sentiment'] == 0]['review'].values\n",
        "\n",
        "# Concatenate all negative reviews into a single string\n",
        "negative_text = ' '.join(negative_reviews)\n",
        "\n",
        "# Generate WordCloud\n",
        "wordcloud = WordCloud(width=800, height=400).generate(negative_text)\n",
        "\n",
        "# Plot the WordCloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('WordCloud - Negative Reviews')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cABY0qphgHKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BERT based approach + GPU https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671\n",
        "gpu_enabled = True  # Set to False if GPU is not available or not desired\n",
        "bert_enabled = True  # Set to False if BERT is not available or not desired\n",
        "https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\n",
        "https://medium.com/intel-student-ambassadors/implementing-attention-models-in-pytorch-f947034b3e66\n",
        "https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/\n"
      ],
      "metadata": {
        "id": "5xuqTG7JlAbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "WQXj46An-OgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Cleaned_MovieReviewDataset.csv')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize and encode the training data\n",
        "train_encodings = tokenizer.batch_encode_plus(X_train.tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "train_input_ids = train_encodings['input_ids']\n",
        "train_attention_mask = train_encodings['attention_mask']\n",
        "\n",
        "# Convert y_train to numeric values\n",
        "train_labels = torch.tensor(y_train.values, dtype=torch.long)\n",
        "\n",
        "# Tokenize and encode the test data\n",
        "test_encodings = tokenizer.batch_encode_plus(X_test.tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "test_input_ids = test_encodings['input_ids']\n",
        "test_attention_mask = test_encodings['attention_mask']\n",
        "\n",
        "# Convert y_test to numeric values\n",
        "test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Create the BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "train_size = len(X_train)\n",
        "steps_per_epoch = int(np.ceil(train_size / batch_size))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        start = step * batch_size\n",
        "        end = min((step + 1) * batch_size, train_size)\n",
        "\n",
        "        input_ids = train_input_ids[start:end].to(device)\n",
        "        attention_mask = train_attention_mask[start:end].to(device)\n",
        "        labels = train_labels[start:end].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= steps_per_epoch\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs} - Training loss: {train_loss}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "eval_loss = 0.0\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        input_ids = test_input_ids[i:i + batch_size].to(device)\n",
        "        attention_mask = test_attention_mask[i:i + batch_size].to(device)\n",
        "        labels = test_labels[i:i + batch_size].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        eval_loss += loss.item()\n",
        "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "eval_loss /= int(np.ceil(len(X_test) / batch_size))\n",
        "predictions = np.array(predictions)"
      ],
      "metadata": {
        "id": "8orvpn-eclXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}