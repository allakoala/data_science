{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "8S1OVvmOZseW",
        "U96pI6wGdJ-K",
        "w4rBEdNwda87",
        "TAZNk4R4dx4B",
        "OlRxc_F7d3LQ",
        "fOy-xqfjd-pj",
        "qVjpLlMSeRFW",
        "x8B7t346jdb4",
        "qGiOepsYhZUE",
        "Nb8ViI4g1qQz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/HomeWork_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW: https://docs.google.com/document/d/1l6R-wlxfCrL_KBRKUmHvOjBRShzPyzDf0xAbGOD0cGs/edit\n",
        "# **Part 1: EDA**"
      ],
      "metadata": {
        "id": "rEV9V4o31JdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#—Åonclusions:\n",
        "1. there are categorical features with \"unknown\" values: job, marital, education, default, housing, loan.\n",
        "2. for the following features the NAN values could be dropped because those columns could be dependable:job, marital, education\n",
        "3. the following features should be replaced by median or mode: default, housing, loan\n",
        "\n",
        "#assumptions (should be discussed with SME):\n",
        "1. the following columns could be dependable and should be handled (dropped or groupped):job, marital, education\n",
        "2. the next substitution for the job feature could be made:\n",
        "if education = 'unknown' and 16<age<21, then job = student"
      ],
      "metadata": {
        "id": "8S1OVvmOZseW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVPlrhkJll8N"
      },
      "outputs": [],
      "source": [
        "#import libriaries\n",
        "#basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import math\n",
        "\n",
        "#data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WAtl7aiTDTtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path of the file to read\n",
        "url = \"/content/drive/MyDrive/data/bank-additional-full.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=';')\n",
        "\n",
        "#examine the data for Univariate analysis: consider features separately, their distribution, descriptive statistics, anomalies, omissions, etc\n",
        "data.head()"
      ],
      "metadata": {
        "id": "jLZkT8WGpN4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "RzRcnj202jNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#devide features into num and categirical\n",
        "\n",
        "#empty lists for numerical and categorical features\n",
        "numerical_cols = []\n",
        "categorical_cols = []\n",
        "\n",
        "#loop over each column and determine its data type\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    else:\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "print(\"Numerical features:\", numerical_cols)\n",
        "print(\"Categorical features:\", categorical_cols)"
      ],
      "metadata": {
        "id": "BZH2Yzo87dMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find duplicate rows\n",
        "duplicate_rows = data.duplicated(subset=numerical_cols+categorical_cols, keep=\"first\")\n",
        "duplicate_rows.sum()"
      ],
      "metadata": {
        "id": "iOVM5BqyL0eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove duplicate rows\n",
        "#data = data[~duplicate_rows]\n",
        "data = data.drop_duplicates()"
      ],
      "metadata": {
        "id": "0jAtc6lGNYlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#identify the data type\n",
        "data.info()"
      ],
      "metadata": {
        "id": "4JJdBFcQ3Gn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for each dataset column print unique values\n",
        "for col in data.columns:\n",
        "    n_unique_values = data[col].nunique()\n",
        "    unique_values = data[col].unique()\n",
        "    print(f\"{col}: {n_unique_values}: {unique_values}\")"
      ],
      "metadata": {
        "id": "95zvJgVUCe08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at the \"unknown\" value count in categorical features\n",
        "data_copy = data.copy()\n",
        "data_copy[categorical_cols] = data_copy[categorical_cols].replace('unknown', np.nan)\n",
        "data_copy.isna().sum()"
      ],
      "metadata": {
        "id": "nEDnuX4H56IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#conclusions:\n",
        "1. the dataset in terms of the target lable is Imbalanced, which means  that for 'y' need to downsample the majority class and upweight the downsampled class.\n",
        "2. this is a binary classification problem"
      ],
      "metadata": {
        "id": "U96pI6wGdJ-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#target lable is a categorical value\n",
        "target = data['y']\n",
        "#check the distribution: descriptive summary statistics and visualization tools to check the distribution of the variable(s)\n",
        "#frequency table using value_counts()\n",
        "target.value_counts()"
      ],
      "metadata": {
        "id": "5gcj83Ma0eo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target.describe()"
      ],
      "metadata": {
        "id": "58TUFYP9baRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#conclusions:\n",
        "1. mean < median (distribution of values in that column is negatively skewed and it has a long tail on the left-hand side of the median and is more spread out towards the lower values. there are more observations with lower values than with higher values): pdays, emp.var.rate, cons.price.index, euribor3m, nr.employed\n",
        "2. mean > median (distribution of values in that column is positevly skewed, presence of abnormal high values): age, duration, campaign, previous, cons.conf.idx\n",
        "3. big difference between 75th %tile and max values (sign of extreme values-Outliers, check out box plots): age, duration, campaign, previous, cons.conf.idx\n",
        "4. high standard deviation (values are more spread out and have a wider range of values) - age, duration, pdays, nr.employed\n",
        "5. low standard deviation (values are more tightly clustered around the mean and have a narrower range of values) - campaign, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m\n",
        "6. positively correlated pairs (positive slope):\n",
        "  1. age v duration\n",
        "  2. age v campaign\n",
        "7. negative correlation: duration and campaign\n",
        "\n",
        "#assumptions\n",
        "1. Age histogram shows that much less people participate in marketing campaigns after 60\n",
        "2. the most active age is between 30 - 50 years\n",
        "2. Age Min value is 17 - is this eligible? This entry should be deleted"
      ],
      "metadata": {
        "id": "w4rBEdNwda87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#continuous variables distribution visualization using a histogram\n",
        "\n",
        "#summary statistics of the numerical features\n",
        "print(data[numerical_cols].describe())\n",
        "\n",
        "#the histogram is colored by the target variable 'y' (which is binary) to see the differences in the distribution between the two target classes.\n",
        "for col in numerical_cols:\n",
        "    sns.histplot(data=data, x=col, hue='y', kde=True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tXre1jfJcdFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scatterplot method\n",
        "sns.set()\n",
        "sns.pairplot(data[numerical_cols], size = 2.5)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "wPzIoda67bxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scatter plot for each pair of numerical columns\n",
        "outcome = 'y'\n",
        "\n",
        "for i in range(len(numerical_cols)):\n",
        "    for j in range(i+1, len(numerical_cols)):\n",
        "      #create scatter plot with color-coded points\n",
        "      sns.scatterplot(x=numerical_cols[i], y=numerical_cols[j], hue=outcome, data=data)\n",
        "\n",
        "      #add regression line\n",
        "      sns.regplot(x=numerical_cols[i], y=numerical_cols[j], data=data, scatter=False, color=\"black\")\n",
        "\n",
        "      #set plot title and axis labels\n",
        "      plt.title(f\"{numerical_cols[i]} vs {numerical_cols[j]}\")\n",
        "      plt.xlabel(numerical_cols[i])\n",
        "      plt.ylabel(numerical_cols[j])\n",
        "\n",
        "      #display plot\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "gh3UBCwBgkAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#conclusions:\n",
        "1. people with housing loan and without personal loan are the most active users\n",
        "2. peopel tend to ignore campaigns in december\n",
        "3. admins are the most active category"
      ],
      "metadata": {
        "id": "TAZNk4R4dx4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical values analysis\n",
        "\n",
        "#frequency table of the categorical features\n",
        "for col in categorical_cols:\n",
        "  print(data[col].value_counts())\n",
        "\n",
        "#distribution of the categorical features visualisation using a bar plot. The bar plot is colored by the target variable 'y' to see the differences in the distribution between the two target classes.\n",
        "for col in categorical_cols:\n",
        "  sns.countplot(data=data, x=col, hue='y')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "3Qh4-G4Qbntz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#conclusions:\n",
        "we don't need an imputation (fillna() or dropna())"
      ],
      "metadata": {
        "id": "OlRxc_F7d3LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable and way to handle it. missing data can imply a reduction of the sample size\n",
        "\n",
        "total = data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (data.isna().sum()/data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "4mPjZPNrV589"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#conclusions:\n",
        "1. this dataset comprises some outliers according to several methods of detection\n",
        "2. the most Low range values are similar and not too far from 0\n",
        "3. High range values are far from 0 and the following columns seem to have outliers: campaign, previous"
      ],
      "metadata": {
        "id": "fOy-xqfjd-pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#outliers detection\n",
        "\n",
        "#Interquartile Range (IQR) method (values outside the normal range)\n",
        "for col in numerical_cols:\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[col] < lower) | (data[col] > upper)]\n",
        "    print(f\"{col} has {len(outliers)} outliers\")"
      ],
      "metadata": {
        "id": "ZVQ8rtIM_AXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#boxplot\n",
        "#red lines in the boxplot indicate the lower and upper limits of the normal range (calculated using the IQR method), and any points outside of these lines are considered outliers\n",
        "for col in numerical_cols:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.boxplot(data[col])\n",
        "    ax.set_title(f\"{col} Distribution\")\n",
        "    ax.set_ylabel(col)\n",
        "    ax.axhline(y=lower, color='r', linestyle='-', label='Lower Limit')\n",
        "    ax.axhline(y=upper, color='g', linestyle='-', label='Upper Limit')\n",
        "    ax.text(0.75, lower, f\"{lower:.2f}\", va='center', ha='center', bbox=dict(facecolor='red', alpha=0.5), fontsize=12)\n",
        "    ax.text(0.75, upper, f\"{upper:.2f}\", va='center', ha='center', bbox=dict(facecolor='green', alpha=0.5), fontsize=12)\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-0Tb5Zf5gh-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardizing data method for outliers\n",
        "for col in numerical_cols:\n",
        "    value_scaled = StandardScaler().fit_transform(data[col][:,np.newaxis])\n",
        "    low_range = value_scaled[value_scaled[:, 0] < -(data[col].std())]\n",
        "    high_range = value_scaled[value_scaled[:, 0] > data[col].std()]\n",
        "    print(f\"{col}: \\nLow range: \\n{low_range} \\nHigh range: \\n{high_range}\\n\")"
      ],
      "metadata": {
        "id": "1MCO2q3qV-lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anomalies or omissions\n",
        "\n",
        "#one-dimentional dataset method\n",
        "#define a list to accumlate anomalies\n",
        "anomalies = []\n",
        "\n",
        "#loop through each column\n",
        "for col in data[numerical_cols].columns:\n",
        "    #compute the mean and standard deviation of the column\n",
        "    col_mean = data[col].mean()\n",
        "    col_std = data[col].std()\n",
        "\n",
        "    #define the upper and lower limits for outliers\n",
        "    lower_limit = col_mean - 3 * col_std\n",
        "    upper_limit = col_mean + 3 * col_std\n",
        "\n",
        "    #find the outliers in the column and append them to the anomalies list\n",
        "    col_anomalies = data[(data[col] < lower_limit) | (data[col] > upper_limit)][col].values.tolist()\n",
        "    anomalies += col_anomalies\n",
        "\n",
        "    #print the results for the column\n",
        "    print(f\"Column {col}: Lower Limit = {lower_limit:.2f}, Upper Limit = {upper_limit:.2f}, Anomalies = {col_anomalies}\")"
      ],
      "metadata": {
        "id": "xyNq0lGQWDPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Isolation forest https://towardsdatascience.com/isolation-forest-auto-anomaly-detection-with-python-e7a8559d4562\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "#initialize the isolation forest model\n",
        "model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
        "\n",
        "#loop through each column and fit the model to identify anomalies\n",
        "for col in data.columns:\n",
        "    #preprocess the data\n",
        "    X = pd.get_dummies(data[col]) if col in categorical_cols else data[[col]]\n",
        "\n",
        "    #fit the model and predict the anomalies\n",
        "    model.fit(X)\n",
        "    anomalies = model.predict(X) == -1\n",
        "\n",
        "    #list of anomalies per column\n",
        "    #print(f'Anomalies in column {col}: {data[anomalies][col].tolist()}')\n",
        "\n",
        "    #list of anomalies unique values\n",
        "    print(f'Unique anomalies in column {col}: {data[anomalies][col].unique().tolist()}')"
      ],
      "metadata": {
        "id": "sLr6codBfg2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DBScan Clustering - craches the whole page :(\n",
        "\n",
        "# DBSCAN clustering parameters\n",
        "#min_samples = 2\n",
        "#eps = 3\n",
        "\n",
        "# list to accumulate anomalies per column\n",
        "#anomalies_list = []\n",
        "\n",
        "# loop over each numerical column in the dataset\n",
        "#for col in numerical_cols:\n",
        "    # extract the column data as a numpy array\n",
        "    #col_data = np.array(data[col]).reshape(-1, 1)\n",
        "\n",
        "    # DBSCAN clustering on the column data\n",
        "    #anomalies_detection = DBSCAN(min_samples=min_samples, eps=eps)\n",
        "    #clusters = anomalies_detection.fit_predict(col_data)\n",
        "\n",
        "    # anomalies == points assigned to -1 cluster label\n",
        "    #anomalies = col_data[clusters == -1]\n",
        "    #anomalies_list.append(anomalies)\n",
        "\n",
        "    #print(f\"Column {col}: Anomalies = {anomalies}\")"
      ],
      "metadata": {
        "id": "o2d1RQeBaIhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#conclusions:\n",
        "1. pairs where one of the variables is almost perfectly predictable from the other variables, and could lead to problems in a regression model:\n",
        "  1. emp.var.rate - cons.price.idx;\n",
        "  2. emp.var.rate - euribor3m;\n",
        "  3. emp.var.rate - nr.employed\n",
        "2. pairs of features which have a high positive correlation coefficient (close to 1), then they are likely measuring similar aspects of the data and may be redundant:\n",
        "  1. cons.price.idx - emp.var.rate;\n",
        "  2. euribor3m - emp.var.rate;\n",
        "  3. nr.employed- emp.var.rate;\n",
        "  4. cons.price.idx - euribor3m;\n",
        "  5. cons.price.idx - nr.employed;\n",
        "  6. euribor3m - nr.employed\n",
        "3. pairs of features which have a high negative correlation coefficient (close to -1), then they may be measuring opposite aspects of the data and may be providing conflicting information:\n",
        "  1. nr.employed - previous;\n",
        "  2. previous - pdays"
      ],
      "metadata": {
        "id": "qVjpLlMSeRFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Multivariate analysis: consider features in pairs, try to introduce new features to consider their relationship with other features, etc.\n",
        "\n",
        "#multicollinearity in Regression Analysis (correlation matrix)\n",
        "corr_matrix = data.corr()\n",
        "print(corr_matrix)"
      ],
      "metadata": {
        "id": "9lA7ZGkItGD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot a correlation matrix plot\n",
        "plt.matshow(corr_matrix)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-TT4aBUdiJfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmap style\n",
        "corrmat = data.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, vmax=.8, square=True);"
      ],
      "metadata": {
        "id": "rMuFEEGn-MB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection"
      ],
      "metadata": {
        "id": "x8B7t346jdb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "categorical_cols = pd.Index(categorical_cols)\n",
        "categorical_cols = categorical_cols.drop('y')\n",
        "cat_data_fs = pd.get_dummies(data_copy[categorical_cols]) #categorical input data (One-hot encode categorical features)\n",
        "num_data_fs = data_copy[numerical_cols] #numeric_input data\n",
        "outcome_feature_y_fs = data_copy['y'].replace({'yes': 1, 'no': 0})\n",
        "data_fs = pd.concat([cat_data_fs, num_data_fs, outcome_feature_y_fs], axis=1) #all input_data with NaN\n",
        "data_fs = data_fs.fillna(data_fs.mode().iloc[0]) #substitute na with mode\n",
        "\n",
        "print(data_fs)"
      ],
      "metadata": {
        "id": "UOahs_yY5Mtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select numerical features with highest correlation\n",
        "num_k_best = SelectKBest(lambda X, Y: np.array(list(map(lambda x: np.abs(np.corrcoef(x, Y)[0, 1]), X.T))).T, k=1).fit_transform(data_fs[numerical_cols], data_fs['y'])\n",
        "\n",
        "#train extra trees classifier to estimate feature importance for categorical features\n",
        "et = ExtraTreesClassifier(random_state=42)\n",
        "et.fit(data_fs.drop('y', axis=1), data_fs['y'])\n",
        "categorical_importance = et.feature_importances_\n",
        "\n",
        "#select categorical features with highest importance\n",
        "cat_k_best = SelectKBest(lambda X, Y: np.array(list(map(lambda x: abs(np.corrcoef(x, Y)[0, 1]), X.T))).T, k=1).fit_transform(data_fs.drop(['y']+numerical_cols, axis=1), data_fs['y'])\n",
        "\n",
        "#concatenate selected features with binary feature\n",
        "selected_features = np.concatenate((num_k_best, cat_k_best, data_fs['y'].values.reshape(-1, 1)), axis=1)\n",
        "\n",
        "#save selected features to file\n",
        "pd.DataFrame(selected_features).to_csv(\"mydataset_selected.csv\", index=False)"
      ],
      "metadata": {
        "id": "nf87qtkocACR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: DP**"
      ],
      "metadata": {
        "id": "pAoFaCUk1Uv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleansing"
      ],
      "metadata": {
        "id": "qGiOepsYhZUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data with the list of feature names\n",
        "feature_names = ['age', 'type_of_job', 'marital_st', 'education', 'credit_in_default', 'housing_loan', 'personal_loan', 'contact_type', 'last_contact_month', 'last_day_of_week', 'last_contact_duration', 'campaign_n', 'past_days', 'pr_n_of_contacts', 'pr_outcom', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'n_employees', 'subscribtion']\n",
        "bm_data = pd.read_csv(url, sep=';', names=feature_names, header=0)\n",
        "bm_data = bm_data.drop_duplicates() #remove duplicates\n",
        "\n",
        "#data randomization\n",
        "bm_data = bm_data.reindex(np.random.permutation(bm_data.index))\n",
        "print(\"Data set loaded. Num examples: \", len(bm_data))"
      ],
      "metadata": {
        "id": "vAUxBwtqEmKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data groupping for categorical and numerical features, converting + cleaning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "LABEL_NAME = 'subscribtion'\n",
        "\n",
        "numeric_feature_names = ['age', 'last_contact_duration', 'campaign_n', 'past_days', 'pr_n_of_contacts', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'n_employees']\n",
        "categorical_feature_y_names = list(set(feature_names) - set(numeric_feature_names))\n",
        "categorical_feature_input_names = list(set(feature_names) - set(numeric_feature_names) - set([LABEL_NAME]))\n",
        "\n",
        "#missing values - replace 'unknown' values in categorical features with NaN so that we may replace it with mode\n",
        "bm_data[categorical_feature_y_names] = bm_data[categorical_feature_y_names].replace('unknown', np.nan)\n",
        "#print(bm_data[categorical_feature_names])\n",
        "\n",
        "#for the following features drop nan: job, marital, education\n",
        "bm_data = bm_data.dropna(subset=['type_of_job', 'marital_st', 'education'])\n",
        "print(bm_data)"
      ],
      "metadata": {
        "id": "D2uebG13khOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert categorical features to numeric using one-hot encoding and replace nan by mode (mode is a robust statistic, meaning it will not be greatly affected by outliers)\n",
        "cat_data = pd.get_dummies(bm_data[categorical_feature_input_names]) #cat into num input data without 'subscribtion', cos there will be 2 outcome features\n",
        "print(cat_data)"
      ],
      "metadata": {
        "id": "zafSZ9lp1XAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_data = bm_data[numeric_feature_names] #numeric_input data\n",
        "X = pd.concat([cat_data, num_data], axis=1) #all input_data with NaN\n",
        "X = X.fillna(X.mode()) #all input_data with mode instead nan\n",
        "\n",
        "#convert object columns to numeric\n",
        "for col in X.select_dtypes(include=['object']):\n",
        "    try:\n",
        "        X[col] = pd.to_numeric(X[col], downcast='unsigned')\n",
        "    except ValueError:\n",
        "        X[col] = X[col].astype('category').cat.codes\n",
        "    except:\n",
        "        print('Error converting column', col)\n",
        "X = X.astype('uint8')\n",
        "\n",
        "print (X)"
      ],
      "metadata": {
        "id": "OBaVxOGuypyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the distribution\n",
        "\n",
        "#plot distribution\n",
        "for col in X.columns:\n",
        "\n",
        "    #calculate distribution statistics\n",
        "    mean = X[col].mean()\n",
        "    std_dev = X[col].std()\n",
        "    skew = X[col].skew()\n",
        "    kurtosis = X[col].kurtosis()\n",
        "\n",
        "    #distribution plots\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.distplot(X[col], ax=ax)\n",
        "    ax.axvline(mean, color='r', linestyle='--', label='mean')\n",
        "    ax.axvline(mean-std_dev, color='g', linestyle='--', label='std dev')\n",
        "    ax.axvline(mean+std_dev, color='g', linestyle='--')\n",
        "    ax.set_title(f'{col} Distribution')\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-_WB73upVyjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outliers\n",
        "how to delete outliers from Y - ? because at the Logistic regression model stage  X and Y have different number of rows"
      ],
      "metadata": {
        "id": "XuYqAjMHigHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare Y\n",
        "Y = bm_data[LABEL_NAME]\n",
        "Y.describe()"
      ],
      "metadata": {
        "id": "jwiSOEvfLJzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace 'yes' with 1 and 'no' with 0\n",
        "Y = Y.replace({'yes': 1, 'no': 0}).astype(int)\n",
        "\n",
        "# X = X + Y(converted)\n",
        "X = pd.concat([X, Y], axis=1) #all data for outliers\n",
        "\n",
        "X.info()"
      ],
      "metadata": {
        "id": "0tpORLRRLzyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downsample the majority class and upweight the minority class with SMOTE classifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils import resample\n",
        "\n",
        "df_majority = X[X[LABEL_NAME]==0]\n",
        "df_minority = X[X[LABEL_NAME]==1]\n",
        "\n",
        "print(df_majority)\n",
        "print(df_minority)"
      ],
      "metadata": {
        "id": "6XKR2T8u5D88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downsample majority class\n",
        "df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
        "\n",
        "#upsample minority class using SMOTE\n",
        "XX = X.drop([LABEL_NAME], axis=1)\n",
        "YY = X[LABEL_NAME]\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "XX_smote, YY_smote = smote.fit_resample(XX, YY)\n",
        "df_minority_upsampled = pd.concat([XX_smote, YY_smote], axis=1)\n",
        "\n",
        "#combine minority and downsampled majority classes\n",
        "df_resampled = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
        "\n",
        "#check class distribution\n",
        "df_resampled[LABEL_NAME].value_counts()"
      ],
      "metadata": {
        "id": "ddzAGbUuHZKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outliers (Robust Z-score method - better for large datasets with more or less normal distribution) https://www.statology.org/remove-outliers-python/\n",
        "#find absolute value of z-score for each observation\n",
        "X = df_resampled\n",
        "z = np.abs(stats.zscore(X))\n",
        "\n",
        "#only keep rows in dataframe with all z-scores less than absolute value of 3\n",
        "X_clean = X[(z<3).all(axis=1)]\n",
        "\n",
        "print(X_clean)"
      ],
      "metadata": {
        "id": "9OcVjLchpm6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing for data"
      ],
      "metadata": {
        "id": "uM9yYX-Ciogj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data rescale with MaxScaler class\n",
        "array = X_clean.values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled = scaler.fit_transform(array)\n",
        "#summarize transformed data\n",
        "np.set_printoptions(precision=3)\n",
        "print(rescaled)"
      ],
      "metadata": {
        "id": "vYoVhRZ1ycnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standatization\n",
        "scaler_s = StandardScaler().fit(array)\n",
        "rescaled_s = scaler_s.transform(array)\n",
        "np.set_printoptions(precision=3)\n",
        "print(rescaled_s)"
      ],
      "metadata": {
        "id": "KZpVADyM1SnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "scaler_n = Normalizer().fit(array)\n",
        "normalized = scaler.transform(array)\n",
        "#summarize transformed data\n",
        "np.set_printoptions(precision=3)\n",
        "print(normalized)"
      ],
      "metadata": {
        "id": "jcSSQ8Qc7tcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Binarize data\n",
        "binarizer = Binarizer(threshold=0.0).fit(array)\n",
        "binary = binarizer.transform(array)\n",
        "np.set_printoptions(precision=3)\n",
        "print(binary)"
      ],
      "metadata": {
        "id": "FxjX6rgx8fPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the distribution after Standatization\n",
        "feature_names_all = X_clean.columns\n",
        "rescaled_s_df = pd.DataFrame(rescaled_s, columns = feature_names_all)"
      ],
      "metadata": {
        "id": "3i7eIZC3Fljx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot distribution\n",
        "for col in rescaled_s_df.columns:\n",
        "\n",
        "    #calculate distribution statistics\n",
        "    mean = rescaled_s_df[col].mean()\n",
        "    std_dev = rescaled_s_df[col].std()\n",
        "    skew = rescaled_s_df[col].skew()\n",
        "    kurtosis = rescaled_s_df[col].kurtosis()\n",
        "\n",
        "    #distribution plots\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.distplot(X[col], ax=ax)\n",
        "    ax.axvline(mean, color='r', linestyle='--', label='mean')\n",
        "    ax.axvline(mean-std_dev, color='g', linestyle='--', label='std dev')\n",
        "    ax.axvline(mean+std_dev, color='g', linestyle='--')\n",
        "    ax.set_title(f'{col} Distribution')\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fSuKkK3tw_wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression model\n",
        "after standartization Y is no binary?"
      ],
      "metadata": {
        "id": "OI7fjmQsjA4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# split data into train and test sets\n",
        "X = rescaled_s_df.drop(LABEL_NAME, axis=1).astype(int) # input_data after upweighting\n",
        "Y = rescaled_s_df[LABEL_NAME].astype(int) # outcome_feature_data after upweighting\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
        "\n",
        "# logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# model evaluation\n",
        "print('Accuracy on training set: ', model.score(X_train, Y_train))\n",
        "print('Accuracy on test set: ', model.score(X_test, Y_test))"
      ],
      "metadata": {
        "id": "UTEsNhSI6bOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LGBMClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "lgbm_clf = LGBMClassifier()\n",
        "lgbm_clf.fit(X_train, Y_train)\n",
        "Y_pred = lgbm_clf.predict(X_test)\n",
        "\n",
        "#model evaluation\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#model report\n",
        "report = classification_report(Y_test, Y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "8AcMtdmrMwtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: DR**\n",
        "#conclusions:\n",
        "1.  Based on the scree plot, it appears that the first two components account for the majority of the variance in the data, while the remaining components account for a relatively small amount of additional variance. Therefore, the target number of components for this analysis could be set to 23."
      ],
      "metadata": {
        "id": "Nb8ViI4g1qQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction Techniques\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD, PCA, NMF\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df_resampled.drop(LABEL_NAME, axis=1) #input_data after upweighting\n",
        "y = df_resampled[LABEL_NAME] #outcome_feature_data after upweighting\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "5H5vnNVP0gPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Singular Value Decomposition (SVD)\n",
        "svd = TruncatedSVD(n_components=2)\n",
        "X_svd = svd.fit_transform(X_scaled)\n",
        "\n",
        "# SVD visualization\n",
        "plt.scatter(X_svd[:,0], X_svd[:,1], c=y)\n",
        "plt.title(\"SVD\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qRA_H7x81gHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# PCA visualization\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y)\n",
        "plt.title(\"PCA\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8lJnYkMq4-vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uniform Manifold Approximation and Projection (UMAP)\n",
        "!pip install umap-learn\n",
        "import umap\n",
        "\n",
        "umap = umap.UMAP()\n",
        "X_umap = umap.fit_transform(X_scaled)\n",
        "\n",
        "# UMAP visualization https://umap-learn.readthedocs.io/en/latest/basic_usage.html\n",
        "plt.scatter(X_umap[:,0], X_umap[:,1], c=y)\n",
        "plt.title(\"UMAP\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rnyw0lbs0MD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-SNE - doesnt work :(\n",
        "#tsne = TSNE()\n",
        "#X_tsne = tsne.fit_transform(X_scaled)\n",
        "#fashion_scatter(X_tsne, y)"
      ],
      "metadata": {
        "id": "vZLVSdMa5fFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-negative Matrix Factorization (NMF) - doesnt work :(\n",
        "#nmf = NMF()\n",
        "#X_nmf = nmf.fit_transform(X_scaled)"
      ],
      "metadata": {
        "id": "a7WKzRYO4hP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explained Variance\n",
        "\n",
        "# SVD explained variance\n",
        "svd_var = np.var(X_svd, axis=0)\n",
        "svd_explained_var_ratio = svd_var / np.sum(svd_var)\n",
        "print(\"SVD explained variance ratio:\", svd_explained_var_ratio)\n",
        "\n",
        "# PCA explained variance\n",
        "print(\"PCA explained variance ratio:\", pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "6LNCBmtfzsNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the best number of components\n",
        "# PCA - Scree plot - ‚Ññ1 https://www.mikulskibartosz.name/pca-how-to-choose-the-number-of-components/\n",
        "pca = PCA().fit(X_scaled)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\n",
        "xi = np.arange(1, len(y)+1)\n",
        "\n",
        "plt.ylim(0.0,1.1)\n",
        "plt.xlim(0.0,30.5)\n",
        "plt.plot(xi, y, marker='o', linestyle='--', color='b', label='Cumulative Variance')\n",
        "\n",
        "plt.xlabel('Number of Components', fontsize=14)\n",
        "plt.xticks(np.arange(1, 25, step=1)) #change from 0-based array index to 1-based human-readable label\n",
        "plt.ylabel('Cumulative variance (%)', fontsize=14)\n",
        "plt.title('The number of components needed to explain variance', fontsize=16)\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-', label='95% Cut-Off Threshold')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=12)\n",
        "\n",
        "#intersection point\n",
        "cutoff_index = np.argmax(y > 0.95)\n",
        "cutoff_x = xi[cutoff_index]\n",
        "cutoff_y = y[cutoff_index]\n",
        "plt.plot(cutoff_x, cutoff_y, marker='o', markersize=10, color='r', label='Intersection Point')\n",
        "\n",
        "\n",
        "ax.grid(axis='x')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TZU1EbzHiL4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the best number of components\n",
        "\n",
        "# PCA - Scree plot‚Ññ2\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "plt.plot(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
        "plt.title(\"PCA Scree plot\")\n",
        "plt.xlabel(\"Number of components\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "\n",
        "#target number of components at 95% cut-off threshold\n",
        "cumulative_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "target_components = np.argmax(cumulative_var >= 0.95) + 1\n",
        "\n",
        "plt.axvline(x=target_components, color='r', linestyle='--', label='Target Number of Components')\n",
        "\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HZVKB9hLhbwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mentor's Feedback**: In general, all acceptance criteria are met. Code is mostly well-structured, although in part 1 the header structure is more mixed up. Good and comprehensive conclusions are provided, it is clearly seen that the topic under research is well understood. **What to improve**: it is better to achieve more reusability of code. Imagine that you will do exactly the same sequence of actions not only on one dataset, but on 100 different datasets. In this case, it would be better if you implemented actions you would have to repeat as functions."
      ],
      "metadata": {
        "id": "ZraDRvTxjbas"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HiRZmi5nk2wz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}