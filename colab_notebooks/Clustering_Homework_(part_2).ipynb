{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "LmVVTBnv87um",
        "g_gYzgPyQPpd",
        "xoyjIXIGOcac",
        "_Tzk-FlxPJcF",
        "WHNlWzhVPNVf",
        "gcadQ8jBPNhJ",
        "-30e2pqjPNpP"
      ],
      "mount_file_id": "1XoDwK7RdAF5kDOvzkGuAeFSnbGuCA-11",
      "authorship_tag": "ABX9TyMrjtwewys7Oomhv9+QXmtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/Clustering_Homework_(part_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW: https://drive.google.com/file/d/1o6wN90vZ6GXwbtM0I3TwdXO8NPCN9vWK/view\n",
        "\n",
        "Dataset: https://drive.google.com/file/d/1iNwpwoIdyXJzzq16WhAG_nXp9Gdd7ZSJ/view?usp=sharing\n",
        "\n",
        "Dataset description: https://www.dbs.ifi.lmu.de/research/outlier-evaluation/DAMI/semantic/Cardiotocography/Cardiotocography_22.html\n",
        "\n",
        "The given data has 16880 rows and 23 columns. Each row represents a record of a cardiotocogram (CTG) examination, which is a fetal monitoring test performed during pregnancy. The columns provide various measurements and features of the CTG test, such as the baseline fetal heart rate, accelerations, decelerations, and other parameters.\n",
        "\n",
        "Some of the important columns and their descriptions are as follows:\n",
        "\n",
        "- LB: FHR baseline (beats per minute)\n",
        "- AC: # of accelerations per second\n",
        "- FM: # of fetal movements per second\n",
        "- UC: # of uterine contractions per second\n",
        "- ASTV: percentage of time with abnormal short-term variability\n",
        "- MSTV: mean value of short-term variability\n",
        "- ALTV: percentage of time with abnormal long-term variability\n",
        "- MLTV: mean value of long-term variability\n",
        "- DL: # of light decelerations per second\n",
        "- DS: # of severe decelerations per second\n",
        "- Min: minimum of FHR histogram\n",
        "- Max: Maximum of FHR histogram\n",
        "- Nmax: # of histogram peaks\n",
        "- Nzeros: # of histogram zeros\n",
        "- Mode: histogram mode\n",
        "- Mean: histogram mean\n",
        "- Median: histogram median\n",
        "- Variance: histogram variance\n",
        "- Tendency: histogram tendency\n",
        "- outlier: whether a record is an outlier or not\n",
        "\n",
        "Q-s: LOF;\n",
        "Reachability distance;\n",
        "Local reachability density\n",
        "https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843"
      ],
      "metadata": {
        "id": "LmVVTBnv87um"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq0sL-3T8zGr"
      },
      "outputs": [],
      "source": [
        "#basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path of the file to read\n",
        "url = \"/content/drive/MyDrive/Colab Notebooks/Cardiotocography.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=',')\n",
        "\n",
        "#examine the data\n",
        "data"
      ],
      "metadata": {
        "id": "Xotjj2BIOXq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA/ data preparation"
      ],
      "metadata": {
        "id": "g_gYzgPyQPpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find duplicate rows\n",
        "duplicate_rows = data.duplicated(subset=data.columns, keep=\"first\")\n",
        "duplicate_rows.sum()"
      ],
      "metadata": {
        "id": "RR_5nkPUQZ_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform the feature using LabelEncoder\n",
        "data['outlier_encoded'] = le.fit_transform(data['outlier'])\n",
        "\n",
        "# Print the original feature and its label-encoded version\n",
        "print(\"Original feature: \", data['outlier'])\n",
        "print(\"Label-encoded feature: \", data['outlier_encoded'])"
      ],
      "metadata": {
        "id": "Gthnwy6oU38b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['outlier_encoded'] = (data['outlier_encoded']).astype(int)\n",
        "data.drop(['outlier', 'id'], axis=1, inplace=True)\n",
        "print(\"Label-encoded feature: \", data['outlier_encoded'])"
      ],
      "metadata": {
        "id": "4DRbCLADXgAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info())\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "5D5aW1p8Q6Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for each dataset column print unique values\n",
        "for col in data.columns:\n",
        "    n_unique_values = data[col].nunique()\n",
        "    unique_values = data[col].unique()\n",
        "    print(f\"{col}: {n_unique_values}: {unique_values}\")"
      ],
      "metadata": {
        "id": "SesIqsV4ReYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['outlier_encoded'].value_counts()"
      ],
      "metadata": {
        "id": "HEXFjVBOLYH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PLOTS"
      ],
      "metadata": {
        "id": "xoyjIXIGOcac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable and way to handle it. missing data can imply a reduction of the sample size\n",
        "\n",
        "total = data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (data.isna().sum()/data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "5chlaeXFRmdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the histogram\n",
        "for col in data.columns:\n",
        "    sns.histplot(data=data, x=col, kde=True)\n",
        "    plt.show()\n",
        "\n",
        "#lists of columns with specified characteristics\n",
        "stats = data.describe()\n",
        "print(f\"\\nColumns with mean < median:\\n{stats.columns[stats.loc['mean'] < stats.loc['50%']]}\")\n",
        "print(f\"\\nColumns with mean > median:\\n{stats.columns[stats.loc['mean'] > stats.loc['50%']]}\")\n",
        "print(f\"\\nColumns with big difference between (75th %tile and max) or/and (25th %tile and min values):\\n{stats.columns[((stats.loc['75%'] - stats.loc['max']).abs() > 100) | ((stats.loc['25%'] - stats.loc['min']).abs() > 100)]}\")\n",
        "print(f\"\\nColumns with high standard deviation:\\n{stats.columns[stats.loc['std'] > 100]}\")\n",
        "print(f\"\\nColumns with low standard deviation:\\n{stats.columns[stats.loc['std'] < 0.1]}\")"
      ],
      "metadata": {
        "id": "ISOaCWMxRv5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scatterplot method\n",
        "sns.set()\n",
        "sns.pairplot(data, size = 2.5)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "x85Y570GR_D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#boxplot\n",
        "\n",
        "#outliers detection\n",
        "\n",
        "#Interquartile Range (IQR) method (values outside the normal range)\n",
        "for col in data.columns:\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[col] < lower) | (data[col] > upper)]\n",
        "    print(f\"{col} has {len(outliers)} outliers\")\n",
        "\n",
        "#red lines in the boxplot indicate the lower and upper limits of the normal range (calculated using the IQR method), and any points outside of these lines are considered outliers\n",
        "for col in data.columns:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.boxplot(data[col])\n",
        "    ax.set_title(f\"{col} Distribution\")\n",
        "    ax.set_ylabel(col)\n",
        "    ax.axhline(y=lower, color='r', linestyle='-', label='Lower Limit')\n",
        "    ax.axhline(y=upper, color='g', linestyle='-', label='Upper Limit')\n",
        "    ax.text(0.75, lower, f\"{lower:.2f}\", va='center', ha='center', bbox=dict(facecolor='red', alpha=0.5), fontsize=12)\n",
        "    ax.text(0.75, upper, f\"{upper:.2f}\", va='center', ha='center', bbox=dict(facecolor='green', alpha=0.5), fontsize=12)\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gUT8swi9Gr5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create empty lists for positive and negative slopes\n",
        "positive_slope_plots = []\n",
        "negative_slope_plots = []\n",
        "\n",
        "for i in range(len(data.columns)):\n",
        "    for j in range(i+1, len(data.columns)):\n",
        "        # calculate the slope of the regression line\n",
        "        slope = data[data.columns[j]].corr(data[data.columns[i]])\n",
        "\n",
        "        # create scatter plot with color-coded points\n",
        "        sns.scatterplot(x=data.columns[i], y=data.columns[j], data=data)\n",
        "\n",
        "        # add regression line\n",
        "        sns.regplot(x=data.columns[i], y=data.columns[j], data=data, scatter=False, color=\"black\")\n",
        "\n",
        "        # set plot title and axis labels\n",
        "        plt.title(f\"{data.columns[i]} vs {data.columns[j]}\")\n",
        "        plt.xlabel(data.columns[i])\n",
        "        plt.ylabel(data.columns[j])\n",
        "\n",
        "        # append plot and feature pair to the appropriate list based on the sign of the slope\n",
        "        if slope > 0:\n",
        "            positive_slope_plots.append((f\"{data.columns[i]} vs {data.columns[j]}\", plt))\n",
        "        elif slope < 0:\n",
        "            negative_slope_plots.append((f\"{data.columns[i]} vs {data.columns[j]}\", plt))\n",
        "\n",
        "        # display plot\n",
        "        plt.show()\n",
        "\n",
        "        # clear current figure to free up memory\n",
        "        plt.clf()\n",
        "\n",
        "# print out the lists of feature pairs\n",
        "print(\"Feature pairs with positive slopes:\")\n",
        "for feature_pair, plot in positive_slope_plots:\n",
        "    print(f\"{feature_pair}: {plot}\")\n",
        "\n",
        "print(\"\\nFeature pairs with negative slopes:\")\n",
        "for feature_pair, plot in negative_slope_plots:\n",
        "    print(f\"{feature_pair}: {plot}\")"
      ],
      "metadata": {
        "id": "UWdkxq1FSvVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmap style\n",
        "sns.set(style='darkgrid')\n",
        "corrmat = data.corr()\n",
        "f, ax = plt.subplots(figsize=(20, 10))\n",
        "sns.heatmap(corrmat, annot=True, fmt=\".2f\", cmap='coolwarm', vmax=.8, square=True)\n",
        "plt.show()\n",
        "\n",
        "# print highly positively correlated pairs\n",
        "pos_corr_pairs = []\n",
        "for i in range(len(corrmat.columns)):\n",
        "    for j in range(i+1, len(corrmat.columns)):\n",
        "        if abs(corrmat.iloc[i, j]) >= 0.5:\n",
        "            pos_corr_pairs.append((corrmat.columns[i], corrmat.columns[j]))\n",
        "\n",
        "print(\"Highly positively correlated pairs:\")\n",
        "for pair in pos_corr_pairs:\n",
        "    print(pair)\n",
        "\n",
        "# print highly negatively correlated pairs\n",
        "neg_corr_pairs = []\n",
        "for i in range(len(corrmat.columns)):\n",
        "    for j in range(i+1, len(corrmat.columns)):\n",
        "        if abs(corrmat.iloc[i, j]) <=  -0.5:\n",
        "            neg_corr_pairs.append((corrmat.columns[i], corrmat.columns[j]))\n",
        "\n",
        "print(\"Highly negatively correlated pairs:\")\n",
        "for pair in neg_corr_pairs:\n",
        "    print(pair)"
      ],
      "metadata": {
        "id": "6DaGESKpSKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature engineering\n",
        "For the following features:\n",
        "1. LB\n",
        "2. ASTV\n",
        "3. ALTV\n",
        "4. DS"
      ],
      "metadata": {
        "id": "_Tzk-FlxPJcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
        "\n",
        "# Scale the variables to have the same range\n",
        "scaler = MinMaxScaler()\n",
        "data[[\"LB\", \"ASTV\", \"ALTV\", \"DS\"]] = scaler.fit_transform(data[[\"LB\", \"ASTV\", \"ALTV\", \"DS\"]])\n",
        "\n",
        "# Create new features by applying non-linear transformations\n",
        "data[\"LB2\"] = data[\"LB\"] ** 2\n",
        "data[\"sqrt_LB\"] = np.sqrt(data[\"LB\"])\n",
        "\n",
        "data[\"ASTV2\"] = data[\"ASTV\"] ** 2\n",
        "data[\"sqrt_ASTV\"] = np.sqrt(data[\"ASTV\"])\n",
        "\n",
        "data[\"ALTV2\"] = data[\"ALTV\"] ** 2\n",
        "data[\"sqrt_ALTV\"] = np.sqrt(data[\"ALTV\"])\n",
        "\n",
        "data[\"DS2\"] = data[\"DS\"] ** 2\n",
        "data[\"sqrt_DS\"] = np.sqrt(data[\"DS\"])\n",
        "\n",
        "data[\"LB_ASTV_ALTV_DS\"] = data[\"LB\"] * data[\"ASTV\"] * data[\"ALTV\"] * data[\"DS\"]"
      ],
      "metadata": {
        "id": "9cLaY3_ZQCh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mahalanobis rule\n",
        "The Mahalanobis distance is a measure of the distance between a point and a distribution. It takes into account the covariance structure of the dataset, unlike the Euclidean distance, which treats each variable independently. The Mahalanobis distance is calculated as the distance between a point and the mean of the distribution, normalized by the covariance matrix."
      ],
      "metadata": {
        "id": "WHNlWzhVPNVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features and target\n",
        "X = data.drop('outlier_encoded', axis=1)\n",
        "y = data['outlier_encoded']\n",
        "X.columns"
      ],
      "metadata": {
        "id": "Z5vKCH3jpeTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the mean and covariance matrix of the dataset for each class\n",
        "mean_vec = []\n",
        "cov_matrix = []\n",
        "\n",
        "for i in [0, 1]:\n",
        "    X_i = X[y == i]\n",
        "    mean_vec.append(np.mean(X_i, axis=0))\n",
        "    cov_matrix.append(np.cov(X_i, rowvar=False))\n",
        "\n",
        "#Calculate the inverse of the covariance matrix for each class\n",
        "inv_cov_matrix = []\n",
        "for i in [0, 1]:\n",
        "    try:\n",
        "        inv_cov_matrix.append(np.linalg.inv(cov_matrix[i]))\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        print(f\"Singular matrix error: {e}\")\n",
        "\n",
        "#Calculate the Mahalanobis distance for each data point\n",
        "mahalanobis_dist = []\n",
        "for x, class_idx in zip(X.values, y.values):\n",
        "    x_minus_mean = x - mean_vec[class_idx]\n",
        "    try:\n",
        "        md = np.sqrt(np.dot(np.dot(x_minus_mean, inv_cov_matrix[class_idx]), x_minus_mean.T))\n",
        "    except IndexError:\n",
        "        md = float('inf')\n",
        "    mahalanobis_dist.append(md)\n",
        "\n",
        "#Create a DataFrame with the Mahalanobis distances and the y feature\n",
        "df = pd.DataFrame({'Mahalanobis Distance': mahalanobis_dist, 'outlier_encoded': y})\n",
        "\n",
        "#Set a threshold for the Mahalanobis distance for each row\n",
        "threshold = np.quantile(df['Mahalanobis Distance'], 0.99)\n",
        "\n",
        "#create a new column 'mr_outlier' and set 1 to that column for each distance which is more than the threshold, otherwise - 0\n",
        "df['mr_outlier'] = np.where(df['Mahalanobis Distance'] > threshold, 1, 0)\n",
        "\n",
        "#Print out the first 20 highest Mahalanobis distances\n",
        "df_sorted = df.sort_values(by='Mahalanobis Distance', ascending=False)\n",
        "print(df_sorted.head(30))\n",
        "\n",
        "#Calculate accuracy by comparing mr_outlier and outlier_encoded column\n",
        "accuracy = (df['mr_outlier'] == df['outlier_encoded']).sum() / len(df)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "ZufjrfjTTTvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print entries where 'mr_outlier' is not equal to 'outlier_encoded'\n",
        "diff_entries = df[df['mr_outlier'] != df['outlier_encoded']]\n",
        "print(\"Entries where 'mr_outlier' is not equal to 'outlier_encoded':\")\n",
        "print(diff_entries)\n",
        "print(\"Count of such entries:\", len(diff_entries))"
      ],
      "metadata": {
        "id": "JU1K8DkEYehf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised models:\n",
        "a. SVM\n",
        "\n",
        "b. Logreg\n",
        "\n",
        "c. KNN\n"
      ],
      "metadata": {
        "id": "gcadQ8jBPNhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('outlier_encoded', axis=1), data['outlier_encoded'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Print outlier_encoded values\n",
        "print(\"outlier_encoded values:\\n\", y_train.value_counts(), \"\\n\")\n",
        "\n",
        "# Support Vector Machine (SVM) model\n",
        "svm_pipeline = Pipeline([('scaler', StandardScaler()), ('svm', SVC())])\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "svm_pred = svm_pipeline.predict(X_test)\n",
        "\n",
        "# Print accuracy, confusion matrix, precision, recall, and F1 score for SVM\n",
        "print(\"Results for SVM:\\nAccuracy:\", accuracy_score(y_test, svm_pred), \"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, svm_pred), \"\\nClassification Report:\\n\", classification_report(y_test, svm_pred), \"\\n\")\n",
        "\n",
        "# Number of entities where y_pred = 1 for SVM\n",
        "svm_entities_pred_1 = sum(svm_pred == 1)\n",
        "print(\"Number of entities where y_pred = 1 for SVM:\", svm_entities_pred_1)\n",
        "\n",
        "# First 15 indexes of entities where y_pred = 1 for SVM\n",
        "svm_entities_pred_1_idx = list(filter(lambda idx: svm_pred[idx] == 1, range(len(svm_pred))))[:15]\n",
        "print(\"First 15 indexes of entities where y_pred = 1 for SVM:\", svm_entities_pred_1_idx)\n",
        "\n",
        "# Logistic Regression (Logreg) model\n",
        "logreg_pipeline = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression())])\n",
        "logreg_pipeline.fit(X_train, y_train)\n",
        "logreg_pred = logreg_pipeline.predict(X_test)\n",
        "\n",
        "# Print accuracy, confusion matrix, precision, recall, and F1 score for Logreg\n",
        "print(\"Results for Logreg:\\nAccuracy:\", accuracy_score(y_test, logreg_pred), \"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, logreg_pred), \"\\nClassification Report:\\n\", classification_report(y_test, logreg_pred), \"\\n\")\n",
        "\n",
        "# Number of entities where y_pred = 1 for Logreg\n",
        "logreg_entities_pred_1 = sum(logreg_pred == 1)\n",
        "print(\"Number of entities where y_pred = 1 for Logreg:\", logreg_entities_pred_1)\n",
        "\n",
        "# First 15 indexes of entities where y_pred = 1 for Logreg\n",
        "logreg_entities_pred_1_idx = list(filter(lambda idx: logreg_pred[idx] == 1, range(len(logreg_pred))))[:15]\n",
        "print(\"First 15 indexes of entities where y_pred = 1 for Logreg:\", logreg_entities_pred_1_idx)\n",
        "\n",
        "# K-Nearest Neighbors (KNN) model\n",
        "knn_pipeline = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "knn_pred = knn_pipeline.predict(X_test)\n",
        "\n",
        "# Print accuracy, confusion matrix, precision, recall, and F1 score for KNN\n",
        "print(\"Results for KNN:\\nAccuracy:\", accuracy_score(y_test, knn_pred), \"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, knn_pred), \"\\nClassification Report:\\n\", classification_report(y_test, knn_pred), \"\\n\")\n",
        "\n",
        "# Number of entities where y_pred = 1 for KNN\n",
        "knn_entities_pred_1 = sum(knn_pred == 1)\n",
        "print(\"Number of entities where y_pred = 1 for KNN:\", knn_entities_pred_1)\n",
        "\n",
        "# First 15 indexes of entities where y_pred = 1 for KNN\n",
        "knn_entities_pred_1_idx = list(filter(lambda idx: knn_pred[idx] == 1, range(len(knn_pred))))[:15]\n",
        "print(\"First 15 indexes of entities where y_pred = 1 for KNN:\", knn_entities_pred_1_idx)\n",
        "print('')\n",
        "\n",
        "# Common entities where y_pred = 1\n",
        "svm_entities = set(svm_pred.nonzero()[0])\n",
        "logreg_entities = set(logreg_pred.nonzero()[0])\n",
        "knn_entities = set(knn_pred.nonzero()[0])\n",
        "common_entities = svm_entities.intersection(logreg_entities, knn_entities)\n",
        "print(\"Number of common entities where y_pred = 1 for all models:\", len(common_entities))\n",
        "print(\"First 15 indexes of common entities where y_pred = 1 for all models:\", list(common_entities)[:15])"
      ],
      "metadata": {
        "id": "SwpSDbJhyX8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of all predictions and their corresponding model names\n",
        "predictions = [(svm_pred, \"SVM\"), (logreg_pred, \"Logreg\"), (knn_pred, \"KNN\")]\n",
        "\n",
        "# Iterate over all predictions and their names\n",
        "for pred, name in predictions:\n",
        "    # Apply t-SNE and UMAP\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    umap = UMAP(n_components=2, random_state=42)\n",
        "    tsne_result = tsne.fit_transform(X_test)\n",
        "    umap_result = umap.fit_transform(X_test)\n",
        "\n",
        "    # Plot t-SNE visualization\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(name + \" t-SNE\")\n",
        "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=pred)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot UMAP visualization\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(name + \" UMAP\")\n",
        "    plt.scatter(umap_result[:, 0], umap_result[:, 1], c=pred)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8zDqddGIWzf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised models:\n",
        "a. One-class SVM\n",
        "\n",
        "b. Isolation Forest\n",
        "\n",
        "c. DBScan"
      ],
      "metadata": {
        "id": "-30e2pqjPNpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('outlier_encoded', axis=1), data['outlier_encoded'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Models\n",
        "svm = OneClassSVM().fit(X_train, y_train)\n",
        "iforest = IsolationForest().fit(X_train, y_train)\n",
        "dbscan = DBSCAN().fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "svm_preds = svm.predict(X_test)\n",
        "iforest_preds = iforest.predict(X_test)\n",
        "dbscan_preds = np.where(dbscan.fit_predict(X_test) == -1, 0, 1)\n",
        "\n",
        "# Number of entities where y_pred = 1 detected by models\n",
        "print('Number of entities where y_pred = 1 detected by models:')\n",
        "for preds, model_name in zip([svm_preds, iforest_preds, dbscan_preds], ['SVM', 'Isolation Forest', 'DBSCAN']):\n",
        "    print(f\"{model_name}: {len(X_test[preds == 1])} instances\")\n",
        "\n",
        "# First 15 indexes of entities where y_pred = 1\n",
        "print('\\nFirst 15 indexes of entities where y_pred = 1:')\n",
        "for preds, model_name in zip([svm_preds, iforest_preds, dbscan_preds], ['SVM', 'Isolation Forest', 'DBSCAN']):\n",
        "    print(f\"{model_name}: {np.where(preds == 1)[0][:15]}\")\n",
        "\n",
        "# Accuracy and Confusion Matrix per each model\n",
        "print('\\nAccuracy and Confusion Matrix per each model:')\n",
        "for preds, model_name in zip([svm_preds, iforest_preds, dbscan_preds], ['SVM', 'Isolation Forest', 'DBSCAN']):\n",
        "    preds_transformed = np.where(preds == -1, 0, 1) if model_name in ['SVM', 'Isolation Forest'] else preds\n",
        "    print(f\"{model_name}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, preds_transformed):.2f}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, preds_transformed)}\\n\")\n",
        "\n",
        "# Precision, Recall, F1score for each model\n",
        "print('Precision, Recall, F1score for each model')\n",
        "for preds, model_name in zip([svm_preds, iforest_preds, dbscan_preds], ['SVM', 'Isolation Forest', 'DBSCAN']):\n",
        "    preds_transformed = np.where(preds == -1, 0, 1) if model_name in ['SVM', 'Isolation Forest'] else preds\n",
        "    precision = precision_score(y_test, preds_transformed)\n",
        "    recall = recall_score(y_test, preds_transformed)\n",
        "    f1score = f1_score(y_test, preds_transformed)\n",
        "    print(f\"{model_name}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 score: {f1score:.2f}\\n\")\n"
      ],
      "metadata": {
        "id": "ifKOhpaQsQb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of common entities where y_pred = 1 for all 3 models\n",
        "common_entities = set(np.where(svm_preds == 1)[0]).intersection(np.where(iforest_preds == 1)[0]).intersection(np.where(dbscan_preds == 1)[0])\n",
        "print(\"Number of common entities where y_pred = 1 for all 3 models: {}\".format(len(common_entities)))\n",
        "print('')\n",
        "\n",
        "# First 15 indexes of common entities where y_pred = 1 for all 3 models\n",
        "print(\"First 15 indexes of common entities where y_pred = 1 for all 3 models:\")\n",
        "common_entities = list(common_entities)\n",
        "for i in range(min(len(common_entities), 15)):\n",
        "    print(common_entities[i])\n",
        "print('')"
      ],
      "metadata": {
        "id": "TkFjV5C7-ejc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import umap #!pip install umap-learn\n",
        "\n",
        "# Transform data using t-SNE\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X_test)\n",
        "\n",
        "# Transform data using UMAP\n",
        "umap = umap.UMAP(n_components=2)\n",
        "X_umap = umap.fit_transform(X_test)\n",
        "\n",
        "# Visualize predictions using t-SNE\n",
        "for preds, model_name in zip([svm_preds, iforest_preds, dbscan_preds], ['SVM', 'Isolation Forest', 'DBSCAN']):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=['red' if pred == 1 else 'green' for pred in preds])\n",
        "    plt.title(f\"{model_name} - t-SNE\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions using UMAP\n",
        "for preds, model_name in zip([svm_preds, iforest_preds, dbscan_preds], ['SVM', 'Isolation Forest', 'DBSCAN']):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(X_umap[:, 0], X_umap[:, 1], c=['red' if pred == 1 else 'green' for pred in preds])\n",
        "    plt.title(f\"{model_name} - UMAP\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "h-O8HKa5VWb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}