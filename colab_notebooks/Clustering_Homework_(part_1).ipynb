{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Nx96_c8r_wvr",
        "le93zU8PBdSp",
        "p4BKlsB6BqAG",
        "Ed61FNtVBqYS",
        "G0vcog6vBqfL",
        "QSVAoUHnBqmB",
        "cpJMGqslCDCI"
      ],
      "authorship_tag": "ABX9TyOLALyg2uABd4IXogEKkcFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/Clustering_Homework_(part_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW - https://drive.google.com/file/d/1JNwW2rGFTRIryYmBeOe8IENrjY-LDFcJ/view\n",
        "The dataset:\n",
        "https://drive.google.com/file/d/1S5stL1xz51y5QkGRHHfWqteacHDYOYXM/view?usp=sharing\n",
        "\n",
        "It contains information about earthquakes: coordinates, depth and strength.\n",
        "\n",
        "Your task is to cluster the data\n",
        "\n",
        "Q-s: Linkage;\n",
        "Dendrogram plot;\n",
        "Silhouette coefficient;"
      ],
      "metadata": {
        "id": "Nx96_c8r_wvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLKuUCrp_nUI"
      },
      "outputs": [],
      "source": [
        "!pip install bds_courseware"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bds_courseware import read_drive_dataset\n",
        "from bds_courseware import print_dataset_description, print_module_datasets\n",
        "from bds_courseware import HOMEWORK_DATASETS\n",
        "\n",
        "print(\"Dataset names: \", list(HOMEWORK_DATASETS.keys())) # Changed to list() for better display\n",
        "name = \"quake\"\n",
        "df = read_drive_dataset(*HOMEWORK_DATASETS[name])\n",
        "print(df.head(10))\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "IaSM_fbV_8Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory analysis (there are some specific characteristics of features, which may change the result of the analysis)\n",
        "\n",
        "1. We see that our dataset's latitudes follow a multimodal distribution of a trimodal distribution to be precise; I assume we can use clustering to cluster the 3 different groups of latitudes and try and understand why those clusters have similar latitudes and what unites those clusters.\n",
        "\n",
        "2. The Laltitude follows a trimodal distribution similar to the latitude. We will perform clustering on those features in a later stage of this kernel, hopefully giving some insight into different groups of earthquake sites\n",
        "\n",
        "3. When we look at the distribution of earthquake depths, we see that most earthquakes follow a bimodal distribution around depth 60. Still, we have some records of earthquakes occurring at depth 600-700, rare and defined as black swans. We will ignore those values in the next steps to see the true distribution without an extremely long tail."
      ],
      "metadata": {
        "id": "le93zU8PBdSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#find duplicate rows\n",
        "duplicate_rows = df.duplicated(subset=df.columns, keep=\"first\")\n",
        "duplicate_rows.sum()"
      ],
      "metadata": {
        "id": "GlW0cPvUBkfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove duplicate rows\n",
        "data = df.drop_duplicates()\n",
        "print(data.info())\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "DQByWT5HIm8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for each dataset column print unique values\n",
        "for col in data.columns:\n",
        "    n_unique_values = data[col].nunique()\n",
        "    unique_values = data[col].unique()\n",
        "    print(f\"{col}: {n_unique_values}: {unique_values}\")"
      ],
      "metadata": {
        "id": "IbF5hHX4I35P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing data for each variable and way to handle it. missing data can imply a reduction of the sample size\n",
        "\n",
        "total = data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
        "total_2 = data.isna().sum().sort_values(ascending=False)\n",
        "percent_2 = (data.isna().sum()/data.isna().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent, total_2, percent_2], axis=1, keys=['Tota_null', 'Percent_null', 'Total_na', 'Percent_na'])\n",
        "missing_data"
      ],
      "metadata": {
        "id": "vLBwFcWVKOPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the histogram\n",
        "for col in data.columns:\n",
        "    sns.histplot(data=data, x=col, kde=True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PWVHkIrdJ48g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scatterplot method\n",
        "sns.set()\n",
        "sns.pairplot(data, size = 2.5)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "EuhjC3k4JuuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outliers detection\n",
        "\n",
        "#Interquartile Range (IQR) method (values outside the normal range)\n",
        "for col in data.columns:\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[col] < lower) | (data[col] > upper)]\n",
        "    print(f\"{col} has {len(outliers)} outliers\")"
      ],
      "metadata": {
        "id": "6SeaZRclKY7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmap style\n",
        "sns.set(style='darkgrid')\n",
        "corrmat = data.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, annot=True, fmt=\".2f\", cmap='coolwarm', vmax=.8, square=True)\n",
        "plt.show()\n",
        "\n",
        "# print highly positively correlated pairs\n",
        "pos_corr_pairs = []\n",
        "for i in range(len(corrmat.columns)):\n",
        "    for j in range(i+1, len(corrmat.columns)):\n",
        "        if abs(corrmat.iloc[i, j]) >= 0.5:\n",
        "            pos_corr_pairs.append((corrmat.columns[i], corrmat.columns[j]))\n",
        "\n",
        "print(\"Highly positively correlated pairs:\")\n",
        "for pair in pos_corr_pairs:\n",
        "    print(pair)\n",
        "\n",
        "# print highly negatively correlated pairs\n",
        "neg_corr_pairs = []\n",
        "for i in range(len(corrmat.columns)):\n",
        "    for j in range(i+1, len(corrmat.columns)):\n",
        "        if abs(corrmat.iloc[i, j]) <=  -0.5:\n",
        "            neg_corr_pairs.append((corrmat.columns[i], corrmat.columns[j]))\n",
        "\n",
        "print(\"Highly negatively correlated pairs:\")\n",
        "for pair in neg_corr_pairs:\n",
        "    print(pair)"
      ],
      "metadata": {
        "id": "9rCoHi2IJ-mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build K-Means with 15 clusters\n"
      ],
      "metadata": {
        "id": "p4BKlsB6BqAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# apply K-Means clustering with 15 clusters\n",
        "kmeans = KMeans(n_clusters=15)\n",
        "kmeans.fit(data)\n",
        "\n",
        "# number of points in each cluster\n",
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "print('The number of points per cluster dictionsry:')\n",
        "print(dict(zip(unique, counts)))"
      ],
      "metadata": {
        "id": "8m7L3gYYBqAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determine optimal number of clusters for K-Means\n"
      ],
      "metadata": {
        "id": "Ed61FNtVBqYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from gap_statistic import OptimalK #!pip install gap-stat\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Suppress future warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
        "\n",
        "# method 1: Elbow method\n",
        "distortions = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k)\n",
        "    kmeans.fit(data)\n",
        "    distortions.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(2, 11), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method')\n",
        "plt.show()\n",
        "\n",
        "# find the optimal number of clusters for the Elbow method\n",
        "elbow_optimal_k = np.argmin(np.diff(distortions)) + 2\n",
        "print('Elbow Method suggests optimal number of clusters: ', elbow_optimal_k)\n",
        "\n",
        "# method 2: Silhouette method\n",
        "silhouette_scores = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k)\n",
        "    kmeans.fit(data)\n",
        "    score = silhouette_score(data, kmeans.labels_)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.title('Silhouette Method')\n",
        "plt.show()\n",
        "\n",
        "# find the optimal number of clusters for the Silhouette method\n",
        "silhouette_optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
        "print('Silhouette Method suggests optimal number of clusters: ', silhouette_optimal_k)\n",
        "\n",
        "# method 3: Calinski-Harabasz index\n",
        "scores = []\n",
        "for k in range(2, 21):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(data)\n",
        "    score = calinski_harabasz_score(data, labels)\n",
        "    scores.append(score)\n",
        "\n",
        "# plot the scores\n",
        "plt.plot(range(2, 21), scores)\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Calinski-Harabasz score')\n",
        "plt.show()\n",
        "\n",
        "# find the optimal number of clusters\n",
        "optimal_k = np.argmax(scores) + 2\n",
        "print(\"Optimal number of clusters: \", optimal_k)"
      ],
      "metadata": {
        "id": "t0n6J4KxBqYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create visualization for the obtained clusters\n"
      ],
      "metadata": {
        "id": "G0vcog6vBqfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KMeans clustering with optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(data)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# plot the clusters\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(data['Longitude'], data['Latitude'], c=labels, cmap='viridis')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8UJ4Kw59BqfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Other clustering algorithms; tune their hyperparameters to achieve better results.\n",
        "\n",
        "\n",
        "#2. Use labelled and unlabeled metrics to estimate quality of clusters you built. As ground truth use cluster labels from K-Means, choose most similar algorithm\n",
        "\n",
        "1. Since K-Means was used as the ground truth for labelling, we can use the Adjusted Rand Index (ARI) as the labelled metric to measure the similarity between the K-Means labels and DBSCAN labels. For the unlabeled metric, we can use the silhouette score. The Adjusted Rand Index (ARI) measures the similarity between the true labels and the predicted labels of the clustering algorithm. An ARI of 1 means that the predicted labels perfectly match the true labels, while an ARI of 0 means that the predicted labels are no better than random.\n",
        "\n",
        "2. The data has been clustered using both KMeans and Agglomerative clustering methods, and the best parameters for each method have been determined to be n_clusters=4.\n",
        "\n",
        "3. The quality of the clusters has been evaluated using both labeled (Adjusted Rand Score) and unlabeled (Silhouette Score) metrics.\n",
        "The KMeans method has a higher Silhouette Score (0.64) than the Agglomerative method (0.63), indicating that the KMeans method has generated more distinct clusters.\n",
        "\n",
        "\n",
        "#3. Explain the final choice of best clusterizations: give interpretation of clusters\n",
        "\n",
        "It's possible that the resulting clusters may represent regions of the world that are more prone to earthquakes, or areas with certain geological characteristics that make them more susceptible to seismic activity.\n",
        "The descriptive statistics show that the clusters differ in terms of focal depth, latitude, longitude, and Richter magnitude:\n",
        "Cluster 0 appears to be characterized by earthquakes with shallow focal depths and occurring in a variety of locations, with a mean Richter magnitude of 5.98.\n",
        "\n",
        "Cluster 1 is characterized by earthquakes with deep focal depths and occurring primarily in a region with negative latitudes and longitudes, with a mean Richter magnitude of 5.93.\n",
        "\n",
        "Cluster 2 is characterized by earthquakes with shallow focal depths occurring primarily in a region with positive longitudes, with a mean Richter magnitude of 5.98.\n",
        "\n",
        "Cluster 3 is characterized by earthquakes with intermediate focal depths occurring primarily in a region with positive latitudes and longitudes, with a mean Richter magnitude of 5.98.\n",
        "\n",
        "It is not possible to draw definitive conclusions about the physical or geological characteristics of the earthquakes in each cluster based on the available data, but the differences in focal depth, location, and magnitude suggest that the clusters may represent different types of earthquakes or seismic activity.\n"
      ],
      "metadata": {
        "id": "QSVAoUHnBqmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score\n",
        "\n",
        "# KMeans clustering algorithm\n",
        "kmeans_scores = []\n",
        "for n_clusters in range(2, 10):\n",
        "    kmeans_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans_labels = kmeans_model.fit_predict(data)\n",
        "    kmeans_silhouette = silhouette_score(data, kmeans_labels)\n",
        "    kmeans_scores.append(kmeans_silhouette)\n",
        "best_kmeans_idx = np.argmax(kmeans_scores)\n",
        "best_kmeans_model = KMeans(n_clusters=best_kmeans_idx+2, random_state=42)\n",
        "best_kmeans_labels = best_kmeans_model.fit_predict(data)\n",
        "best_kmeans_score = kmeans_scores[best_kmeans_idx]\n",
        "\n",
        "# Calculate adjusted_rand_score for KMeans\n",
        "kmeans_ari = adjusted_rand_score(best_kmeans_labels, best_kmeans_labels)\n",
        "\n",
        "# Calculate descriptive statistics for KMeans clusters\n",
        "kmeans_clusters = pd.DataFrame(data)\n",
        "kmeans_clusters['Cluster'] = best_kmeans_labels\n",
        "kmeans_stats = kmeans_clusters.groupby('Cluster').describe()\n",
        "\n",
        "print(f\"Best KMeans parameters: n_clusters={best_kmeans_idx+2}\")\n",
        "print(f\"KMeans Silhouette Score: {best_kmeans_score:.2f}\")\n",
        "print(f\"KMeans Adjusted Rand Score: {kmeans_ari:.2f}\")\n",
        "print(f\"KMeans Cluster Descriptive Statistics:\\n{kmeans_stats}\")\n",
        "\n",
        "# Agglomerative Clustering algorithm\n",
        "agg_scores = []\n",
        "for n_clusters in range(2, 10):\n",
        "    agg_model = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    agg_labels = agg_model.fit_predict(data)\n",
        "    agg_silhouette = silhouette_score(data, agg_labels)\n",
        "    agg_scores.append(agg_silhouette)\n",
        "best_agg_idx = np.argmax(agg_scores)\n",
        "best_agg_model = AgglomerativeClustering(n_clusters=best_agg_idx+2)\n",
        "best_agg_labels = best_agg_model.fit_predict(data)\n",
        "best_agg_score = agg_scores[best_agg_idx]\n",
        "\n",
        "# Calculate adjusted_rand_score for Agglomerative Clustering\n",
        "agg_ari = adjusted_rand_score(best_kmeans_labels, best_agg_labels)\n",
        "\n",
        "# Calculate descriptive statistics for Agglomerative Clustering clusters\n",
        "agg_clusters = pd.DataFrame(data)\n",
        "agg_clusters['Cluster'] = best_agg_labels\n",
        "agg_stats = agg_clusters.groupby('Cluster').describe()\n",
        "\n",
        "print(f\"Best Agglomerative Clustering parameters: n_clusters={best_agg_idx+2}\")\n",
        "print(f\"Agglomerative Clustering Silhouette Score: {best_agg_score:.2f}\")\n",
        "print(f\"Agglomerative Clustering Adjusted Rand Score: {agg_ari:.2f}\")\n",
        "print(f\"Agglomerative Clustering Cluster Descriptive Statistics:\\n{agg_stats}\")\n",
        "\n",
        "# DBSCAN clustering algorithm\n",
        "dbscan_scores = []\n",
        "for eps in [0.1, 0.5, 1, 2, 5, 10]:\n",
        "    for min_samples in range(2, 10):\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        dbscan_labels = dbscan.fit_predict(data)\n",
        "        if len(np.unique(dbscan_labels)) > 1:\n",
        "            dbscan_silhouette = silhouette_score(data, dbscan_labels)\n",
        "            dbscan_scores.append(dbscan_silhouette)\n",
        "best_dbscan_idx = np.argmax(dbscan_scores)\n",
        "best_dbscan_eps = [0.1, 0.5, 1, 2, 5, 10][best_dbscan_idx // 8]\n",
        "best_dbscan_min_samples = list(range(2, 10))[best_dbscan_idx % 8]\n",
        "best_dbscan_eps_idx = best_dbscan_idx // 8\n",
        "best_dbscan_eps = [0.1, 0.5, 1, 2, 5, 10][best_dbscan_eps_idx]\n",
        "best_dbscan_model = DBSCAN(eps=best_dbscan_eps, min_samples=best_dbscan_min_samples)\n",
        "best_dbscan_labels = best_dbscan_model.fit_predict(data)\n",
        "best_dbscan_score = dbscan_scores[best_dbscan_idx]\n",
        "\n",
        "#Calculate adjusted_rand_score for DBSCAN\n",
        "dbscan_ari = adjusted_rand_score(best_kmeans_labels, best_dbscan_labels)\n",
        "\n",
        "#Calculate descriptive statistics for DBSCAN clusters\n",
        "dbscan_clusters = pd.DataFrame(data)\n",
        "dbscan_clusters['Cluster'] = best_dbscan_labels\n",
        "dbscan_stats = dbscan_clusters.groupby('Cluster').describe()\n",
        "\n",
        "print(f\"Best DBSCAN parameters: eps={best_dbscan_eps}, min_samples={best_dbscan_min_samples}\")\n",
        "print(f\"DBSCAN Silhouette Score: {best_dbscan_score:.2f}\")\n",
        "print(f\"DBSCAN Adjusted Rand Score: {dbscan_ari:.2f}\")\n",
        "print(f\"DBSCAN Cluster Descriptive Statistics:\\n{dbscan_stats}\")"
      ],
      "metadata": {
        "id": "ijoXiF_pn9Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the results\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(data['Longitude'], data['Latitude'], c=best_kmeans_labels, cmap='viridis')\n",
        "plt.title(f'KMeans Clustering (Score: {best_kmeans_score:.2f})')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(data['Longitude'], data['Latitude'], c=best_agg_labels, cmap='viridis')\n",
        "plt.title(f'Agglomerative Clustering (Score: {best_agg_score:.2f})')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(data['Longitude'], data['Latitude'], c=best_dbscan_labels, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C7yKdbT0BqmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as ex\n",
        "\n",
        "data['Cluster_DBSCAN'] = dbscan.labels_\n",
        "\n",
        "fig = ex.scatter_3d(data,x='Longitude',y='Focal depth',z='Latitude',color='Cluster_DBSCAN',height=900)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "E9XLf9CAKp8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8RRxxkJpBqs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize the best clusterizations in your opinion on the world map\n",
        "Geographically, the areas with the highest frequency and intensity of earthquakes are typically located along the Ring of Fire, a horseshoe-shaped region that encircles the Pacific Ocean. This region includes the west coast of North and South America, Japan, Indonesia, and the Philippines, among other countries.\n",
        "\n",
        "Other regions with high earthquake activity include the Himalayan region in Asia, the Alpide Belt in Europe and Asia, and the Mid-Atlantic Ridge. These regions are all characterized by active tectonic plate boundaries or volcanic activity."
      ],
      "metadata": {
        "id": "cpJMGqslCDCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "\n",
        "data['B_Cluster'] = agg_labels\n",
        "\n",
        "# create a dictionary of DataFrames for each cluster\n",
        "labels = {}\n",
        "for cluster in data['B_Cluster'].unique():\n",
        "    labels[cluster] = data.loc[data['B_Cluster'] == cluster, ['Latitude', 'Longitude', 'Richter', 'Focal depth']]\n",
        "\n",
        "# create a map centered at (0, 0)\n",
        "world_map = folium.Map(location=[0, 0], zoom_start=2)\n",
        "\n",
        "# create a feature group for each cluster\n",
        "for cluster in labels.keys():\n",
        "    feature_group = folium.FeatureGroup(name=f'B_Cluster {cluster}')\n",
        "\n",
        "    # add markers to the feature group for each earthquake in the cluster\n",
        "    for index, row in labels[cluster].iterrows():\n",
        "        folium.Marker([row['Latitude'], row['Longitude']],\n",
        "                      popup=f\"Magnitude: {row['Richter']}, Depth: {row['Focal depth']} km\").add_to(feature_group)\n",
        "\n",
        "    # add the feature group to the map\n",
        "    feature_group.add_to(world_map)\n",
        "\n",
        "# add a layer control to the map\n",
        "folium.LayerControl().add_to(world_map)\n",
        "\n",
        "# display the map\n",
        "world_map"
      ],
      "metadata": {
        "id": "6rwUbWso5mqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}