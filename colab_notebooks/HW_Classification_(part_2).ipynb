{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FDGeiHonkfVg",
        "I7WOqhFMkvJq",
        "pOtmwcNwkvEW",
        "wnmvx9SskvBG"
      ],
      "authorship_tag": "ABX9TyPQXbvEOkaw4eRd0fZLdXf5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allakoala/data_science/blob/main/colab_notebooks/HW_Classification_(part_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW - https://docs.google.com/document/d/1L7p-GnVDEUal3JkgR9E2Kqp62yT06TXKWYwXTnVOhuk/edit\n",
        "\n",
        "Data -This is a dataset for binary sentiment classification. We provide a set of 50,000 highly polar movie reviews for training and testing\n",
        "https://drive.google.com/file/d/14TaFIFoslOAAljV2uj5rU9biAuocKScX/view?usp=sharing\n",
        "\n",
        "To do - Take the provided dataset and solve the binary classification task.\n",
        "\n",
        "Target – sentiment pos/neg\n",
        "\n",
        "Evaluation - Metric AUC-ROC with visualisation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FDGeiHonkfVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number"
      ],
      "metadata": {
        "id": "NYSWXZ5XaJwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StbTKheJkX2S"
      },
      "outputs": [],
      "source": [
        "#basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import sklearn\n",
        "import nltk #pip install nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import unidecode #!pip install unidecode\n",
        "from word2number import w2n #!pip install word2number\n",
        "import contractions #!pip install contractions\n",
        "\n",
        "import en_core_web_sm\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "nlp = spacy.load('en_core_web_md')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XV7qIDKDKO2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text preprocessing with explanations of all steps:\n",
        "The given code defines a function clean_text() which takes a list of documents as input and performs a series of text preprocessing steps on each document. The processed documents are returned as a list.\n",
        "\n",
        "Here is a breakdown of the steps performed in the function:\n",
        "\n",
        "- Remove HTML tags using BeautifulSoup library and replace them with a space separator.\n",
        "- Remove accented characters using unidecode library.\n",
        "- Expand contractions using contractions library.\n",
        "- Convert number words to numeric form using word2number library.\n",
        "- Remove numbers using regular expressions.\n",
        "- Remove words with all duplicated letters (currently commented out).\n",
        "- Remove leading and ending spaces using string methods.\n",
        "- Remove punctuation using string methods.\n",
        "- Convert all text to lowercase using string methods.\n",
        "- Tokenize text using nltk library and count the number of words in the corpus.\n",
        "- Remove stop words using a set of stop words defined earlier in the code.\n",
        "\n",
        "#Conclusion:\n",
        "1.Stemming is a process of reducing words to their root form, while lemmatization involves reducing words to their base form, which is usually a dictionary word. In this case, we can see that both techniques have reduced the words to their base form, although they have different results. For example, the word \"comfortable\" has been stemmed to \"comfort\" but has been lemmatized to \"comfortable\". Similarly, the word \"mentioned\" has been stemmed to \"mention\" but has been lemmatized to \"mentioned\". In conclusion, both stemming and lemmatization are useful techniques for text normalization, but they have different approaches and results. Stemming is a simpler and more aggressive approach that involves removing the suffixes of words to reduce them to their root form, while lemmatization is a more complex and accurate approach that involves reducing words to their base form. The choice between these techniques will depend on the specific needs and goals of the text processing task. Starting from this pair Index: 0 Normalization1: happened | Normalization2: goe words comparison is asynchronised, that means that aparently stemming removes some words at some points."
      ],
      "metadata": {
        "id": "I7WOqhFMkvJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path of the file to read\n",
        "url = \"/content/drive/MyDrive/Colab Notebooks/data_class_2/LargeMovieReviewDataset.csv\"\n",
        "\n",
        "#read the file into a variable\n",
        "data = pd.read_csv(url, sep=',')\n",
        "\n",
        "#examine the data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "HrZHAbaMlEA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = {word for word in stopwords.words('english') if word not in {'no', 'not'}}\n",
        "\n",
        "def clean_text(documents):\n",
        "    # Remove HTML tags\n",
        "    documents = [BeautifulSoup(doc, \"html.parser\").get_text(separator=\" \") for doc in documents]\n",
        "\n",
        "    # Remove accented characters from text, e.g. café\n",
        "    documents = [unidecode.unidecode(doc) for doc in documents]\n",
        "\n",
        "    # Expand contractions\n",
        "    documents = [contractions.fix(doc) for doc in documents]\n",
        "\n",
        "    # Convert number words to numeric form\n",
        "    new_documents = []\n",
        "    for doc in documents:\n",
        "        words = []\n",
        "        for word in doc.split():\n",
        "            if word.isalpha():\n",
        "                try:\n",
        "                    num = w2n.word_to_num(word)\n",
        "                    words.append(str(num))\n",
        "                except ValueError:\n",
        "                    words.append(word)\n",
        "            else:\n",
        "                words.append(word)\n",
        "        new_documents.append(' '.join(words))\n",
        "    documents = new_documents\n",
        "\n",
        "    # Remove numbers\n",
        "    documents = [re.sub(r'\\b\\d+\\b', '', doc) for doc in documents]\n",
        "\n",
        "    # Remove words with all duplicated letters\n",
        "    # documents = [re.sub(r'\\b\\w*(\\w)\\1{1,}\\w*\\b', '', doc) for doc in documents]\n",
        "\n",
        "    # Remove leading and ending spaces\n",
        "    documents = [\" \".join(doc.strip().split()) for doc in documents]\n",
        "\n",
        "    # Remove punctuation\n",
        "    documents = [doc.translate(str.maketrans('', '', string.punctuation)) for doc in documents]\n",
        "\n",
        "    # Convert to lowercase\n",
        "    documents = [doc.lower() for doc in documents]\n",
        "\n",
        "    # Tokenize text and count the number of words in the corpus\n",
        "    tokens = [nltk.word_tokenize(doc) for doc in documents]\n",
        "    corpus_size_words = np.sum([len(d) for d in tokens])\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [[token for token in doc_tokens if token not in stop_words] for doc_tokens in tokens]\n",
        "\n",
        "    # Print descriptive statistics\n",
        "    corpus_size_docs = len(documents)\n",
        "    sentiment_distr = Counter(data['sentiment'])\n",
        "    print('Corpus Size (Number of Documents): {}'.format(corpus_size_docs))\n",
        "    print('Corpus Size (Number of Words): {}'.format(corpus_size_words))\n",
        "    print('Sentiment Distribution: {}'.format(sentiment_distr))\n",
        "\n",
        "    return [' '.join(filtered_doc_tokens) for filtered_doc_tokens in filtered_tokens]\n",
        "\n",
        "data['review_cleaned'] = clean_text(data['review'])\n",
        "data['review_cleaned']"
      ],
      "metadata": {
        "id": "QoAnBI-dFLMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization 1 - lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text1(text):\n",
        "    # Tokenize words\n",
        "    text = nltk.word_tokenize(text)\n",
        "    # Lemmatize words\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    return text\n",
        "\n",
        "data['review_normalized1'] = data['review_cleaned'].apply(normalize_text1)\n",
        "\n",
        "# concatenate the lists of normalized words for all reviews\n",
        "normalized1 = []\n",
        "for review in data['review_normalized1']:\n",
        "    normalized1.extend(review)\n",
        "\n",
        "print(normalized1[:7]) # print first 7 words"
      ],
      "metadata": {
        "id": "QfMvMggAg2OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization 2 - stemming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_text2(text):\n",
        "    # Tokenize words\n",
        "    text = nltk.word_tokenize(text)\n",
        "    # Stem words\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "data['review_normalized2'] = data['review_cleaned'].apply(normalize_text2)\n",
        "\n",
        "# concatenate the lists of normalized words for all reviews\n",
        "normalized2 = []\n",
        "for review in data['review_normalized2']:\n",
        "    normalized2.extend(review)\n",
        "\n",
        "print(normalized2[:7]) # print first 7 words"
      ],
      "metadata": {
        "id": "JODboubkhF6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the list of the word pairs that are different after Normalization 1 and Normalization 2\n",
        "for i in range(min(len(data), 10)):\n",
        "    normalized1 = set(data['review_normalized1'][i])\n",
        "    normalized2 = set(data['review_normalized2'][i])\n",
        "    diff = normalized1.symmetric_difference(normalized2)\n",
        "    if diff:\n",
        "        for word1, word2 in zip(sorted(normalized1 - normalized2), sorted(normalized2 - normalized1)):\n",
        "            print(\"Index:\", i, \"Normalization1:\", word1, \"| Normalization2:\", word2)"
      ],
      "metadata": {
        "id": "uYaHJHlRhQIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert sentiment to binary (0 or 1)\n",
        "data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
        "#data[['review_normalized1','review_normalized2','sentiment']].head()\n",
        "data[['review_normalized2','sentiment']]"
      ],
      "metadata": {
        "id": "m5CrmgWdj7Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[['review_normalized1','sentiment']]"
      ],
      "metadata": {
        "id": "e0byQsB4U1HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Words importance\n",
        "See the WordClouds for both sentiments.\n"
      ],
      "metadata": {
        "id": "frgtxThUkvHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a dictionary to count the frequency of each word in each sentiment class\n",
        "freq_dict = defaultdict(lambda: defaultdict(int))\n",
        "for i, row in data[['review_normalized1','sentiment']].iterrows():\n",
        "    sentiment = row['sentiment']\n",
        "    words = \" \".join(row['review_normalized1'])  # join the list of words with a space separator\n",
        "    for word in words.split():\n",
        "        freq_dict[sentiment][word] += 1\n",
        "\n",
        "# Sort the dictionary by frequency and print out the top 50 words for each sentiment class\n",
        "for sentiment in freq_dict:\n",
        "    print(f\"Sentiment {sentiment}:\")\n",
        "    top_words = sorted(freq_dict[sentiment].items(), key=itemgetter(1), reverse=True)[:50]\n",
        "    for word, freq in top_words:\n",
        "        print(f\"{word}: {freq}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "XnPs_5KulEXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a dictionary to count the frequency of each word in each sentiment class\n",
        "freq_dict = defaultdict(lambda: defaultdict(int))\n",
        "for i, row in data[['review_normalized1','sentiment']].iterrows():\n",
        "    sentiment = row['sentiment']\n",
        "    words = \" \".join(row['review_normalized1'])  # join the list of words with a space separator\n",
        "    for word in words.split():\n",
        "        freq_dict[sentiment][word] += 1\n",
        "\n",
        "# Check for same words in both lists and delete the word with the lowest frequency count\n",
        "for word in freq_dict[1].keys() & freq_dict[0].keys():\n",
        "    if freq_dict[1][word] < freq_dict[0][word]:\n",
        "        del freq_dict[1][word]\n",
        "    else:\n",
        "        del freq_dict[0][word]\n",
        "\n",
        "# Sort the dictionary by frequency and print out the top 50 words for each sentiment class\n",
        "for sentiment in freq_dict:\n",
        "    print(f\"Sentiment {sentiment}:\")\n",
        "    top_words = sorted(freq_dict[sentiment].items(), key=itemgetter(1), reverse=True)[:50]\n",
        "    for word, freq in top_words:\n",
        "        print(f\"{word}: {freq}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "9zUkCQYxmdVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Define colors for each sentiment class\n",
        "colors = {1: 'Greens', 0: 'Reds'}\n",
        "\n",
        "for sentiment in freq_dict:\n",
        "    # Generate word cloud for each sentiment\n",
        "    wordcloud = WordCloud(width=600,\n",
        "                          height=400,\n",
        "                          random_state=2,\n",
        "                          max_font_size=100,\n",
        "                          background_color='white',\n",
        "                          colormap=colors[sentiment]).generate_from_frequencies(freq_dict[sentiment])\n",
        "\n",
        "    # Plot the word cloud\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Sentiment {sentiment}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0xLRBOyAMxk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters tuning\n",
        "SGD best params: {'alpha': 0.001}\n",
        "\n",
        "SVM best params: {'C': 1, 'kernel': 'linear'}\n",
        "\n",
        "NB best params: {'alpha': 0.1}\n",
        "\n",
        "SGD accuracy: 0.808\n",
        "\n",
        "SVM accuracy: 0.808\n",
        "\n",
        "NB accuracy: 0.778"
      ],
      "metadata": {
        "id": "pOtmwcNwkvEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data[['review_normalized1', 'sentiment']], test_size=0.33, random_state=42)\n",
        "\n",
        "#Sentiment Distribution for Train and Test\n",
        "print(\"Sentiment Distribution for Train:\", Counter(train['sentiment']))\n",
        "print(\"Sentiment Distribution for Test:\", Counter(test['sentiment']))\n",
        "\n",
        "X_train = [' '.join(words) for words in train['review_normalized1']]\n",
        "X_test = [' '.join(words) for words in test['review_normalized1']]\n",
        "y_train = train['sentiment']\n",
        "y_test = test['sentiment']"
      ],
      "metadata": {
        "id": "q5qgOu0jOInm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(min_df = 10, token_pattern = r'[a-zA-Z]+')\n",
        "X_train_bow = tfidf_vec.fit_transform(X_train) # fit train\n",
        "X_test_bow = tfidf_vec.transform(X_test) # transform test\n",
        "print(X_train_bow.shape)\n",
        "print(X_test_bow.shape)"
      ],
      "metadata": {
        "id": "Adp310p1Qbq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from scipy.stats import uniform\n",
        "import numpy as np\n",
        "\n",
        "# Limit the size of the dataset\n",
        "X_train_bow_reduced = X_train_bow[:1000]\n",
        "y_train_reduced = y_train[:1000]\n",
        "X_test_bow_reduced = X_test_bow[:500]\n",
        "y_test_reduced = y_test[:500]\n",
        "\n",
        "# Define parameter distributions for each model\n",
        "sgd_params = {\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
        "}\n",
        "\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "}\n",
        "\n",
        "nb_params = {\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV objects for each model\n",
        "sgd_gs = GridSearchCV(\n",
        "    SGDClassifier(),\n",
        "    sgd_params,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "svm_gs = GridSearchCV(\n",
        "    SVC(),\n",
        "    svm_params,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "nb_gs = GridSearchCV(\n",
        "    MultinomialNB(),\n",
        "    nb_params,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the models to the training data\n",
        "sgd_gs.fit(X_train_bow_reduced, y_train_reduced)\n",
        "svm_gs.fit(X_train_bow_reduced, y_train_reduced)\n",
        "nb_gs.fit(X_train_bow_reduced, y_train_reduced)\n",
        "\n",
        "# Evaluate the models on the test data\n",
        "sgd_score = sgd_gs.score(X_test_bow_reduced, y_test_reduced)\n",
        "svm_score = svm_gs.score(X_test_bow_reduced, y_test_reduced)\n",
        "nb_score = nb_gs.score(X_test_bow_reduced, y_test_reduced)\n",
        "\n",
        "print(\"SGD best params:\", sgd_gs.best_params_)\n",
        "print(\"SVM best params:\", svm_gs.best_params_)\n",
        "print(\"NB best params:\", nb_gs.best_params_)\n",
        "print(\"SGD accuracy:\", sgd_score)\n",
        "print(\"SVM accuracy:\", svm_score)\n",
        "print(\"NB accuracy:\", nb_score)"
      ],
      "metadata": {
        "id": "jlzRfNz7aDi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare performance of models\n",
        "Based on the evaluation results, the best model SGDClassifier AUC-ROC score : 0.8215260851719264"
      ],
      "metadata": {
        "id": "wnmvx9SskvBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Comparing the performance of the models\n",
        "sgd_pred = sgd_gs.predict(X_test_bow)\n",
        "svm_pred = svm_gs.predict(X_test_bow)\n",
        "nb_pred = nb_gs.predict(X_test_bow)\n",
        "\n",
        "sgd_auc = roc_auc_score(y_test, sgd_pred)\n",
        "svm_auc = roc_auc_score(y_test, svm_pred)\n",
        "nb_auc = roc_auc_score(y_test, nb_pred)\n",
        "\n",
        "print(\"AUC-ROC score for SGDClassifier:\", sgd_auc)\n",
        "print(\"AUC-ROC score for SVM:\", svm_auc)\n",
        "print(\"AUC-ROC score for Naive Bayes:\", nb_auc)\n",
        "\n",
        "# Visualizing AUC-ROC scores\n",
        "fpr_sgd, tpr_sgd, _ = roc_curve(y_test, sgd_pred)\n",
        "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm_pred)\n",
        "fpr_nb, tpr_nb, _ = roc_curve(y_test, nb_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_sgd, tpr_sgd, label=\"SGDClassifier (AUC = {:.2f})\".format(sgd_auc))\n",
        "plt.plot(fpr_svm, tpr_svm, label=\"SVM (AUC = {:.2f})\".format(svm_auc))\n",
        "plt.plot(fpr_nb, tpr_nb, label=\"Naive Bayes (AUC = {:.2f})\".format(nb_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pUOIcAvips7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Importance with the best model and its best parameters\n",
        "To evaluate the importance of the features is to look at their corresponding coefficients.\n",
        "\n",
        "Positive weights imply positive contribution of the feature to the prediction; negative weights imply negative contribution of the feature to the prediction.\n",
        "\n",
        "The absolute values of the weights indicate the effect sizes of the features.\n",
        "See the table below."
      ],
      "metadata": {
        "id": "jH9_LdThkn0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', SGDClassifier(alpha=0.001))\n",
        "])\n",
        "\n",
        "# Fit pipeline on training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Extract the coefficients of the model from the pipeline\n",
        "importances = pipeline.named_steps['clf'].coef_.flatten()\n",
        "\n",
        "# Select top 10 positive/negative weights\n",
        "top_indices_pos = np.argsort(importances)[::-1][:10]\n",
        "top_indices_neg = np.argsort(importances)[:10]\n",
        "\n",
        "# Get featnames from tfidfvectorizer\n",
        "feature_names = np.array(pipeline.named_steps['tfidf'].get_feature_names_out())\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'FEATURE': feature_names[np.concatenate((top_indices_pos, top_indices_neg))],\n",
        "    'IMPORTANCE': importances[np.concatenate((top_indices_pos, top_indices_neg))],\n",
        "    'SENTIMENT': ['pos' for _ in range(len(top_indices_pos))] + ['neg' for _ in range(len(top_indices_neg))]\n",
        "})\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "id": "EhF8vVTfkvuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}